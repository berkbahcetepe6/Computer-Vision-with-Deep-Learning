{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e71d01a",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "\n",
    "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc12e4",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d72f410",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036a63dd",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./comp411/datasets\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3768ff27942d4bce86ed8e4834b26a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./comp411/datasets\\cifar-10-python.tar.gz to ./comp411/datasets\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a8ae5",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe6be6f",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13778a2c",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cc7bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is conducted as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "        \n",
    "        attention_logits = torch.matmul(q, k.permute(0,1,3,2)) / np.sqrt(self.head_dims)\n",
    "        \n",
    "        ## Compute attention Weights. Note that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "        \n",
    "        attention_weights = F.softmax(attention_logits, dim=-1)\n",
    "        \n",
    "        ## Compute output values. Note that this operation is conducted as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H] = [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        \n",
    "        attn_out = torch.matmul(attention_weights, v)\n",
    "        attn_out = torch.permute(attn_out, dims=[0, 2, 1, 3]) #[B, N, H, D//H]\n",
    "        attn_out = torch.reshape(attn_out, [b, n, -1])\n",
    "\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a4472b",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90e290b5",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 256\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9abef",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0a95ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.elu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6100bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "    ###############################################################\n",
    "    # TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "    ###############################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "    \n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(hidden_dims)\n",
    "        self.selfattention = SelfAttention(hidden_dims, head_dims=(hidden_dims//num_heads),num_heads=num_heads, bias=bias)\n",
    "        self.layernorm2 = nn.LayerNorm(hidden_dims)\n",
    "        self.mlp = MLP(hidden_dims, hidden_dims, hidden_dims, bias=bias)\n",
    "        \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###################################################################\n",
    "    #                                 END OF YOUR CODE                #             \n",
    "    ###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "    ##############################################################\n",
    "    # TODO: Complete the forward of TransformerBlock module      #\n",
    "    ##############################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "        \n",
    "        x_norm = self.layernorm1(x)\n",
    "        attn_out = self.selfattention(x_norm)\n",
    "        x_skipped = x + attn_out\n",
    "        x_norm_2 = self.layernorm2(x_skipped)\n",
    "        mlp = self.mlp(x_norm_2)\n",
    "        x_skipped_2 = x_skipped + mlp\n",
    "        return x_skipped_2\n",
    "        \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###################################################################\n",
    "    #                                 END OF YOUR CODE                #             \n",
    "    ###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c6003",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f690c15",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,128]\n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac899ac",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "673ea1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51600d07",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38cfd1b8",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size())  # you should see [64,10]\n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc86a72",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291e1e8",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0219e4a3",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, np.float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5038353",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c38b1e0d",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af14aa",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e17ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efb82e",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c1d4d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ebea85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.074 | Acc: 8.000% (2/25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\envs\\comp411\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.882 | Acc: 10.000% (5/50)\n",
      "Loss: 4.601 | Acc: 10.667% (8/75)\n",
      "Loss: 5.011 | Acc: 8.000% (8/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 8.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 5.118 | Acc: 16.000% (4/25)\n",
      "Loss: 4.969 | Acc: 14.000% (7/50)\n",
      "Loss: 4.993 | Acc: 10.667% (8/75)\n",
      "Loss: 4.868 | Acc: 13.000% (13/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 13.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 5.322 | Acc: 16.000% (4/25)\n",
      "Loss: 4.671 | Acc: 20.000% (10/50)\n",
      "Loss: 4.727 | Acc: 16.000% (12/75)\n",
      "Loss: 4.384 | Acc: 18.000% (18/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 18.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.594 | Acc: 16.000% (4/25)\n",
      "Loss: 3.730 | Acc: 12.000% (6/50)\n",
      "Loss: 3.523 | Acc: 10.667% (8/75)\n",
      "Loss: 3.516 | Acc: 10.000% (10/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 10.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.489 | Acc: 28.000% (7/25)\n",
      "Loss: 2.919 | Acc: 20.000% (10/50)\n",
      "Loss: 2.826 | Acc: 22.667% (17/75)\n",
      "Loss: 2.807 | Acc: 22.000% (22/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 22.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.678 | Acc: 8.000% (2/25)\n",
      "Loss: 2.881 | Acc: 8.000% (4/50)\n",
      "Loss: 2.821 | Acc: 12.000% (9/75)\n",
      "Loss: 2.800 | Acc: 14.000% (14/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 14.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 2.988 | Acc: 12.000% (3/25)\n",
      "Loss: 2.426 | Acc: 28.000% (14/50)\n",
      "Loss: 2.316 | Acc: 24.000% (18/75)\n",
      "Loss: 2.377 | Acc: 23.000% (23/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 23.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.912 | Acc: 16.000% (4/25)\n",
      "Loss: 3.022 | Acc: 14.000% (7/50)\n",
      "Loss: 2.675 | Acc: 21.333% (16/75)\n",
      "Loss: 2.571 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.674 | Acc: 40.000% (10/25)\n",
      "Loss: 1.971 | Acc: 38.000% (19/50)\n",
      "Loss: 2.009 | Acc: 34.667% (26/75)\n",
      "Loss: 1.895 | Acc: 38.000% (38/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 38.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.014 | Acc: 28.000% (7/25)\n",
      "Loss: 3.060 | Acc: 24.000% (12/50)\n",
      "Loss: 2.912 | Acc: 24.000% (18/75)\n",
      "Loss: 2.790 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.488 | Acc: 48.000% (12/25)\n",
      "Loss: 1.595 | Acc: 48.000% (24/50)\n",
      "Loss: 1.505 | Acc: 48.000% (36/75)\n",
      "Loss: 1.484 | Acc: 50.000% (50/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 50.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.089 | Acc: 16.000% (4/25)\n",
      "Loss: 3.235 | Acc: 14.000% (7/50)\n",
      "Loss: 2.981 | Acc: 16.000% (12/75)\n",
      "Loss: 2.789 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.353 | Acc: 68.000% (17/25)\n",
      "Loss: 1.201 | Acc: 58.000% (29/50)\n",
      "Loss: 1.212 | Acc: 54.667% (41/75)\n",
      "Loss: 1.199 | Acc: 55.000% (55/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 55.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.643 | Acc: 20.000% (5/25)\n",
      "Loss: 2.632 | Acc: 18.000% (9/50)\n",
      "Loss: 2.443 | Acc: 21.333% (16/75)\n",
      "Loss: 2.404 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 0.866 | Acc: 72.000% (18/25)\n",
      "Loss: 0.807 | Acc: 76.000% (38/50)\n",
      "Loss: 0.784 | Acc: 72.000% (54/75)\n",
      "Loss: 0.863 | Acc: 70.000% (70/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 70.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.959 | Acc: 16.000% (4/25)\n",
      "Loss: 3.135 | Acc: 16.000% (8/50)\n",
      "Loss: 2.783 | Acc: 24.000% (18/75)\n",
      "Loss: 2.693 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 0.661 | Acc: 76.000% (19/25)\n",
      "Loss: 0.577 | Acc: 88.000% (44/50)\n",
      "Loss: 0.703 | Acc: 82.667% (62/75)\n",
      "Loss: 0.691 | Acc: 80.000% (80/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 80.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.963 | Acc: 12.000% (3/25)\n",
      "Loss: 3.205 | Acc: 14.000% (7/50)\n",
      "Loss: 2.935 | Acc: 20.000% (15/75)\n",
      "Loss: 2.876 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 0.505 | Acc: 88.000% (22/25)\n",
      "Loss: 0.464 | Acc: 92.000% (46/50)\n",
      "Loss: 0.419 | Acc: 92.000% (69/75)\n",
      "Loss: 0.429 | Acc: 91.000% (91/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 91.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.411 | Acc: 20.000% (5/25)\n",
      "Loss: 3.630 | Acc: 14.000% (7/50)\n",
      "Loss: 3.218 | Acc: 22.667% (17/75)\n",
      "Loss: 3.015 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Final train set accuracy is 91.0\n",
      "Final val set accuracy is 23.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims, input_dims, output_dims, num_trans_layers, num_heads, image_k, patch_k)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(10):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9994b2",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5204f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\envs\\comp411\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.264 | Acc: 9.375% (6/64)\n",
      "Loss: 5.532 | Acc: 9.375% (12/128)\n",
      "Loss: 5.447 | Acc: 8.333% (16/192)\n",
      "Loss: 5.144 | Acc: 10.938% (28/256)\n",
      "Loss: 4.954 | Acc: 10.938% (35/320)\n",
      "Loss: 4.689 | Acc: 13.281% (51/384)\n",
      "Loss: 4.505 | Acc: 13.393% (60/448)\n",
      "Loss: 4.330 | Acc: 13.477% (69/512)\n",
      "Loss: 4.207 | Acc: 13.021% (75/576)\n",
      "Loss: 4.078 | Acc: 13.281% (85/640)\n",
      "Loss: 3.967 | Acc: 13.920% (98/704)\n",
      "Loss: 3.843 | Acc: 14.844% (114/768)\n",
      "Loss: 3.771 | Acc: 14.904% (124/832)\n",
      "Loss: 3.726 | Acc: 15.067% (135/896)\n",
      "Loss: 3.684 | Acc: 15.104% (145/960)\n",
      "Loss: 3.611 | Acc: 14.648% (150/1024)\n",
      "Loss: 3.531 | Acc: 14.982% (163/1088)\n",
      "Loss: 3.456 | Acc: 15.451% (178/1152)\n",
      "Loss: 3.410 | Acc: 15.132% (184/1216)\n",
      "Loss: 3.354 | Acc: 15.312% (196/1280)\n",
      "Loss: 3.311 | Acc: 15.699% (211/1344)\n",
      "Loss: 3.271 | Acc: 15.838% (223/1408)\n",
      "Loss: 3.239 | Acc: 15.897% (234/1472)\n",
      "Loss: 3.196 | Acc: 16.406% (252/1536)\n",
      "Loss: 3.168 | Acc: 16.438% (263/1600)\n",
      "Loss: 3.131 | Acc: 16.887% (281/1664)\n",
      "Loss: 3.101 | Acc: 16.956% (293/1728)\n",
      "Loss: 3.072 | Acc: 17.243% (309/1792)\n",
      "Loss: 3.052 | Acc: 17.403% (323/1856)\n",
      "Loss: 3.032 | Acc: 17.344% (333/1920)\n",
      "Loss: 3.007 | Acc: 17.389% (345/1984)\n",
      "Loss: 2.987 | Acc: 17.383% (356/2048)\n",
      "Loss: 2.959 | Acc: 17.566% (371/2112)\n",
      "Loss: 2.935 | Acc: 17.693% (385/2176)\n",
      "Loss: 2.907 | Acc: 17.946% (402/2240)\n",
      "Loss: 2.889 | Acc: 18.142% (418/2304)\n",
      "Loss: 2.872 | Acc: 18.074% (428/2368)\n",
      "Loss: 2.849 | Acc: 18.339% (446/2432)\n",
      "Loss: 2.823 | Acc: 18.710% (467/2496)\n",
      "Loss: 2.809 | Acc: 18.633% (477/2560)\n",
      "Loss: 2.793 | Acc: 18.864% (495/2624)\n",
      "Loss: 2.772 | Acc: 19.122% (514/2688)\n",
      "Loss: 2.757 | Acc: 19.150% (527/2752)\n",
      "Loss: 2.743 | Acc: 19.070% (537/2816)\n",
      "Loss: 2.729 | Acc: 19.132% (551/2880)\n",
      "Loss: 2.713 | Acc: 19.361% (570/2944)\n",
      "Loss: 2.698 | Acc: 19.448% (585/3008)\n",
      "Loss: 2.682 | Acc: 19.792% (608/3072)\n",
      "Loss: 2.667 | Acc: 19.866% (623/3136)\n",
      "Loss: 2.651 | Acc: 19.906% (637/3200)\n",
      "Loss: 2.637 | Acc: 20.129% (657/3264)\n",
      "Loss: 2.627 | Acc: 20.102% (669/3328)\n",
      "Loss: 2.615 | Acc: 20.165% (684/3392)\n",
      "Loss: 2.601 | Acc: 20.428% (706/3456)\n",
      "Loss: 2.587 | Acc: 20.682% (728/3520)\n",
      "Loss: 2.577 | Acc: 20.843% (747/3584)\n",
      "Loss: 2.566 | Acc: 20.916% (763/3648)\n",
      "Loss: 2.555 | Acc: 20.959% (778/3712)\n",
      "Loss: 2.544 | Acc: 20.975% (792/3776)\n",
      "Loss: 2.532 | Acc: 21.250% (816/3840)\n",
      "Loss: 2.524 | Acc: 21.440% (837/3904)\n",
      "Loss: 2.513 | Acc: 21.648% (859/3968)\n",
      "Loss: 2.506 | Acc: 21.677% (874/4032)\n",
      "Loss: 2.496 | Acc: 21.729% (890/4096)\n",
      "Loss: 2.488 | Acc: 21.875% (910/4160)\n",
      "Loss: 2.485 | Acc: 21.899% (925/4224)\n",
      "Loss: 2.478 | Acc: 21.992% (943/4288)\n",
      "Loss: 2.471 | Acc: 21.990% (957/4352)\n",
      "Loss: 2.461 | Acc: 22.237% (982/4416)\n",
      "Loss: 2.458 | Acc: 22.121% (991/4480)\n",
      "Loss: 2.450 | Acc: 22.139% (1006/4544)\n",
      "Loss: 2.445 | Acc: 22.092% (1018/4608)\n",
      "Loss: 2.437 | Acc: 22.217% (1038/4672)\n",
      "Loss: 2.430 | Acc: 22.382% (1060/4736)\n",
      "Loss: 2.426 | Acc: 22.396% (1075/4800)\n",
      "Loss: 2.418 | Acc: 22.533% (1096/4864)\n",
      "Loss: 2.413 | Acc: 22.565% (1112/4928)\n",
      "Loss: 2.407 | Acc: 22.736% (1135/4992)\n",
      "Loss: 2.399 | Acc: 22.884% (1157/5056)\n",
      "Loss: 2.393 | Acc: 22.910% (1173/5120)\n",
      "Loss: 2.386 | Acc: 22.975% (1191/5184)\n",
      "Loss: 2.378 | Acc: 23.152% (1215/5248)\n",
      "Loss: 2.373 | Acc: 23.212% (1233/5312)\n",
      "Loss: 2.366 | Acc: 23.382% (1257/5376)\n",
      "Loss: 2.362 | Acc: 23.456% (1276/5440)\n",
      "Loss: 2.360 | Acc: 23.347% (1285/5504)\n",
      "Loss: 2.356 | Acc: 23.420% (1304/5568)\n",
      "Loss: 2.352 | Acc: 23.438% (1320/5632)\n",
      "Loss: 2.348 | Acc: 23.508% (1339/5696)\n",
      "Loss: 2.347 | Acc: 23.559% (1357/5760)\n",
      "Loss: 2.343 | Acc: 23.592% (1374/5824)\n",
      "Loss: 2.339 | Acc: 23.573% (1388/5888)\n",
      "Loss: 2.335 | Acc: 23.673% (1409/5952)\n",
      "Loss: 2.329 | Acc: 23.787% (1431/6016)\n",
      "Loss: 2.324 | Acc: 23.914% (1454/6080)\n",
      "Loss: 2.322 | Acc: 23.861% (1466/6144)\n",
      "Loss: 2.319 | Acc: 23.905% (1484/6208)\n",
      "Loss: 2.315 | Acc: 23.932% (1501/6272)\n",
      "Loss: 2.310 | Acc: 24.053% (1524/6336)\n",
      "Loss: 2.307 | Acc: 24.203% (1549/6400)\n",
      "Loss: 2.300 | Acc: 24.288% (1570/6464)\n",
      "Loss: 2.298 | Acc: 24.295% (1586/6528)\n",
      "Loss: 2.295 | Acc: 24.454% (1612/6592)\n",
      "Loss: 2.291 | Acc: 24.534% (1633/6656)\n",
      "Loss: 2.287 | Acc: 24.658% (1657/6720)\n",
      "Loss: 2.283 | Acc: 24.720% (1677/6784)\n",
      "Loss: 2.279 | Acc: 24.869% (1703/6848)\n",
      "Loss: 2.273 | Acc: 24.986% (1727/6912)\n",
      "Loss: 2.269 | Acc: 25.115% (1752/6976)\n",
      "Loss: 2.265 | Acc: 25.241% (1777/7040)\n",
      "Loss: 2.261 | Acc: 25.267% (1795/7104)\n",
      "Loss: 2.258 | Acc: 25.321% (1815/7168)\n",
      "Loss: 2.256 | Acc: 25.318% (1831/7232)\n",
      "Loss: 2.254 | Acc: 25.329% (1848/7296)\n",
      "Loss: 2.249 | Acc: 25.448% (1873/7360)\n",
      "Loss: 2.249 | Acc: 25.418% (1887/7424)\n",
      "Loss: 2.246 | Acc: 25.521% (1911/7488)\n",
      "Loss: 2.242 | Acc: 25.609% (1934/7552)\n",
      "Loss: 2.241 | Acc: 25.670% (1955/7616)\n",
      "Loss: 2.239 | Acc: 25.677% (1972/7680)\n",
      "Loss: 2.236 | Acc: 25.762% (1995/7744)\n",
      "Loss: 2.232 | Acc: 25.884% (2021/7808)\n",
      "Loss: 2.229 | Acc: 25.965% (2044/7872)\n",
      "Loss: 2.226 | Acc: 26.021% (2065/7936)\n",
      "Loss: 2.223 | Acc: 26.100% (2088/8000)\n",
      "Loss: 2.220 | Acc: 26.091% (2104/8064)\n",
      "Loss: 2.216 | Acc: 26.144% (2125/8128)\n",
      "Loss: 2.213 | Acc: 26.221% (2148/8192)\n",
      "Loss: 2.208 | Acc: 26.332% (2174/8256)\n",
      "Loss: 2.205 | Acc: 26.418% (2198/8320)\n",
      "Loss: 2.203 | Acc: 26.431% (2216/8384)\n",
      "Loss: 2.199 | Acc: 26.491% (2238/8448)\n",
      "Loss: 2.196 | Acc: 26.480% (2254/8512)\n",
      "Loss: 2.193 | Acc: 26.539% (2276/8576)\n",
      "Loss: 2.190 | Acc: 26.574% (2296/8640)\n",
      "Loss: 2.188 | Acc: 26.540% (2310/8704)\n",
      "Loss: 2.186 | Acc: 26.608% (2333/8768)\n",
      "Loss: 2.182 | Acc: 26.687% (2357/8832)\n",
      "Loss: 2.180 | Acc: 26.664% (2372/8896)\n",
      "Loss: 2.176 | Acc: 26.752% (2397/8960)\n",
      "Loss: 2.174 | Acc: 26.806% (2419/9024)\n",
      "Loss: 2.172 | Acc: 26.838% (2439/9088)\n",
      "Loss: 2.170 | Acc: 26.847% (2457/9152)\n",
      "Loss: 2.168 | Acc: 26.888% (2478/9216)\n",
      "Loss: 2.165 | Acc: 26.940% (2500/9280)\n",
      "Loss: 2.161 | Acc: 27.033% (2526/9344)\n",
      "Loss: 2.159 | Acc: 27.105% (2550/9408)\n",
      "Loss: 2.158 | Acc: 27.143% (2571/9472)\n",
      "Loss: 2.156 | Acc: 27.181% (2592/9536)\n",
      "Loss: 2.154 | Acc: 27.229% (2614/9600)\n",
      "Loss: 2.152 | Acc: 27.225% (2631/9664)\n",
      "Loss: 2.151 | Acc: 27.303% (2656/9728)\n",
      "Loss: 2.150 | Acc: 27.318% (2675/9792)\n",
      "Loss: 2.148 | Acc: 27.405% (2701/9856)\n",
      "Loss: 2.146 | Acc: 27.490% (2727/9920)\n",
      "Loss: 2.145 | Acc: 27.494% (2745/9984)\n",
      "Loss: 2.144 | Acc: 27.488% (2762/10048)\n",
      "Loss: 2.142 | Acc: 27.522% (2783/10112)\n",
      "Loss: 2.140 | Acc: 27.526% (2801/10176)\n",
      "Loss: 2.138 | Acc: 27.568% (2823/10240)\n",
      "Loss: 2.136 | Acc: 27.611% (2845/10304)\n",
      "Loss: 2.135 | Acc: 27.633% (2865/10368)\n",
      "Loss: 2.133 | Acc: 27.694% (2889/10432)\n",
      "Loss: 2.132 | Acc: 27.668% (2904/10496)\n",
      "Loss: 2.130 | Acc: 27.727% (2928/10560)\n",
      "Loss: 2.129 | Acc: 27.730% (2946/10624)\n",
      "Loss: 2.128 | Acc: 27.760% (2967/10688)\n",
      "Loss: 2.125 | Acc: 27.818% (2991/10752)\n",
      "Loss: 2.123 | Acc: 27.894% (3017/10816)\n",
      "Loss: 2.121 | Acc: 27.941% (3040/10880)\n",
      "Loss: 2.119 | Acc: 27.988% (3063/10944)\n",
      "Loss: 2.119 | Acc: 28.025% (3085/11008)\n",
      "Loss: 2.116 | Acc: 28.080% (3109/11072)\n",
      "Loss: 2.115 | Acc: 28.116% (3131/11136)\n",
      "Loss: 2.114 | Acc: 28.170% (3155/11200)\n",
      "Loss: 2.112 | Acc: 28.205% (3177/11264)\n",
      "Loss: 2.109 | Acc: 28.204% (3195/11328)\n",
      "Loss: 2.107 | Acc: 28.186% (3211/11392)\n",
      "Loss: 2.105 | Acc: 28.256% (3237/11456)\n",
      "Loss: 2.104 | Acc: 28.255% (3255/11520)\n",
      "Loss: 2.102 | Acc: 28.324% (3281/11584)\n",
      "Loss: 2.101 | Acc: 28.348% (3302/11648)\n",
      "Loss: 2.100 | Acc: 28.356% (3321/11712)\n",
      "Loss: 2.097 | Acc: 28.448% (3350/11776)\n",
      "Loss: 2.095 | Acc: 28.454% (3369/11840)\n",
      "Loss: 2.094 | Acc: 28.478% (3390/11904)\n",
      "Loss: 2.091 | Acc: 28.593% (3422/11968)\n",
      "Loss: 2.089 | Acc: 28.665% (3449/12032)\n",
      "Loss: 2.088 | Acc: 28.695% (3471/12096)\n",
      "Loss: 2.086 | Acc: 28.717% (3492/12160)\n",
      "Loss: 2.085 | Acc: 28.706% (3509/12224)\n",
      "Loss: 2.085 | Acc: 28.687% (3525/12288)\n",
      "Loss: 2.083 | Acc: 28.716% (3547/12352)\n",
      "Loss: 2.081 | Acc: 28.777% (3573/12416)\n",
      "Loss: 2.080 | Acc: 28.766% (3590/12480)\n",
      "Loss: 2.078 | Acc: 28.819% (3615/12544)\n",
      "Loss: 2.075 | Acc: 28.878% (3641/12608)\n",
      "Loss: 2.074 | Acc: 28.914% (3664/12672)\n",
      "Loss: 2.072 | Acc: 28.918% (3683/12736)\n",
      "Loss: 2.070 | Acc: 28.977% (3709/12800)\n",
      "Loss: 2.069 | Acc: 29.050% (3737/12864)\n",
      "Loss: 2.067 | Acc: 29.076% (3759/12928)\n",
      "Loss: 2.065 | Acc: 29.118% (3783/12992)\n",
      "Loss: 2.064 | Acc: 29.098% (3799/13056)\n",
      "Loss: 2.063 | Acc: 29.131% (3822/13120)\n",
      "Loss: 2.060 | Acc: 29.232% (3854/13184)\n",
      "Loss: 2.059 | Acc: 29.242% (3874/13248)\n",
      "Loss: 2.057 | Acc: 29.304% (3901/13312)\n",
      "Loss: 2.056 | Acc: 29.344% (3925/13376)\n",
      "Loss: 2.055 | Acc: 29.368% (3947/13440)\n",
      "Loss: 2.054 | Acc: 29.414% (3972/13504)\n",
      "Loss: 2.053 | Acc: 29.444% (3995/13568)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.050 | Acc: 29.541% (4027/13632)\n",
      "Loss: 2.047 | Acc: 29.644% (4060/13696)\n",
      "Loss: 2.045 | Acc: 29.680% (4084/13760)\n",
      "Loss: 2.044 | Acc: 29.731% (4110/13824)\n",
      "Loss: 2.042 | Acc: 29.788% (4137/13888)\n",
      "Loss: 2.041 | Acc: 29.824% (4161/13952)\n",
      "Loss: 2.040 | Acc: 29.852% (4184/14016)\n",
      "Loss: 2.039 | Acc: 29.865% (4205/14080)\n",
      "Loss: 2.037 | Acc: 29.900% (4229/14144)\n",
      "Loss: 2.035 | Acc: 29.934% (4253/14208)\n",
      "Loss: 2.035 | Acc: 29.933% (4272/14272)\n",
      "Loss: 2.033 | Acc: 30.015% (4303/14336)\n",
      "Loss: 2.031 | Acc: 30.028% (4324/14400)\n",
      "Loss: 2.029 | Acc: 30.088% (4352/14464)\n",
      "Loss: 2.028 | Acc: 30.094% (4372/14528)\n",
      "Loss: 2.027 | Acc: 30.140% (4398/14592)\n",
      "Loss: 2.025 | Acc: 30.151% (4419/14656)\n",
      "Loss: 2.023 | Acc: 30.183% (4443/14720)\n",
      "Loss: 2.022 | Acc: 30.249% (4472/14784)\n",
      "Loss: 2.021 | Acc: 30.260% (4493/14848)\n",
      "Loss: 2.018 | Acc: 30.291% (4517/14912)\n",
      "Loss: 2.017 | Acc: 30.315% (4540/14976)\n",
      "Loss: 2.016 | Acc: 30.326% (4561/15040)\n",
      "Loss: 2.015 | Acc: 30.369% (4587/15104)\n",
      "Loss: 2.014 | Acc: 30.380% (4608/15168)\n",
      "Loss: 2.012 | Acc: 30.462% (4640/15232)\n",
      "Loss: 2.010 | Acc: 30.524% (4669/15296)\n",
      "Loss: 2.009 | Acc: 30.579% (4697/15360)\n",
      "Loss: 2.008 | Acc: 30.608% (4721/15424)\n",
      "Loss: 2.006 | Acc: 30.643% (4746/15488)\n",
      "Loss: 2.005 | Acc: 30.652% (4767/15552)\n",
      "Loss: 2.004 | Acc: 30.712% (4796/15616)\n",
      "Loss: 2.003 | Acc: 30.778% (4826/15680)\n",
      "Loss: 2.001 | Acc: 30.837% (4855/15744)\n",
      "Loss: 2.000 | Acc: 30.832% (4874/15808)\n",
      "Loss: 1.999 | Acc: 30.891% (4903/15872)\n",
      "Loss: 1.998 | Acc: 30.905% (4925/15936)\n",
      "Loss: 1.997 | Acc: 30.950% (4952/16000)\n",
      "Loss: 1.996 | Acc: 30.970% (4975/16064)\n",
      "Loss: 1.993 | Acc: 31.045% (5007/16128)\n",
      "Loss: 1.991 | Acc: 31.114% (5038/16192)\n",
      "Loss: 1.991 | Acc: 31.121% (5059/16256)\n",
      "Loss: 1.990 | Acc: 31.170% (5087/16320)\n",
      "Loss: 1.989 | Acc: 31.195% (5111/16384)\n",
      "Loss: 1.990 | Acc: 31.189% (5130/16448)\n",
      "Loss: 1.988 | Acc: 31.232% (5157/16512)\n",
      "Loss: 1.987 | Acc: 31.250% (5180/16576)\n",
      "Loss: 1.986 | Acc: 31.292% (5207/16640)\n",
      "Loss: 1.984 | Acc: 31.352% (5237/16704)\n",
      "Loss: 1.983 | Acc: 31.363% (5259/16768)\n",
      "Loss: 1.983 | Acc: 31.381% (5282/16832)\n",
      "Loss: 1.981 | Acc: 31.410% (5307/16896)\n",
      "Loss: 1.981 | Acc: 31.403% (5326/16960)\n",
      "Loss: 1.980 | Acc: 31.420% (5349/17024)\n",
      "Loss: 1.980 | Acc: 31.420% (5369/17088)\n",
      "Loss: 1.978 | Acc: 31.431% (5391/17152)\n",
      "Loss: 1.977 | Acc: 31.453% (5415/17216)\n",
      "Loss: 1.977 | Acc: 31.470% (5438/17280)\n",
      "Loss: 1.976 | Acc: 31.486% (5461/17344)\n",
      "Loss: 1.975 | Acc: 31.520% (5487/17408)\n",
      "Loss: 1.975 | Acc: 31.525% (5508/17472)\n",
      "Loss: 1.974 | Acc: 31.547% (5532/17536)\n",
      "Loss: 1.973 | Acc: 31.585% (5559/17600)\n",
      "Loss: 1.971 | Acc: 31.629% (5587/17664)\n",
      "Loss: 1.972 | Acc: 31.639% (5609/17728)\n",
      "Loss: 1.971 | Acc: 31.660% (5633/17792)\n",
      "Loss: 1.970 | Acc: 31.664% (5654/17856)\n",
      "Loss: 1.969 | Acc: 31.663% (5674/17920)\n",
      "Loss: 1.968 | Acc: 31.695% (5700/17984)\n",
      "Loss: 1.967 | Acc: 31.738% (5728/18048)\n",
      "Loss: 1.966 | Acc: 31.758% (5752/18112)\n",
      "Loss: 1.966 | Acc: 31.784% (5777/18176)\n",
      "Loss: 1.965 | Acc: 31.793% (5799/18240)\n",
      "Loss: 1.963 | Acc: 31.835% (5827/18304)\n",
      "Loss: 1.962 | Acc: 31.887% (5857/18368)\n",
      "Loss: 1.961 | Acc: 31.896% (5879/18432)\n",
      "Loss: 1.960 | Acc: 31.915% (5903/18496)\n",
      "Loss: 1.959 | Acc: 31.934% (5927/18560)\n",
      "Loss: 1.957 | Acc: 31.975% (5955/18624)\n",
      "Loss: 1.957 | Acc: 31.972% (5975/18688)\n",
      "Loss: 1.956 | Acc: 32.002% (6001/18752)\n",
      "Loss: 1.954 | Acc: 32.063% (6033/18816)\n",
      "Loss: 1.953 | Acc: 32.092% (6059/18880)\n",
      "Loss: 1.952 | Acc: 32.121% (6085/18944)\n",
      "Loss: 1.951 | Acc: 32.171% (6115/19008)\n",
      "Loss: 1.949 | Acc: 32.204% (6142/19072)\n",
      "Loss: 1.948 | Acc: 32.217% (6165/19136)\n",
      "Loss: 1.947 | Acc: 32.266% (6195/19200)\n",
      "Loss: 1.946 | Acc: 32.309% (6224/19264)\n",
      "Loss: 1.945 | Acc: 32.331% (6249/19328)\n",
      "Loss: 1.946 | Acc: 32.292% (6262/19392)\n",
      "Loss: 1.945 | Acc: 32.299% (6284/19456)\n",
      "Loss: 1.944 | Acc: 32.295% (6304/19520)\n",
      "Loss: 1.943 | Acc: 32.302% (6326/19584)\n",
      "Loss: 1.942 | Acc: 32.329% (6352/19648)\n",
      "Loss: 1.941 | Acc: 32.341% (6375/19712)\n",
      "Loss: 1.940 | Acc: 32.368% (6401/19776)\n",
      "Loss: 1.938 | Acc: 32.404% (6429/19840)\n",
      "Loss: 1.938 | Acc: 32.411% (6451/19904)\n",
      "Loss: 1.936 | Acc: 32.457% (6481/19968)\n",
      "Loss: 1.935 | Acc: 32.498% (6510/20032)\n",
      "Loss: 1.933 | Acc: 32.534% (6538/20096)\n",
      "Loss: 1.932 | Acc: 32.589% (6570/20160)\n",
      "Loss: 1.930 | Acc: 32.630% (6599/20224)\n",
      "Loss: 1.928 | Acc: 32.674% (6629/20288)\n",
      "Loss: 1.927 | Acc: 32.734% (6662/20352)\n",
      "Loss: 1.927 | Acc: 32.768% (6690/20416)\n",
      "Loss: 1.926 | Acc: 32.827% (6723/20480)\n",
      "Loss: 1.925 | Acc: 32.832% (6745/20544)\n",
      "Loss: 1.924 | Acc: 32.880% (6776/20608)\n",
      "Loss: 1.923 | Acc: 32.914% (6804/20672)\n",
      "Loss: 1.922 | Acc: 32.919% (6826/20736)\n",
      "Loss: 1.922 | Acc: 32.952% (6854/20800)\n",
      "Loss: 1.921 | Acc: 32.961% (6877/20864)\n",
      "Loss: 1.919 | Acc: 32.970% (6900/20928)\n",
      "Loss: 1.919 | Acc: 32.989% (6925/20992)\n",
      "Loss: 1.918 | Acc: 33.017% (6952/21056)\n",
      "Loss: 1.918 | Acc: 33.026% (6975/21120)\n",
      "Loss: 1.918 | Acc: 33.020% (6995/21184)\n",
      "Loss: 1.917 | Acc: 33.034% (7019/21248)\n",
      "Loss: 1.916 | Acc: 33.033% (7040/21312)\n",
      "Loss: 1.916 | Acc: 33.042% (7063/21376)\n",
      "Loss: 1.915 | Acc: 33.060% (7088/21440)\n",
      "Loss: 1.914 | Acc: 33.054% (7108/21504)\n",
      "Loss: 1.914 | Acc: 33.063% (7131/21568)\n",
      "Loss: 1.913 | Acc: 33.094% (7159/21632)\n",
      "Loss: 1.912 | Acc: 33.098% (7181/21696)\n",
      "Loss: 1.911 | Acc: 33.139% (7211/21760)\n",
      "Loss: 1.911 | Acc: 33.142% (7233/21824)\n",
      "Loss: 1.910 | Acc: 33.178% (7262/21888)\n",
      "Loss: 1.909 | Acc: 33.177% (7283/21952)\n",
      "Loss: 1.908 | Acc: 33.208% (7311/22016)\n",
      "Loss: 1.906 | Acc: 33.297% (7352/22080)\n",
      "Loss: 1.905 | Acc: 33.323% (7379/22144)\n",
      "Loss: 1.904 | Acc: 33.348% (7406/22208)\n",
      "Loss: 1.903 | Acc: 33.365% (7431/22272)\n",
      "Loss: 1.903 | Acc: 33.372% (7454/22336)\n",
      "Loss: 1.902 | Acc: 33.415% (7485/22400)\n",
      "Loss: 1.901 | Acc: 33.405% (7504/22464)\n",
      "Loss: 1.900 | Acc: 33.465% (7539/22528)\n",
      "Loss: 1.899 | Acc: 33.481% (7564/22592)\n",
      "Loss: 1.899 | Acc: 33.475% (7584/22656)\n",
      "Loss: 1.898 | Acc: 33.508% (7613/22720)\n",
      "Loss: 1.897 | Acc: 33.524% (7638/22784)\n",
      "Loss: 1.896 | Acc: 33.508% (7656/22848)\n",
      "Loss: 1.895 | Acc: 33.515% (7679/22912)\n",
      "Loss: 1.895 | Acc: 33.522% (7702/22976)\n",
      "Loss: 1.894 | Acc: 33.537% (7727/23040)\n",
      "Loss: 1.893 | Acc: 33.553% (7752/23104)\n",
      "Loss: 1.893 | Acc: 33.564% (7776/23168)\n",
      "Loss: 1.892 | Acc: 33.566% (7798/23232)\n",
      "Loss: 1.891 | Acc: 33.598% (7827/23296)\n",
      "Loss: 1.891 | Acc: 33.596% (7848/23360)\n",
      "Loss: 1.889 | Acc: 33.649% (7882/23424)\n",
      "Loss: 1.888 | Acc: 33.677% (7910/23488)\n",
      "Loss: 1.887 | Acc: 33.696% (7936/23552)\n",
      "Loss: 1.887 | Acc: 33.685% (7955/23616)\n",
      "Loss: 1.886 | Acc: 33.691% (7978/23680)\n",
      "Loss: 1.885 | Acc: 33.743% (8012/23744)\n",
      "Loss: 1.884 | Acc: 33.774% (8041/23808)\n",
      "Loss: 1.884 | Acc: 33.793% (8067/23872)\n",
      "Loss: 1.882 | Acc: 33.828% (8097/23936)\n",
      "Loss: 1.881 | Acc: 33.862% (8127/24000)\n",
      "Loss: 1.881 | Acc: 33.864% (8149/24064)\n",
      "Loss: 1.881 | Acc: 33.907% (8181/24128)\n",
      "Loss: 1.881 | Acc: 33.891% (8199/24192)\n",
      "Loss: 1.880 | Acc: 33.913% (8226/24256)\n",
      "Loss: 1.879 | Acc: 33.923% (8250/24320)\n",
      "Loss: 1.879 | Acc: 33.940% (8276/24384)\n",
      "Loss: 1.878 | Acc: 33.982% (8308/24448)\n",
      "Loss: 1.877 | Acc: 33.996% (8333/24512)\n",
      "Loss: 1.877 | Acc: 34.017% (8360/24576)\n",
      "Loss: 1.876 | Acc: 34.038% (8387/24640)\n",
      "Loss: 1.876 | Acc: 34.051% (8412/24704)\n",
      "Loss: 1.875 | Acc: 34.056% (8435/24768)\n",
      "Loss: 1.875 | Acc: 34.053% (8456/24832)\n",
      "Loss: 1.874 | Acc: 34.094% (8488/24896)\n",
      "Loss: 1.874 | Acc: 34.107% (8513/24960)\n",
      "Loss: 1.873 | Acc: 34.123% (8539/25024)\n",
      "Loss: 1.872 | Acc: 34.136% (8564/25088)\n",
      "Loss: 1.872 | Acc: 34.144% (8588/25152)\n",
      "Loss: 1.872 | Acc: 34.137% (8608/25216)\n",
      "Loss: 1.871 | Acc: 34.173% (8639/25280)\n",
      "Loss: 1.871 | Acc: 34.197% (8667/25344)\n",
      "Loss: 1.870 | Acc: 34.229% (8697/25408)\n",
      "Loss: 1.869 | Acc: 34.269% (8729/25472)\n",
      "Loss: 1.868 | Acc: 34.281% (8754/25536)\n",
      "Loss: 1.867 | Acc: 34.316% (8785/25600)\n",
      "Loss: 1.866 | Acc: 34.344% (8814/25664)\n",
      "Loss: 1.865 | Acc: 34.387% (8847/25728)\n",
      "Loss: 1.865 | Acc: 34.387% (8869/25792)\n",
      "Loss: 1.864 | Acc: 34.398% (8894/25856)\n",
      "Loss: 1.864 | Acc: 34.410% (8919/25920)\n",
      "Loss: 1.864 | Acc: 34.433% (8947/25984)\n",
      "Loss: 1.864 | Acc: 34.440% (8971/26048)\n",
      "Loss: 1.862 | Acc: 34.471% (9001/26112)\n",
      "Loss: 1.862 | Acc: 34.467% (9022/26176)\n",
      "Loss: 1.862 | Acc: 34.470% (9045/26240)\n",
      "Loss: 1.861 | Acc: 34.485% (9071/26304)\n",
      "Loss: 1.860 | Acc: 34.549% (9110/26368)\n",
      "Loss: 1.860 | Acc: 34.526% (9126/26432)\n",
      "Loss: 1.859 | Acc: 34.530% (9149/26496)\n",
      "Loss: 1.859 | Acc: 34.518% (9168/26560)\n",
      "Loss: 1.859 | Acc: 34.525% (9192/26624)\n",
      "Loss: 1.858 | Acc: 34.544% (9219/26688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.857 | Acc: 34.566% (9247/26752)\n",
      "Loss: 1.856 | Acc: 34.588% (9275/26816)\n",
      "Loss: 1.856 | Acc: 34.587% (9297/26880)\n",
      "Loss: 1.855 | Acc: 34.601% (9323/26944)\n",
      "Loss: 1.854 | Acc: 34.623% (9351/27008)\n",
      "Loss: 1.854 | Acc: 34.652% (9381/27072)\n",
      "Loss: 1.853 | Acc: 34.681% (9411/27136)\n",
      "Loss: 1.852 | Acc: 34.695% (9437/27200)\n",
      "Loss: 1.852 | Acc: 34.712% (9464/27264)\n",
      "Loss: 1.851 | Acc: 34.734% (9492/27328)\n",
      "Loss: 1.851 | Acc: 34.733% (9514/27392)\n",
      "Loss: 1.850 | Acc: 34.765% (9545/27456)\n",
      "Loss: 1.849 | Acc: 34.775% (9570/27520)\n",
      "Loss: 1.849 | Acc: 34.781% (9594/27584)\n",
      "Loss: 1.849 | Acc: 34.795% (9620/27648)\n",
      "Loss: 1.848 | Acc: 34.822% (9650/27712)\n",
      "Loss: 1.847 | Acc: 34.847% (9679/27776)\n",
      "Loss: 1.846 | Acc: 34.881% (9711/27840)\n",
      "Loss: 1.846 | Acc: 34.898% (9738/27904)\n",
      "Loss: 1.845 | Acc: 34.911% (9764/27968)\n",
      "Loss: 1.845 | Acc: 34.907% (9785/28032)\n",
      "Loss: 1.845 | Acc: 34.909% (9808/28096)\n",
      "Loss: 1.845 | Acc: 34.922% (9834/28160)\n",
      "Loss: 1.844 | Acc: 34.931% (9859/28224)\n",
      "Loss: 1.844 | Acc: 34.930% (9881/28288)\n",
      "Loss: 1.843 | Acc: 34.964% (9913/28352)\n",
      "Loss: 1.842 | Acc: 35.001% (9946/28416)\n",
      "Loss: 1.842 | Acc: 35.014% (9972/28480)\n",
      "Loss: 1.842 | Acc: 35.002% (9991/28544)\n",
      "Loss: 1.841 | Acc: 35.015% (10017/28608)\n",
      "Loss: 1.841 | Acc: 35.038% (10046/28672)\n",
      "Loss: 1.840 | Acc: 35.071% (10078/28736)\n",
      "Loss: 1.840 | Acc: 35.066% (10099/28800)\n",
      "Loss: 1.839 | Acc: 35.054% (10118/28864)\n",
      "Loss: 1.838 | Acc: 35.087% (10150/28928)\n",
      "Loss: 1.838 | Acc: 35.099% (10176/28992)\n",
      "Loss: 1.838 | Acc: 35.115% (10203/29056)\n",
      "Loss: 1.837 | Acc: 35.137% (10232/29120)\n",
      "Loss: 1.837 | Acc: 35.125% (10251/29184)\n",
      "Loss: 1.836 | Acc: 35.151% (10281/29248)\n",
      "Loss: 1.835 | Acc: 35.190% (10315/29312)\n",
      "Loss: 1.835 | Acc: 35.199% (10340/29376)\n",
      "Loss: 1.835 | Acc: 35.200% (10363/29440)\n",
      "Loss: 1.834 | Acc: 35.236% (10396/29504)\n",
      "Loss: 1.833 | Acc: 35.248% (10422/29568)\n",
      "Loss: 1.833 | Acc: 35.242% (10443/29632)\n",
      "Loss: 1.832 | Acc: 35.257% (10470/29696)\n",
      "Loss: 1.832 | Acc: 35.262% (10494/29760)\n",
      "Loss: 1.831 | Acc: 35.294% (10526/29824)\n",
      "Loss: 1.830 | Acc: 35.312% (10554/29888)\n",
      "Loss: 1.830 | Acc: 35.313% (10577/29952)\n",
      "Loss: 1.829 | Acc: 35.328% (10604/30016)\n",
      "Loss: 1.828 | Acc: 35.339% (10630/30080)\n",
      "Loss: 1.828 | Acc: 35.354% (10657/30144)\n",
      "Loss: 1.828 | Acc: 35.348% (10678/30208)\n",
      "Loss: 1.827 | Acc: 35.379% (10710/30272)\n",
      "Loss: 1.826 | Acc: 35.413% (10743/30336)\n",
      "Loss: 1.825 | Acc: 35.447% (10776/30400)\n",
      "Loss: 1.825 | Acc: 35.475% (10807/30464)\n",
      "Loss: 1.824 | Acc: 35.485% (10833/30528)\n",
      "Loss: 1.824 | Acc: 35.486% (10856/30592)\n",
      "Loss: 1.824 | Acc: 35.504% (10884/30656)\n",
      "Loss: 1.823 | Acc: 35.514% (10910/30720)\n",
      "Loss: 1.822 | Acc: 35.505% (10930/30784)\n",
      "Loss: 1.822 | Acc: 35.513% (10955/30848)\n",
      "Loss: 1.821 | Acc: 35.543% (10987/30912)\n",
      "Loss: 1.821 | Acc: 35.566% (11017/30976)\n",
      "Loss: 1.820 | Acc: 35.577% (11043/31040)\n",
      "Loss: 1.820 | Acc: 35.581% (11067/31104)\n",
      "Loss: 1.820 | Acc: 35.578% (11089/31168)\n",
      "Loss: 1.819 | Acc: 35.611% (11122/31232)\n",
      "Loss: 1.818 | Acc: 35.640% (11154/31296)\n",
      "Loss: 1.817 | Acc: 35.666% (11185/31360)\n",
      "Loss: 1.817 | Acc: 35.661% (11206/31424)\n",
      "Loss: 1.817 | Acc: 35.671% (11232/31488)\n",
      "Loss: 1.817 | Acc: 35.659% (11251/31552)\n",
      "Loss: 1.816 | Acc: 35.669% (11277/31616)\n",
      "Loss: 1.816 | Acc: 35.663% (11298/31680)\n",
      "Loss: 1.815 | Acc: 35.676% (11325/31744)\n",
      "Loss: 1.814 | Acc: 35.683% (11350/31808)\n",
      "Loss: 1.814 | Acc: 35.708% (11381/31872)\n",
      "Loss: 1.813 | Acc: 35.721% (11408/31936)\n",
      "Loss: 1.813 | Acc: 35.737% (11436/32000)\n",
      "Loss: 1.812 | Acc: 35.782% (11473/32064)\n",
      "Loss: 1.811 | Acc: 35.791% (11499/32128)\n",
      "Loss: 1.810 | Acc: 35.819% (11531/32192)\n",
      "Loss: 1.810 | Acc: 35.854% (11565/32256)\n",
      "Loss: 1.809 | Acc: 35.897% (11602/32320)\n",
      "Loss: 1.808 | Acc: 35.904% (11627/32384)\n",
      "Loss: 1.808 | Acc: 35.922% (11656/32448)\n",
      "Loss: 1.808 | Acc: 35.931% (11682/32512)\n",
      "Loss: 1.808 | Acc: 35.928% (11704/32576)\n",
      "Loss: 1.807 | Acc: 35.941% (11731/32640)\n",
      "Loss: 1.807 | Acc: 35.934% (11752/32704)\n",
      "Loss: 1.806 | Acc: 35.947% (11779/32768)\n",
      "Loss: 1.806 | Acc: 35.953% (11804/32832)\n",
      "Loss: 1.806 | Acc: 35.956% (11828/32896)\n",
      "Loss: 1.805 | Acc: 35.947% (11848/32960)\n",
      "Loss: 1.805 | Acc: 35.953% (11873/33024)\n",
      "Loss: 1.804 | Acc: 35.968% (11901/33088)\n",
      "Loss: 1.804 | Acc: 35.971% (11925/33152)\n",
      "Loss: 1.803 | Acc: 35.998% (11957/33216)\n",
      "Loss: 1.803 | Acc: 36.007% (11983/33280)\n",
      "Loss: 1.802 | Acc: 36.021% (12011/33344)\n",
      "Loss: 1.802 | Acc: 36.027% (12036/33408)\n",
      "Loss: 1.801 | Acc: 36.033% (12061/33472)\n",
      "Loss: 1.801 | Acc: 36.027% (12082/33536)\n",
      "Loss: 1.801 | Acc: 36.045% (12111/33600)\n",
      "Loss: 1.800 | Acc: 36.068% (12142/33664)\n",
      "Loss: 1.799 | Acc: 36.077% (12168/33728)\n",
      "Loss: 1.799 | Acc: 36.103% (12200/33792)\n",
      "Loss: 1.798 | Acc: 36.129% (12232/33856)\n",
      "Loss: 1.798 | Acc: 36.150% (12262/33920)\n",
      "Loss: 1.797 | Acc: 36.143% (12283/33984)\n",
      "Loss: 1.797 | Acc: 36.131% (12302/34048)\n",
      "Loss: 1.796 | Acc: 36.160% (12335/34112)\n",
      "Loss: 1.796 | Acc: 36.172% (12362/34176)\n",
      "Loss: 1.796 | Acc: 36.174% (12386/34240)\n",
      "Loss: 1.796 | Acc: 36.177% (12410/34304)\n",
      "Loss: 1.795 | Acc: 36.182% (12435/34368)\n",
      "Loss: 1.795 | Acc: 36.196% (12463/34432)\n",
      "Loss: 1.794 | Acc: 36.207% (12490/34496)\n",
      "Loss: 1.793 | Acc: 36.236% (12523/34560)\n",
      "Loss: 1.793 | Acc: 36.244% (12549/34624)\n",
      "Loss: 1.792 | Acc: 36.249% (12574/34688)\n",
      "Loss: 1.792 | Acc: 36.271% (12605/34752)\n",
      "Loss: 1.791 | Acc: 36.294% (12636/34816)\n",
      "Loss: 1.791 | Acc: 36.319% (12668/34880)\n",
      "Loss: 1.791 | Acc: 36.312% (12689/34944)\n",
      "Loss: 1.790 | Acc: 36.320% (12715/35008)\n",
      "Loss: 1.790 | Acc: 36.325% (12740/35072)\n",
      "Loss: 1.790 | Acc: 36.347% (12771/35136)\n",
      "Loss: 1.790 | Acc: 36.352% (12796/35200)\n",
      "Loss: 1.789 | Acc: 36.383% (12830/35264)\n",
      "Loss: 1.789 | Acc: 36.396% (12858/35328)\n",
      "Loss: 1.789 | Acc: 36.407% (12885/35392)\n",
      "Loss: 1.789 | Acc: 36.397% (12905/35456)\n",
      "Loss: 1.788 | Acc: 36.416% (12935/35520)\n",
      "Loss: 1.787 | Acc: 36.452% (12971/35584)\n",
      "Loss: 1.787 | Acc: 36.470% (13001/35648)\n",
      "Loss: 1.786 | Acc: 36.500% (13035/35712)\n",
      "Loss: 1.786 | Acc: 36.516% (13064/35776)\n",
      "Loss: 1.785 | Acc: 36.549% (13099/35840)\n",
      "Loss: 1.785 | Acc: 36.553% (13124/35904)\n",
      "Loss: 1.785 | Acc: 36.577% (13156/35968)\n",
      "Loss: 1.784 | Acc: 36.606% (13190/36032)\n",
      "Loss: 1.784 | Acc: 36.605% (13213/36096)\n",
      "Loss: 1.783 | Acc: 36.607% (13237/36160)\n",
      "Loss: 1.783 | Acc: 36.603% (13259/36224)\n",
      "Loss: 1.782 | Acc: 36.629% (13292/36288)\n",
      "Loss: 1.782 | Acc: 36.647% (13322/36352)\n",
      "Loss: 1.781 | Acc: 36.662% (13351/36416)\n",
      "Loss: 1.781 | Acc: 36.678% (13380/36480)\n",
      "Loss: 1.781 | Acc: 36.695% (13410/36544)\n",
      "Loss: 1.780 | Acc: 36.705% (13437/36608)\n",
      "Loss: 1.780 | Acc: 36.723% (13467/36672)\n",
      "Loss: 1.779 | Acc: 36.741% (13497/36736)\n",
      "Loss: 1.779 | Acc: 36.772% (13532/36800)\n",
      "Loss: 1.778 | Acc: 36.770% (13555/36864)\n",
      "Loss: 1.778 | Acc: 36.780% (13582/36928)\n",
      "Loss: 1.778 | Acc: 36.784% (13607/36992)\n",
      "Loss: 1.777 | Acc: 36.809% (13640/37056)\n",
      "Loss: 1.777 | Acc: 36.810% (13664/37120)\n",
      "Loss: 1.776 | Acc: 36.828% (13694/37184)\n",
      "Loss: 1.776 | Acc: 36.834% (13720/37248)\n",
      "Loss: 1.776 | Acc: 36.849% (13749/37312)\n",
      "Loss: 1.776 | Acc: 36.853% (13774/37376)\n",
      "Loss: 1.775 | Acc: 36.846% (13795/37440)\n",
      "Loss: 1.775 | Acc: 36.857% (13823/37504)\n",
      "Loss: 1.774 | Acc: 36.858% (13847/37568)\n",
      "Loss: 1.774 | Acc: 36.881% (13879/37632)\n",
      "Loss: 1.773 | Acc: 36.903% (13911/37696)\n",
      "Loss: 1.773 | Acc: 36.931% (13945/37760)\n",
      "Loss: 1.772 | Acc: 36.947% (13975/37824)\n",
      "Loss: 1.772 | Acc: 36.954% (14001/37888)\n",
      "Loss: 1.772 | Acc: 36.965% (14029/37952)\n",
      "Loss: 1.771 | Acc: 36.979% (14058/38016)\n",
      "Loss: 1.771 | Acc: 37.012% (14094/38080)\n",
      "Loss: 1.770 | Acc: 37.018% (14120/38144)\n",
      "Loss: 1.770 | Acc: 37.018% (14144/38208)\n",
      "Loss: 1.770 | Acc: 37.019% (14168/38272)\n",
      "Loss: 1.769 | Acc: 37.028% (14195/38336)\n",
      "Loss: 1.768 | Acc: 37.049% (14227/38400)\n",
      "Loss: 1.768 | Acc: 37.055% (14253/38464)\n",
      "Loss: 1.767 | Acc: 37.080% (14286/38528)\n",
      "Loss: 1.767 | Acc: 37.088% (14313/38592)\n",
      "Loss: 1.767 | Acc: 37.107% (14344/38656)\n",
      "Loss: 1.767 | Acc: 37.118% (14372/38720)\n",
      "Loss: 1.766 | Acc: 37.126% (14399/38784)\n",
      "Loss: 1.766 | Acc: 37.142% (14429/38848)\n",
      "Loss: 1.766 | Acc: 37.166% (14462/38912)\n",
      "Loss: 1.765 | Acc: 37.184% (14493/38976)\n",
      "Loss: 1.765 | Acc: 37.182% (14516/39040)\n",
      "Loss: 1.765 | Acc: 37.191% (14543/39104)\n",
      "Loss: 1.764 | Acc: 37.206% (14573/39168)\n",
      "Loss: 1.764 | Acc: 37.204% (14596/39232)\n",
      "Loss: 1.763 | Acc: 37.210% (14622/39296)\n",
      "Loss: 1.763 | Acc: 37.223% (14651/39360)\n",
      "Loss: 1.763 | Acc: 37.226% (14676/39424)\n",
      "Loss: 1.762 | Acc: 37.229% (14701/39488)\n",
      "Loss: 1.762 | Acc: 37.222% (14722/39552)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.762 | Acc: 37.220% (14745/39616)\n",
      "Loss: 1.762 | Acc: 37.235% (14775/39680)\n",
      "Loss: 1.762 | Acc: 37.251% (14805/39744)\n",
      "Loss: 1.761 | Acc: 37.269% (14836/39808)\n",
      "Loss: 1.761 | Acc: 37.284% (14866/39872)\n",
      "Loss: 1.760 | Acc: 37.327% (14907/39936)\n",
      "Loss: 1.759 | Acc: 37.343% (14937/40000)\n",
      "Loss: 1.759 | Acc: 37.353% (14965/40064)\n",
      "Loss: 1.759 | Acc: 37.353% (14989/40128)\n",
      "Loss: 1.758 | Acc: 37.373% (15021/40192)\n",
      "Loss: 1.758 | Acc: 37.381% (15048/40256)\n",
      "Loss: 1.758 | Acc: 37.391% (15076/40320)\n",
      "Loss: 1.757 | Acc: 37.408% (15107/40384)\n",
      "Loss: 1.757 | Acc: 37.413% (15133/40448)\n",
      "Loss: 1.756 | Acc: 37.433% (15165/40512)\n",
      "Loss: 1.756 | Acc: 37.443% (15193/40576)\n",
      "Loss: 1.755 | Acc: 37.475% (15230/40640)\n",
      "Loss: 1.755 | Acc: 37.473% (15253/40704)\n",
      "Loss: 1.754 | Acc: 37.473% (15277/40768)\n",
      "Loss: 1.754 | Acc: 37.490% (15308/40832)\n",
      "Loss: 1.754 | Acc: 37.498% (15335/40896)\n",
      "Loss: 1.754 | Acc: 37.502% (15361/40960)\n",
      "Loss: 1.753 | Acc: 37.524% (15394/41024)\n",
      "Loss: 1.752 | Acc: 37.537% (15423/41088)\n",
      "Loss: 1.752 | Acc: 37.536% (15447/41152)\n",
      "Loss: 1.752 | Acc: 37.561% (15481/41216)\n",
      "Loss: 1.752 | Acc: 37.575% (15511/41280)\n",
      "Loss: 1.751 | Acc: 37.585% (15539/41344)\n",
      "Loss: 1.751 | Acc: 37.604% (15571/41408)\n",
      "Loss: 1.750 | Acc: 37.630% (15606/41472)\n",
      "Loss: 1.750 | Acc: 37.654% (15640/41536)\n",
      "Loss: 1.750 | Acc: 37.656% (15665/41600)\n",
      "Loss: 1.749 | Acc: 37.682% (15700/41664)\n",
      "Loss: 1.749 | Acc: 37.692% (15728/41728)\n",
      "Loss: 1.748 | Acc: 37.696% (15754/41792)\n",
      "Loss: 1.748 | Acc: 37.698% (15779/41856)\n",
      "Loss: 1.748 | Acc: 37.696% (15802/41920)\n",
      "Loss: 1.748 | Acc: 37.705% (15830/41984)\n",
      "Loss: 1.747 | Acc: 37.724% (15862/42048)\n",
      "Loss: 1.747 | Acc: 37.730% (15889/42112)\n",
      "Loss: 1.747 | Acc: 37.749% (15921/42176)\n",
      "Loss: 1.746 | Acc: 37.765% (15952/42240)\n",
      "Loss: 1.746 | Acc: 37.762% (15975/42304)\n",
      "Loss: 1.746 | Acc: 37.776% (16005/42368)\n",
      "Loss: 1.745 | Acc: 37.790% (16035/42432)\n",
      "Loss: 1.745 | Acc: 37.778% (16054/42496)\n",
      "Loss: 1.745 | Acc: 37.784% (16081/42560)\n",
      "Loss: 1.745 | Acc: 37.796% (16110/42624)\n",
      "Loss: 1.744 | Acc: 37.805% (16138/42688)\n",
      "Loss: 1.744 | Acc: 37.806% (16163/42752)\n",
      "Loss: 1.744 | Acc: 37.820% (16193/42816)\n",
      "Loss: 1.744 | Acc: 37.826% (16220/42880)\n",
      "Loss: 1.744 | Acc: 37.831% (16246/42944)\n",
      "Loss: 1.743 | Acc: 37.837% (16273/43008)\n",
      "Loss: 1.743 | Acc: 37.862% (16308/43072)\n",
      "Loss: 1.743 | Acc: 37.859% (16331/43136)\n",
      "Loss: 1.743 | Acc: 37.868% (16359/43200)\n",
      "Loss: 1.743 | Acc: 37.872% (16385/43264)\n",
      "Loss: 1.743 | Acc: 37.879% (16412/43328)\n",
      "Loss: 1.742 | Acc: 37.901% (16446/43392)\n",
      "Loss: 1.742 | Acc: 37.919% (16478/43456)\n",
      "Loss: 1.742 | Acc: 37.932% (16508/43520)\n",
      "Loss: 1.741 | Acc: 37.957% (16543/43584)\n",
      "Loss: 1.741 | Acc: 37.977% (16576/43648)\n",
      "Loss: 1.740 | Acc: 37.994% (16608/43712)\n",
      "Loss: 1.740 | Acc: 38.005% (16637/43776)\n",
      "Loss: 1.739 | Acc: 38.031% (16673/43840)\n",
      "Loss: 1.739 | Acc: 38.053% (16707/43904)\n",
      "Loss: 1.738 | Acc: 38.059% (16734/43968)\n",
      "Loss: 1.737 | Acc: 38.088% (16771/44032)\n",
      "Loss: 1.737 | Acc: 38.096% (16799/44096)\n",
      "Loss: 1.737 | Acc: 38.109% (16829/44160)\n",
      "Loss: 1.737 | Acc: 38.099% (16849/44224)\n",
      "Loss: 1.736 | Acc: 38.114% (16880/44288)\n",
      "Loss: 1.736 | Acc: 38.145% (16918/44352)\n",
      "Loss: 1.736 | Acc: 38.148% (16944/44416)\n",
      "Loss: 1.735 | Acc: 38.156% (16972/44480)\n",
      "Loss: 1.735 | Acc: 38.167% (17001/44544)\n",
      "Loss: 1.735 | Acc: 38.175% (17029/44608)\n",
      "Loss: 1.735 | Acc: 38.174% (17053/44672)\n",
      "Loss: 1.734 | Acc: 38.186% (17083/44736)\n",
      "Loss: 1.734 | Acc: 38.210% (17118/44800)\n",
      "Loss: 1.734 | Acc: 38.211% (17143/44864)\n",
      "Loss: 1.733 | Acc: 38.226% (17174/44928)\n",
      "Loss: 1.733 | Acc: 38.240% (17205/44992)\n",
      "Loss: 1.732 | Acc: 38.266% (17241/45056)\n",
      "Loss: 1.732 | Acc: 38.298% (17280/45120)\n",
      "Loss: 1.731 | Acc: 38.306% (17308/45184)\n",
      "Loss: 1.731 | Acc: 38.302% (17331/45248)\n",
      "Loss: 1.731 | Acc: 38.312% (17360/45312)\n",
      "Loss: 1.730 | Acc: 38.329% (17392/45376)\n",
      "Loss: 1.730 | Acc: 38.338% (17421/45440)\n",
      "Loss: 1.730 | Acc: 38.350% (17451/45504)\n",
      "Loss: 1.729 | Acc: 38.371% (17485/45568)\n",
      "Loss: 1.729 | Acc: 38.390% (17518/45632)\n",
      "Loss: 1.729 | Acc: 38.397% (17546/45696)\n",
      "Loss: 1.728 | Acc: 38.407% (17575/45760)\n",
      "Loss: 1.728 | Acc: 38.419% (17605/45824)\n",
      "Loss: 1.728 | Acc: 38.428% (17634/45888)\n",
      "Loss: 1.727 | Acc: 38.436% (17662/45952)\n",
      "Loss: 1.727 | Acc: 38.437% (17687/46016)\n",
      "Loss: 1.727 | Acc: 38.435% (17711/46080)\n",
      "Loss: 1.727 | Acc: 38.441% (17738/46144)\n",
      "Loss: 1.727 | Acc: 38.439% (17762/46208)\n",
      "Loss: 1.726 | Acc: 38.440% (17787/46272)\n",
      "Loss: 1.726 | Acc: 38.454% (17818/46336)\n",
      "Loss: 1.726 | Acc: 38.470% (17850/46400)\n",
      "Loss: 1.725 | Acc: 38.486% (17882/46464)\n",
      "Loss: 1.725 | Acc: 38.495% (17911/46528)\n",
      "Loss: 1.725 | Acc: 38.511% (17943/46592)\n",
      "Loss: 1.724 | Acc: 38.510% (17967/46656)\n",
      "Loss: 1.724 | Acc: 38.521% (17997/46720)\n",
      "Loss: 1.724 | Acc: 38.547% (18034/46784)\n",
      "Loss: 1.723 | Acc: 38.559% (18064/46848)\n",
      "Loss: 1.723 | Acc: 38.579% (18098/46912)\n",
      "Loss: 1.722 | Acc: 38.603% (18134/46976)\n",
      "Loss: 1.722 | Acc: 38.622% (18168/47040)\n",
      "Loss: 1.721 | Acc: 38.638% (18200/47104)\n",
      "Loss: 1.721 | Acc: 38.634% (18223/47168)\n",
      "Loss: 1.721 | Acc: 38.652% (18256/47232)\n",
      "Loss: 1.721 | Acc: 38.640% (18275/47296)\n",
      "Loss: 1.720 | Acc: 38.647% (18303/47360)\n",
      "Loss: 1.720 | Acc: 38.658% (18333/47424)\n",
      "Loss: 1.720 | Acc: 38.669% (18363/47488)\n",
      "Loss: 1.720 | Acc: 38.667% (18387/47552)\n",
      "Loss: 1.720 | Acc: 38.670% (18413/47616)\n",
      "Loss: 1.719 | Acc: 38.679% (18442/47680)\n",
      "Loss: 1.720 | Acc: 38.654% (18455/47744)\n",
      "Loss: 1.720 | Acc: 38.646% (18476/47808)\n",
      "Loss: 1.719 | Acc: 38.636% (18496/47872)\n",
      "Loss: 1.719 | Acc: 38.654% (18529/47936)\n",
      "Loss: 1.719 | Acc: 38.654% (18554/48000)\n",
      "Loss: 1.719 | Acc: 38.657% (18580/48064)\n",
      "Loss: 1.718 | Acc: 38.670% (18611/48128)\n",
      "Loss: 1.718 | Acc: 38.689% (18645/48192)\n",
      "Loss: 1.717 | Acc: 38.683% (18667/48256)\n",
      "Loss: 1.717 | Acc: 38.717% (18708/48320)\n",
      "Loss: 1.716 | Acc: 38.728% (18738/48384)\n",
      "Loss: 1.716 | Acc: 38.734% (18766/48448)\n",
      "Loss: 1.716 | Acc: 38.751% (18799/48512)\n",
      "Loss: 1.716 | Acc: 38.758% (18827/48576)\n",
      "Loss: 1.715 | Acc: 38.758% (18852/48640)\n",
      "Loss: 1.715 | Acc: 38.771% (18883/48704)\n",
      "Loss: 1.715 | Acc: 38.773% (18909/48768)\n",
      "Loss: 1.715 | Acc: 38.782% (18938/48832)\n",
      "Loss: 1.714 | Acc: 38.790% (18967/48896)\n",
      "Loss: 1.714 | Acc: 38.791% (18992/48960)\n",
      "Loss: 1.714 | Acc: 38.790% (19007/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 38.789795918367346\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.402 | Acc: 50.000% (32/64)\n",
      "Loss: 1.391 | Acc: 51.562% (66/128)\n",
      "Loss: 1.393 | Acc: 51.042% (98/192)\n",
      "Loss: 1.491 | Acc: 46.094% (118/256)\n",
      "Loss: 1.484 | Acc: 46.562% (149/320)\n",
      "Loss: 1.496 | Acc: 46.354% (178/384)\n",
      "Loss: 1.542 | Acc: 44.643% (200/448)\n",
      "Loss: 1.543 | Acc: 44.727% (229/512)\n",
      "Loss: 1.526 | Acc: 45.833% (264/576)\n",
      "Loss: 1.514 | Acc: 45.781% (293/640)\n",
      "Loss: 1.531 | Acc: 45.312% (319/704)\n",
      "Loss: 1.535 | Acc: 45.443% (349/768)\n",
      "Loss: 1.547 | Acc: 45.553% (379/832)\n",
      "Loss: 1.548 | Acc: 45.536% (408/896)\n",
      "Loss: 1.540 | Acc: 45.417% (436/960)\n",
      "Loss: 1.525 | Acc: 45.996% (471/1024)\n",
      "Loss: 1.527 | Acc: 46.048% (501/1088)\n",
      "Loss: 1.521 | Acc: 46.007% (530/1152)\n",
      "Loss: 1.517 | Acc: 46.135% (561/1216)\n",
      "Loss: 1.529 | Acc: 45.391% (581/1280)\n",
      "Loss: 1.529 | Acc: 45.164% (607/1344)\n",
      "Loss: 1.536 | Acc: 45.028% (634/1408)\n",
      "Loss: 1.524 | Acc: 45.516% (670/1472)\n",
      "Loss: 1.534 | Acc: 45.508% (699/1536)\n",
      "Loss: 1.538 | Acc: 45.188% (723/1600)\n",
      "Loss: 1.542 | Acc: 44.952% (748/1664)\n",
      "Loss: 1.540 | Acc: 45.197% (781/1728)\n",
      "Loss: 1.539 | Acc: 44.866% (804/1792)\n",
      "Loss: 1.540 | Acc: 44.828% (832/1856)\n",
      "Loss: 1.541 | Acc: 44.896% (862/1920)\n",
      "Loss: 1.546 | Acc: 44.607% (885/1984)\n",
      "Loss: 1.550 | Acc: 44.434% (910/2048)\n",
      "Loss: 1.545 | Acc: 44.366% (937/2112)\n",
      "Loss: 1.546 | Acc: 44.439% (967/2176)\n",
      "Loss: 1.546 | Acc: 44.420% (995/2240)\n",
      "Loss: 1.542 | Acc: 44.661% (1029/2304)\n",
      "Loss: 1.540 | Acc: 44.806% (1061/2368)\n",
      "Loss: 1.537 | Acc: 44.942% (1093/2432)\n",
      "Loss: 1.534 | Acc: 45.353% (1132/2496)\n",
      "Loss: 1.541 | Acc: 45.156% (1156/2560)\n",
      "Loss: 1.541 | Acc: 45.046% (1182/2624)\n",
      "Loss: 1.541 | Acc: 45.164% (1214/2688)\n",
      "Loss: 1.539 | Acc: 45.131% (1242/2752)\n",
      "Loss: 1.541 | Acc: 45.028% (1268/2816)\n",
      "Loss: 1.541 | Acc: 45.104% (1299/2880)\n",
      "Loss: 1.541 | Acc: 45.211% (1331/2944)\n",
      "Loss: 1.540 | Acc: 45.213% (1360/3008)\n",
      "Loss: 1.541 | Acc: 45.182% (1388/3072)\n",
      "Loss: 1.540 | Acc: 45.153% (1416/3136)\n",
      "Loss: 1.538 | Acc: 45.281% (1449/3200)\n",
      "Loss: 1.538 | Acc: 45.343% (1480/3264)\n",
      "Loss: 1.541 | Acc: 45.222% (1505/3328)\n",
      "Loss: 1.540 | Acc: 45.254% (1535/3392)\n",
      "Loss: 1.541 | Acc: 45.139% (1560/3456)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.541 | Acc: 45.085% (1587/3520)\n",
      "Loss: 1.538 | Acc: 45.117% (1617/3584)\n",
      "Loss: 1.539 | Acc: 44.901% (1638/3648)\n",
      "Loss: 1.537 | Acc: 44.881% (1666/3712)\n",
      "Loss: 1.538 | Acc: 44.889% (1695/3776)\n",
      "Loss: 1.539 | Acc: 44.922% (1725/3840)\n",
      "Loss: 1.539 | Acc: 44.928% (1754/3904)\n",
      "Loss: 1.540 | Acc: 44.859% (1780/3968)\n",
      "Loss: 1.540 | Acc: 44.866% (1809/4032)\n",
      "Loss: 1.547 | Acc: 44.727% (1832/4096)\n",
      "Loss: 1.549 | Acc: 44.591% (1855/4160)\n",
      "Loss: 1.547 | Acc: 44.673% (1887/4224)\n",
      "Loss: 1.548 | Acc: 44.566% (1911/4288)\n",
      "Loss: 1.545 | Acc: 44.692% (1945/4352)\n",
      "Loss: 1.543 | Acc: 44.769% (1977/4416)\n",
      "Loss: 1.540 | Acc: 44.866% (2010/4480)\n",
      "Loss: 1.541 | Acc: 44.784% (2035/4544)\n",
      "Loss: 1.541 | Acc: 44.683% (2059/4608)\n",
      "Loss: 1.541 | Acc: 44.628% (2085/4672)\n",
      "Loss: 1.539 | Acc: 44.595% (2112/4736)\n",
      "Loss: 1.539 | Acc: 44.667% (2144/4800)\n",
      "Loss: 1.539 | Acc: 44.593% (2169/4864)\n",
      "Loss: 1.540 | Acc: 44.602% (2198/4928)\n",
      "Loss: 1.541 | Acc: 44.571% (2225/4992)\n",
      "Loss: 1.542 | Acc: 44.541% (2252/5056)\n",
      "Loss: 1.543 | Acc: 44.453% (2276/5120)\n",
      "Loss: 1.542 | Acc: 44.579% (2311/5184)\n",
      "Loss: 1.544 | Acc: 44.455% (2333/5248)\n",
      "Loss: 1.542 | Acc: 44.597% (2369/5312)\n",
      "Loss: 1.545 | Acc: 44.513% (2393/5376)\n",
      "Loss: 1.545 | Acc: 44.540% (2423/5440)\n",
      "Loss: 1.546 | Acc: 44.586% (2454/5504)\n",
      "Loss: 1.548 | Acc: 44.558% (2481/5568)\n",
      "Loss: 1.551 | Acc: 44.496% (2506/5632)\n",
      "Loss: 1.554 | Acc: 44.452% (2532/5696)\n",
      "Loss: 1.552 | Acc: 44.462% (2561/5760)\n",
      "Loss: 1.550 | Acc: 44.488% (2591/5824)\n",
      "Loss: 1.550 | Acc: 44.514% (2621/5888)\n",
      "Loss: 1.550 | Acc: 44.523% (2650/5952)\n",
      "Loss: 1.549 | Acc: 44.531% (2679/6016)\n",
      "Loss: 1.549 | Acc: 44.507% (2706/6080)\n",
      "Loss: 1.549 | Acc: 44.596% (2740/6144)\n",
      "Loss: 1.551 | Acc: 44.523% (2764/6208)\n",
      "Loss: 1.552 | Acc: 44.467% (2789/6272)\n",
      "Loss: 1.550 | Acc: 44.539% (2822/6336)\n",
      "Loss: 1.550 | Acc: 44.516% (2849/6400)\n",
      "Loss: 1.551 | Acc: 44.524% (2878/6464)\n",
      "Loss: 1.550 | Acc: 44.531% (2907/6528)\n",
      "Loss: 1.551 | Acc: 44.463% (2931/6592)\n",
      "Loss: 1.550 | Acc: 44.426% (2957/6656)\n",
      "Loss: 1.549 | Acc: 44.405% (2984/6720)\n",
      "Loss: 1.549 | Acc: 44.354% (3009/6784)\n",
      "Loss: 1.547 | Acc: 44.480% (3046/6848)\n",
      "Loss: 1.548 | Acc: 44.444% (3072/6912)\n",
      "Loss: 1.549 | Acc: 44.409% (3098/6976)\n",
      "Loss: 1.550 | Acc: 44.318% (3120/7040)\n",
      "Loss: 1.550 | Acc: 44.327% (3149/7104)\n",
      "Loss: 1.551 | Acc: 44.294% (3175/7168)\n",
      "Loss: 1.553 | Acc: 44.192% (3196/7232)\n",
      "Loss: 1.552 | Acc: 44.175% (3223/7296)\n",
      "Loss: 1.550 | Acc: 44.266% (3258/7360)\n",
      "Loss: 1.552 | Acc: 44.248% (3285/7424)\n",
      "Loss: 1.551 | Acc: 44.284% (3316/7488)\n",
      "Loss: 1.550 | Acc: 44.346% (3349/7552)\n",
      "Loss: 1.551 | Acc: 44.288% (3373/7616)\n",
      "Loss: 1.551 | Acc: 44.284% (3401/7680)\n",
      "Loss: 1.548 | Acc: 44.383% (3437/7744)\n",
      "Loss: 1.549 | Acc: 44.352% (3463/7808)\n",
      "Loss: 1.550 | Acc: 44.360% (3492/7872)\n",
      "Loss: 1.550 | Acc: 44.367% (3521/7936)\n",
      "Loss: 1.551 | Acc: 44.325% (3546/8000)\n",
      "Loss: 1.549 | Acc: 44.296% (3572/8064)\n",
      "Loss: 1.551 | Acc: 44.242% (3596/8128)\n",
      "Loss: 1.550 | Acc: 44.287% (3628/8192)\n",
      "Loss: 1.550 | Acc: 44.247% (3653/8256)\n",
      "Loss: 1.553 | Acc: 44.123% (3671/8320)\n",
      "Loss: 1.554 | Acc: 44.144% (3701/8384)\n",
      "Loss: 1.554 | Acc: 44.117% (3727/8448)\n",
      "Loss: 1.554 | Acc: 44.067% (3751/8512)\n",
      "Loss: 1.554 | Acc: 44.135% (3785/8576)\n",
      "Loss: 1.554 | Acc: 44.155% (3815/8640)\n",
      "Loss: 1.555 | Acc: 44.083% (3837/8704)\n",
      "Loss: 1.556 | Acc: 43.990% (3857/8768)\n",
      "Loss: 1.555 | Acc: 44.010% (3887/8832)\n",
      "Loss: 1.554 | Acc: 44.065% (3920/8896)\n",
      "Loss: 1.553 | Acc: 44.141% (3955/8960)\n",
      "Loss: 1.554 | Acc: 44.182% (3987/9024)\n",
      "Loss: 1.554 | Acc: 44.157% (4013/9088)\n",
      "Loss: 1.555 | Acc: 44.154% (4041/9152)\n",
      "Loss: 1.553 | Acc: 44.195% (4073/9216)\n",
      "Loss: 1.552 | Acc: 44.192% (4101/9280)\n",
      "Loss: 1.552 | Acc: 44.178% (4128/9344)\n",
      "Loss: 1.553 | Acc: 44.207% (4159/9408)\n",
      "Loss: 1.554 | Acc: 44.204% (4187/9472)\n",
      "Loss: 1.552 | Acc: 44.274% (4222/9536)\n",
      "Loss: 1.552 | Acc: 44.292% (4252/9600)\n",
      "Loss: 1.551 | Acc: 44.257% (4277/9664)\n",
      "Loss: 1.551 | Acc: 44.285% (4308/9728)\n",
      "Loss: 1.552 | Acc: 44.199% (4328/9792)\n",
      "Loss: 1.552 | Acc: 44.207% (4357/9856)\n",
      "Loss: 1.552 | Acc: 44.173% (4382/9920)\n",
      "Loss: 1.552 | Acc: 44.131% (4406/9984)\n",
      "Loss: 1.552 | Acc: 44.140% (4414/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 44.14\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.716 | Acc: 39.062% (25/64)\n",
      "Loss: 1.670 | Acc: 40.625% (52/128)\n",
      "Loss: 1.571 | Acc: 44.271% (85/192)\n",
      "Loss: 1.483 | Acc: 46.875% (120/256)\n",
      "Loss: 1.453 | Acc: 47.500% (152/320)\n",
      "Loss: 1.422 | Acc: 48.698% (187/384)\n",
      "Loss: 1.459 | Acc: 46.875% (210/448)\n",
      "Loss: 1.467 | Acc: 46.680% (239/512)\n",
      "Loss: 1.474 | Acc: 47.396% (273/576)\n",
      "Loss: 1.471 | Acc: 47.500% (304/640)\n",
      "Loss: 1.449 | Acc: 48.011% (338/704)\n",
      "Loss: 1.461 | Acc: 47.656% (366/768)\n",
      "Loss: 1.466 | Acc: 46.995% (391/832)\n",
      "Loss: 1.478 | Acc: 46.540% (417/896)\n",
      "Loss: 1.472 | Acc: 46.562% (447/960)\n",
      "Loss: 1.467 | Acc: 46.680% (478/1024)\n",
      "Loss: 1.482 | Acc: 46.415% (505/1088)\n",
      "Loss: 1.479 | Acc: 46.615% (537/1152)\n",
      "Loss: 1.475 | Acc: 46.711% (568/1216)\n",
      "Loss: 1.468 | Acc: 46.797% (599/1280)\n",
      "Loss: 1.471 | Acc: 46.875% (630/1344)\n",
      "Loss: 1.475 | Acc: 46.733% (658/1408)\n",
      "Loss: 1.477 | Acc: 46.671% (687/1472)\n",
      "Loss: 1.481 | Acc: 46.549% (715/1536)\n",
      "Loss: 1.475 | Acc: 46.562% (745/1600)\n",
      "Loss: 1.471 | Acc: 46.695% (777/1664)\n",
      "Loss: 1.475 | Acc: 46.238% (799/1728)\n",
      "Loss: 1.475 | Acc: 46.317% (830/1792)\n",
      "Loss: 1.474 | Acc: 46.228% (858/1856)\n",
      "Loss: 1.469 | Acc: 46.406% (891/1920)\n",
      "Loss: 1.468 | Acc: 46.421% (921/1984)\n",
      "Loss: 1.463 | Acc: 46.777% (958/2048)\n",
      "Loss: 1.467 | Acc: 46.828% (989/2112)\n",
      "Loss: 1.466 | Acc: 46.875% (1020/2176)\n",
      "Loss: 1.464 | Acc: 46.920% (1051/2240)\n",
      "Loss: 1.470 | Acc: 46.875% (1080/2304)\n",
      "Loss: 1.470 | Acc: 46.875% (1110/2368)\n",
      "Loss: 1.470 | Acc: 46.957% (1142/2432)\n",
      "Loss: 1.473 | Acc: 46.915% (1171/2496)\n",
      "Loss: 1.473 | Acc: 46.875% (1200/2560)\n",
      "Loss: 1.471 | Acc: 46.989% (1233/2624)\n",
      "Loss: 1.472 | Acc: 46.875% (1260/2688)\n",
      "Loss: 1.471 | Acc: 47.166% (1298/2752)\n",
      "Loss: 1.471 | Acc: 47.266% (1331/2816)\n",
      "Loss: 1.466 | Acc: 47.431% (1366/2880)\n",
      "Loss: 1.463 | Acc: 47.418% (1396/2944)\n",
      "Loss: 1.460 | Acc: 47.673% (1434/3008)\n",
      "Loss: 1.457 | Acc: 47.493% (1459/3072)\n",
      "Loss: 1.455 | Acc: 47.577% (1492/3136)\n",
      "Loss: 1.453 | Acc: 47.688% (1526/3200)\n",
      "Loss: 1.453 | Acc: 47.855% (1562/3264)\n",
      "Loss: 1.454 | Acc: 47.656% (1586/3328)\n",
      "Loss: 1.454 | Acc: 47.730% (1619/3392)\n",
      "Loss: 1.456 | Acc: 47.598% (1645/3456)\n",
      "Loss: 1.455 | Acc: 47.642% (1677/3520)\n",
      "Loss: 1.453 | Acc: 47.656% (1708/3584)\n",
      "Loss: 1.451 | Acc: 47.697% (1740/3648)\n",
      "Loss: 1.446 | Acc: 47.764% (1773/3712)\n",
      "Loss: 1.443 | Acc: 47.828% (1806/3776)\n",
      "Loss: 1.445 | Acc: 47.839% (1837/3840)\n",
      "Loss: 1.448 | Acc: 47.618% (1859/3904)\n",
      "Loss: 1.447 | Acc: 47.681% (1892/3968)\n",
      "Loss: 1.450 | Acc: 47.817% (1928/4032)\n",
      "Loss: 1.448 | Acc: 47.778% (1957/4096)\n",
      "Loss: 1.448 | Acc: 47.788% (1988/4160)\n",
      "Loss: 1.449 | Acc: 47.822% (2020/4224)\n",
      "Loss: 1.450 | Acc: 47.831% (2051/4288)\n",
      "Loss: 1.450 | Acc: 47.955% (2087/4352)\n",
      "Loss: 1.446 | Acc: 48.143% (2126/4416)\n",
      "Loss: 1.446 | Acc: 48.013% (2151/4480)\n",
      "Loss: 1.446 | Acc: 47.953% (2179/4544)\n",
      "Loss: 1.453 | Acc: 47.808% (2203/4608)\n",
      "Loss: 1.451 | Acc: 47.795% (2233/4672)\n",
      "Loss: 1.455 | Acc: 47.614% (2255/4736)\n",
      "Loss: 1.456 | Acc: 47.708% (2290/4800)\n",
      "Loss: 1.459 | Acc: 47.656% (2318/4864)\n",
      "Loss: 1.457 | Acc: 47.666% (2349/4928)\n",
      "Loss: 1.459 | Acc: 47.616% (2377/4992)\n",
      "Loss: 1.457 | Acc: 47.686% (2411/5056)\n",
      "Loss: 1.456 | Acc: 47.695% (2442/5120)\n",
      "Loss: 1.456 | Acc: 47.666% (2471/5184)\n",
      "Loss: 1.456 | Acc: 47.694% (2503/5248)\n",
      "Loss: 1.459 | Acc: 47.590% (2528/5312)\n",
      "Loss: 1.460 | Acc: 47.656% (2562/5376)\n",
      "Loss: 1.459 | Acc: 47.684% (2594/5440)\n",
      "Loss: 1.460 | Acc: 47.620% (2621/5504)\n",
      "Loss: 1.460 | Acc: 47.575% (2649/5568)\n",
      "Loss: 1.463 | Acc: 47.425% (2671/5632)\n",
      "Loss: 1.462 | Acc: 47.437% (2702/5696)\n",
      "Loss: 1.460 | Acc: 47.396% (2730/5760)\n",
      "Loss: 1.460 | Acc: 47.356% (2758/5824)\n",
      "Loss: 1.463 | Acc: 47.300% (2785/5888)\n",
      "Loss: 1.463 | Acc: 47.177% (2808/5952)\n",
      "Loss: 1.464 | Acc: 47.108% (2834/6016)\n",
      "Loss: 1.462 | Acc: 47.188% (2869/6080)\n",
      "Loss: 1.460 | Acc: 47.217% (2901/6144)\n",
      "Loss: 1.459 | Acc: 47.213% (2931/6208)\n",
      "Loss: 1.461 | Acc: 47.114% (2955/6272)\n",
      "Loss: 1.462 | Acc: 47.064% (2982/6336)\n",
      "Loss: 1.467 | Acc: 46.906% (3002/6400)\n",
      "Loss: 1.468 | Acc: 46.829% (3027/6464)\n",
      "Loss: 1.467 | Acc: 46.906% (3062/6528)\n",
      "Loss: 1.465 | Acc: 46.951% (3095/6592)\n",
      "Loss: 1.466 | Acc: 46.980% (3127/6656)\n",
      "Loss: 1.465 | Acc: 47.009% (3159/6720)\n",
      "Loss: 1.465 | Acc: 47.008% (3189/6784)\n",
      "Loss: 1.466 | Acc: 47.006% (3219/6848)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.467 | Acc: 47.005% (3249/6912)\n",
      "Loss: 1.466 | Acc: 47.076% (3284/6976)\n",
      "Loss: 1.466 | Acc: 47.102% (3316/7040)\n",
      "Loss: 1.466 | Acc: 47.100% (3346/7104)\n",
      "Loss: 1.466 | Acc: 47.084% (3375/7168)\n",
      "Loss: 1.466 | Acc: 47.055% (3403/7232)\n",
      "Loss: 1.466 | Acc: 47.039% (3432/7296)\n",
      "Loss: 1.465 | Acc: 47.065% (3464/7360)\n",
      "Loss: 1.465 | Acc: 47.050% (3493/7424)\n",
      "Loss: 1.466 | Acc: 47.089% (3526/7488)\n",
      "Loss: 1.467 | Acc: 47.060% (3554/7552)\n",
      "Loss: 1.465 | Acc: 47.124% (3589/7616)\n",
      "Loss: 1.468 | Acc: 47.018% (3611/7680)\n",
      "Loss: 1.468 | Acc: 46.965% (3637/7744)\n",
      "Loss: 1.470 | Acc: 46.952% (3666/7808)\n",
      "Loss: 1.470 | Acc: 46.913% (3693/7872)\n",
      "Loss: 1.469 | Acc: 46.951% (3726/7936)\n",
      "Loss: 1.471 | Acc: 46.925% (3754/8000)\n",
      "Loss: 1.471 | Acc: 46.900% (3782/8064)\n",
      "Loss: 1.472 | Acc: 46.912% (3813/8128)\n",
      "Loss: 1.471 | Acc: 46.887% (3841/8192)\n",
      "Loss: 1.473 | Acc: 46.875% (3870/8256)\n",
      "Loss: 1.474 | Acc: 46.827% (3896/8320)\n",
      "Loss: 1.473 | Acc: 46.863% (3929/8384)\n",
      "Loss: 1.473 | Acc: 46.887% (3961/8448)\n",
      "Loss: 1.472 | Acc: 46.887% (3991/8512)\n",
      "Loss: 1.472 | Acc: 46.863% (4019/8576)\n",
      "Loss: 1.474 | Acc: 46.840% (4047/8640)\n",
      "Loss: 1.474 | Acc: 46.818% (4075/8704)\n",
      "Loss: 1.473 | Acc: 46.818% (4105/8768)\n",
      "Loss: 1.473 | Acc: 46.830% (4136/8832)\n",
      "Loss: 1.474 | Acc: 46.796% (4163/8896)\n",
      "Loss: 1.473 | Acc: 46.864% (4199/8960)\n",
      "Loss: 1.473 | Acc: 46.875% (4230/9024)\n",
      "Loss: 1.474 | Acc: 46.798% (4253/9088)\n",
      "Loss: 1.475 | Acc: 46.777% (4281/9152)\n",
      "Loss: 1.476 | Acc: 46.734% (4307/9216)\n",
      "Loss: 1.475 | Acc: 46.778% (4341/9280)\n",
      "Loss: 1.475 | Acc: 46.789% (4372/9344)\n",
      "Loss: 1.474 | Acc: 46.843% (4407/9408)\n",
      "Loss: 1.474 | Acc: 46.833% (4436/9472)\n",
      "Loss: 1.473 | Acc: 46.854% (4468/9536)\n",
      "Loss: 1.474 | Acc: 46.844% (4497/9600)\n",
      "Loss: 1.475 | Acc: 46.813% (4524/9664)\n",
      "Loss: 1.473 | Acc: 46.854% (4558/9728)\n",
      "Loss: 1.473 | Acc: 46.844% (4587/9792)\n",
      "Loss: 1.472 | Acc: 46.885% (4621/9856)\n",
      "Loss: 1.473 | Acc: 46.845% (4647/9920)\n",
      "Loss: 1.474 | Acc: 46.825% (4675/9984)\n",
      "Loss: 1.475 | Acc: 46.775% (4700/10048)\n",
      "Loss: 1.474 | Acc: 46.826% (4735/10112)\n",
      "Loss: 1.473 | Acc: 46.865% (4769/10176)\n",
      "Loss: 1.473 | Acc: 46.836% (4796/10240)\n",
      "Loss: 1.472 | Acc: 46.865% (4829/10304)\n",
      "Loss: 1.471 | Acc: 46.894% (4862/10368)\n",
      "Loss: 1.470 | Acc: 46.971% (4900/10432)\n",
      "Loss: 1.468 | Acc: 47.056% (4939/10496)\n",
      "Loss: 1.468 | Acc: 47.102% (4974/10560)\n",
      "Loss: 1.468 | Acc: 47.110% (5005/10624)\n",
      "Loss: 1.469 | Acc: 47.062% (5030/10688)\n",
      "Loss: 1.469 | Acc: 47.098% (5064/10752)\n",
      "Loss: 1.468 | Acc: 47.125% (5097/10816)\n",
      "Loss: 1.469 | Acc: 47.123% (5127/10880)\n",
      "Loss: 1.468 | Acc: 47.122% (5157/10944)\n",
      "Loss: 1.470 | Acc: 47.075% (5182/11008)\n",
      "Loss: 1.469 | Acc: 47.083% (5213/11072)\n",
      "Loss: 1.469 | Acc: 47.055% (5240/11136)\n",
      "Loss: 1.469 | Acc: 47.125% (5278/11200)\n",
      "Loss: 1.469 | Acc: 47.079% (5303/11264)\n",
      "Loss: 1.468 | Acc: 47.122% (5338/11328)\n",
      "Loss: 1.469 | Acc: 47.156% (5372/11392)\n",
      "Loss: 1.469 | Acc: 47.111% (5397/11456)\n",
      "Loss: 1.469 | Acc: 47.127% (5429/11520)\n",
      "Loss: 1.468 | Acc: 47.151% (5462/11584)\n",
      "Loss: 1.469 | Acc: 47.141% (5491/11648)\n",
      "Loss: 1.468 | Acc: 47.182% (5526/11712)\n",
      "Loss: 1.469 | Acc: 47.138% (5551/11776)\n",
      "Loss: 1.468 | Acc: 47.128% (5580/11840)\n",
      "Loss: 1.469 | Acc: 47.077% (5604/11904)\n",
      "Loss: 1.469 | Acc: 47.101% (5637/11968)\n",
      "Loss: 1.470 | Acc: 47.083% (5665/12032)\n",
      "Loss: 1.470 | Acc: 47.073% (5694/12096)\n",
      "Loss: 1.471 | Acc: 47.039% (5720/12160)\n",
      "Loss: 1.470 | Acc: 47.088% (5756/12224)\n",
      "Loss: 1.470 | Acc: 47.078% (5785/12288)\n",
      "Loss: 1.470 | Acc: 47.102% (5818/12352)\n",
      "Loss: 1.470 | Acc: 47.092% (5847/12416)\n",
      "Loss: 1.471 | Acc: 47.067% (5874/12480)\n",
      "Loss: 1.470 | Acc: 47.074% (5905/12544)\n",
      "Loss: 1.470 | Acc: 47.073% (5935/12608)\n",
      "Loss: 1.469 | Acc: 47.096% (5968/12672)\n",
      "Loss: 1.469 | Acc: 47.118% (6001/12736)\n",
      "Loss: 1.469 | Acc: 47.086% (6027/12800)\n",
      "Loss: 1.470 | Acc: 47.007% (6047/12864)\n",
      "Loss: 1.469 | Acc: 47.014% (6078/12928)\n",
      "Loss: 1.469 | Acc: 47.014% (6108/12992)\n",
      "Loss: 1.468 | Acc: 46.998% (6136/13056)\n",
      "Loss: 1.468 | Acc: 47.012% (6168/13120)\n",
      "Loss: 1.467 | Acc: 47.057% (6204/13184)\n",
      "Loss: 1.467 | Acc: 47.094% (6239/13248)\n",
      "Loss: 1.467 | Acc: 47.100% (6270/13312)\n",
      "Loss: 1.467 | Acc: 47.137% (6305/13376)\n",
      "Loss: 1.466 | Acc: 47.173% (6340/13440)\n",
      "Loss: 1.466 | Acc: 47.171% (6370/13504)\n",
      "Loss: 1.466 | Acc: 47.162% (6399/13568)\n",
      "Loss: 1.466 | Acc: 47.139% (6426/13632)\n",
      "Loss: 1.466 | Acc: 47.160% (6459/13696)\n",
      "Loss: 1.465 | Acc: 47.166% (6490/13760)\n",
      "Loss: 1.464 | Acc: 47.208% (6526/13824)\n",
      "Loss: 1.463 | Acc: 47.192% (6554/13888)\n",
      "Loss: 1.465 | Acc: 47.155% (6579/13952)\n",
      "Loss: 1.465 | Acc: 47.153% (6609/14016)\n",
      "Loss: 1.464 | Acc: 47.173% (6642/14080)\n",
      "Loss: 1.464 | Acc: 47.221% (6679/14144)\n",
      "Loss: 1.464 | Acc: 47.206% (6707/14208)\n",
      "Loss: 1.463 | Acc: 47.225% (6740/14272)\n",
      "Loss: 1.463 | Acc: 47.217% (6769/14336)\n",
      "Loss: 1.463 | Acc: 47.229% (6801/14400)\n",
      "Loss: 1.462 | Acc: 47.269% (6837/14464)\n",
      "Loss: 1.461 | Acc: 47.281% (6869/14528)\n",
      "Loss: 1.461 | Acc: 47.314% (6904/14592)\n",
      "Loss: 1.460 | Acc: 47.291% (6931/14656)\n",
      "Loss: 1.460 | Acc: 47.289% (6961/14720)\n",
      "Loss: 1.460 | Acc: 47.288% (6991/14784)\n",
      "Loss: 1.460 | Acc: 47.306% (7024/14848)\n",
      "Loss: 1.459 | Acc: 47.297% (7053/14912)\n",
      "Loss: 1.460 | Acc: 47.289% (7082/14976)\n",
      "Loss: 1.460 | Acc: 47.294% (7113/15040)\n",
      "Loss: 1.461 | Acc: 47.272% (7140/15104)\n",
      "Loss: 1.461 | Acc: 47.251% (7167/15168)\n",
      "Loss: 1.461 | Acc: 47.256% (7198/15232)\n",
      "Loss: 1.460 | Acc: 47.287% (7233/15296)\n",
      "Loss: 1.460 | Acc: 47.285% (7263/15360)\n",
      "Loss: 1.460 | Acc: 47.290% (7294/15424)\n",
      "Loss: 1.460 | Acc: 47.295% (7325/15488)\n",
      "Loss: 1.459 | Acc: 47.319% (7359/15552)\n",
      "Loss: 1.459 | Acc: 47.317% (7389/15616)\n",
      "Loss: 1.459 | Acc: 47.321% (7420/15680)\n",
      "Loss: 1.458 | Acc: 47.332% (7452/15744)\n",
      "Loss: 1.459 | Acc: 47.330% (7482/15808)\n",
      "Loss: 1.458 | Acc: 47.335% (7513/15872)\n",
      "Loss: 1.459 | Acc: 47.333% (7543/15936)\n",
      "Loss: 1.459 | Acc: 47.319% (7571/16000)\n",
      "Loss: 1.459 | Acc: 47.336% (7604/16064)\n",
      "Loss: 1.459 | Acc: 47.309% (7630/16128)\n",
      "Loss: 1.459 | Acc: 47.301% (7659/16192)\n",
      "Loss: 1.459 | Acc: 47.293% (7688/16256)\n",
      "Loss: 1.459 | Acc: 47.298% (7719/16320)\n",
      "Loss: 1.459 | Acc: 47.278% (7746/16384)\n",
      "Loss: 1.459 | Acc: 47.295% (7779/16448)\n",
      "Loss: 1.459 | Acc: 47.281% (7807/16512)\n",
      "Loss: 1.460 | Acc: 47.225% (7828/16576)\n",
      "Loss: 1.460 | Acc: 47.242% (7861/16640)\n",
      "Loss: 1.459 | Acc: 47.258% (7894/16704)\n",
      "Loss: 1.460 | Acc: 47.245% (7922/16768)\n",
      "Loss: 1.460 | Acc: 47.267% (7956/16832)\n",
      "Loss: 1.460 | Acc: 47.254% (7984/16896)\n",
      "Loss: 1.460 | Acc: 47.276% (8018/16960)\n",
      "Loss: 1.460 | Acc: 47.298% (8052/17024)\n",
      "Loss: 1.460 | Acc: 47.320% (8086/17088)\n",
      "Loss: 1.460 | Acc: 47.295% (8112/17152)\n",
      "Loss: 1.460 | Acc: 47.328% (8148/17216)\n",
      "Loss: 1.460 | Acc: 47.332% (8179/17280)\n",
      "Loss: 1.460 | Acc: 47.313% (8206/17344)\n",
      "Loss: 1.460 | Acc: 47.277% (8230/17408)\n",
      "Loss: 1.460 | Acc: 47.270% (8259/17472)\n",
      "Loss: 1.459 | Acc: 47.303% (8295/17536)\n",
      "Loss: 1.459 | Acc: 47.301% (8325/17600)\n",
      "Loss: 1.460 | Acc: 47.322% (8359/17664)\n",
      "Loss: 1.459 | Acc: 47.321% (8389/17728)\n",
      "Loss: 1.460 | Acc: 47.291% (8414/17792)\n",
      "Loss: 1.460 | Acc: 47.306% (8447/17856)\n",
      "Loss: 1.460 | Acc: 47.310% (8478/17920)\n",
      "Loss: 1.461 | Acc: 47.286% (8504/17984)\n",
      "Loss: 1.460 | Acc: 47.313% (8539/18048)\n",
      "Loss: 1.460 | Acc: 47.278% (8563/18112)\n",
      "Loss: 1.460 | Acc: 47.293% (8596/18176)\n",
      "Loss: 1.460 | Acc: 47.264% (8621/18240)\n",
      "Loss: 1.460 | Acc: 47.296% (8657/18304)\n",
      "Loss: 1.460 | Acc: 47.278% (8684/18368)\n",
      "Loss: 1.460 | Acc: 47.293% (8717/18432)\n",
      "Loss: 1.460 | Acc: 47.264% (8742/18496)\n",
      "Loss: 1.460 | Acc: 47.241% (8768/18560)\n",
      "Loss: 1.460 | Acc: 47.235% (8797/18624)\n",
      "Loss: 1.460 | Acc: 47.234% (8827/18688)\n",
      "Loss: 1.460 | Acc: 47.248% (8860/18752)\n",
      "Loss: 1.460 | Acc: 47.231% (8887/18816)\n",
      "Loss: 1.460 | Acc: 47.188% (8909/18880)\n",
      "Loss: 1.460 | Acc: 47.181% (8938/18944)\n",
      "Loss: 1.459 | Acc: 47.201% (8972/19008)\n",
      "Loss: 1.459 | Acc: 47.237% (9009/19072)\n",
      "Loss: 1.459 | Acc: 47.199% (9032/19136)\n",
      "Loss: 1.459 | Acc: 47.203% (9063/19200)\n",
      "Loss: 1.459 | Acc: 47.223% (9097/19264)\n",
      "Loss: 1.458 | Acc: 47.253% (9133/19328)\n",
      "Loss: 1.458 | Acc: 47.257% (9164/19392)\n",
      "Loss: 1.458 | Acc: 47.276% (9198/19456)\n",
      "Loss: 1.458 | Acc: 47.280% (9229/19520)\n",
      "Loss: 1.458 | Acc: 47.268% (9257/19584)\n",
      "Loss: 1.458 | Acc: 47.277% (9289/19648)\n",
      "Loss: 1.458 | Acc: 47.266% (9317/19712)\n",
      "Loss: 1.458 | Acc: 47.274% (9349/19776)\n",
      "Loss: 1.458 | Acc: 47.308% (9386/19840)\n",
      "Loss: 1.457 | Acc: 47.322% (9419/19904)\n",
      "Loss: 1.457 | Acc: 47.316% (9448/19968)\n",
      "Loss: 1.458 | Acc: 47.309% (9477/20032)\n",
      "Loss: 1.458 | Acc: 47.308% (9507/20096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.458 | Acc: 47.272% (9530/20160)\n",
      "Loss: 1.458 | Acc: 47.280% (9562/20224)\n",
      "Loss: 1.458 | Acc: 47.279% (9592/20288)\n",
      "Loss: 1.458 | Acc: 47.298% (9626/20352)\n",
      "Loss: 1.457 | Acc: 47.301% (9657/20416)\n",
      "Loss: 1.457 | Acc: 47.305% (9688/20480)\n",
      "Loss: 1.457 | Acc: 47.294% (9716/20544)\n",
      "Loss: 1.457 | Acc: 47.312% (9750/20608)\n",
      "Loss: 1.456 | Acc: 47.349% (9788/20672)\n",
      "Loss: 1.456 | Acc: 47.357% (9820/20736)\n",
      "Loss: 1.456 | Acc: 47.365% (9852/20800)\n",
      "Loss: 1.457 | Acc: 47.335% (9876/20864)\n",
      "Loss: 1.457 | Acc: 47.315% (9902/20928)\n",
      "Loss: 1.457 | Acc: 47.280% (9925/20992)\n",
      "Loss: 1.457 | Acc: 47.274% (9954/21056)\n",
      "Loss: 1.457 | Acc: 47.273% (9984/21120)\n",
      "Loss: 1.457 | Acc: 47.267% (10013/21184)\n",
      "Loss: 1.457 | Acc: 47.280% (10046/21248)\n",
      "Loss: 1.457 | Acc: 47.283% (10077/21312)\n",
      "Loss: 1.457 | Acc: 47.287% (10108/21376)\n",
      "Loss: 1.456 | Acc: 47.295% (10140/21440)\n",
      "Loss: 1.457 | Acc: 47.289% (10169/21504)\n",
      "Loss: 1.457 | Acc: 47.292% (10200/21568)\n",
      "Loss: 1.457 | Acc: 47.282% (10228/21632)\n",
      "Loss: 1.458 | Acc: 47.271% (10256/21696)\n",
      "Loss: 1.458 | Acc: 47.266% (10285/21760)\n",
      "Loss: 1.458 | Acc: 47.242% (10310/21824)\n",
      "Loss: 1.458 | Acc: 47.231% (10338/21888)\n",
      "Loss: 1.458 | Acc: 47.230% (10368/21952)\n",
      "Loss: 1.458 | Acc: 47.225% (10397/22016)\n",
      "Loss: 1.458 | Acc: 47.228% (10428/22080)\n",
      "Loss: 1.458 | Acc: 47.232% (10459/22144)\n",
      "Loss: 1.457 | Acc: 47.244% (10492/22208)\n",
      "Loss: 1.457 | Acc: 47.266% (10527/22272)\n",
      "Loss: 1.457 | Acc: 47.229% (10549/22336)\n",
      "Loss: 1.457 | Acc: 47.250% (10584/22400)\n",
      "Loss: 1.456 | Acc: 47.271% (10619/22464)\n",
      "Loss: 1.456 | Acc: 47.266% (10648/22528)\n",
      "Loss: 1.456 | Acc: 47.296% (10685/22592)\n",
      "Loss: 1.455 | Acc: 47.308% (10718/22656)\n",
      "Loss: 1.455 | Acc: 47.311% (10749/22720)\n",
      "Loss: 1.455 | Acc: 47.331% (10784/22784)\n",
      "Loss: 1.455 | Acc: 47.335% (10815/22848)\n",
      "Loss: 1.454 | Acc: 47.359% (10851/22912)\n",
      "Loss: 1.454 | Acc: 47.358% (10881/22976)\n",
      "Loss: 1.454 | Acc: 47.374% (10915/23040)\n",
      "Loss: 1.454 | Acc: 47.364% (10943/23104)\n",
      "Loss: 1.454 | Acc: 47.367% (10974/23168)\n",
      "Loss: 1.453 | Acc: 47.353% (11001/23232)\n",
      "Loss: 1.453 | Acc: 47.360% (11033/23296)\n",
      "Loss: 1.454 | Acc: 47.333% (11057/23360)\n",
      "Loss: 1.454 | Acc: 47.323% (11085/23424)\n",
      "Loss: 1.454 | Acc: 47.343% (11120/23488)\n",
      "Loss: 1.454 | Acc: 47.334% (11148/23552)\n",
      "Loss: 1.455 | Acc: 47.307% (11172/23616)\n",
      "Loss: 1.455 | Acc: 47.297% (11200/23680)\n",
      "Loss: 1.454 | Acc: 47.305% (11232/23744)\n",
      "Loss: 1.455 | Acc: 47.299% (11261/23808)\n",
      "Loss: 1.454 | Acc: 47.302% (11292/23872)\n",
      "Loss: 1.454 | Acc: 47.297% (11321/23936)\n",
      "Loss: 1.454 | Acc: 47.292% (11350/24000)\n",
      "Loss: 1.454 | Acc: 47.266% (11374/24064)\n",
      "Loss: 1.455 | Acc: 47.248% (11400/24128)\n",
      "Loss: 1.455 | Acc: 47.247% (11430/24192)\n",
      "Loss: 1.454 | Acc: 47.258% (11463/24256)\n",
      "Loss: 1.454 | Acc: 47.274% (11497/24320)\n",
      "Loss: 1.454 | Acc: 47.273% (11527/24384)\n",
      "Loss: 1.454 | Acc: 47.276% (11558/24448)\n",
      "Loss: 1.454 | Acc: 47.254% (11583/24512)\n",
      "Loss: 1.454 | Acc: 47.253% (11613/24576)\n",
      "Loss: 1.454 | Acc: 47.261% (11645/24640)\n",
      "Loss: 1.453 | Acc: 47.288% (11682/24704)\n",
      "Loss: 1.453 | Acc: 47.291% (11713/24768)\n",
      "Loss: 1.454 | Acc: 47.258% (11735/24832)\n",
      "Loss: 1.453 | Acc: 47.285% (11772/24896)\n",
      "Loss: 1.453 | Acc: 47.296% (11805/24960)\n",
      "Loss: 1.453 | Acc: 47.315% (11840/25024)\n",
      "Loss: 1.453 | Acc: 47.309% (11869/25088)\n",
      "Loss: 1.453 | Acc: 47.312% (11900/25152)\n",
      "Loss: 1.452 | Acc: 47.315% (11931/25216)\n",
      "Loss: 1.452 | Acc: 47.330% (11965/25280)\n",
      "Loss: 1.451 | Acc: 47.356% (12002/25344)\n",
      "Loss: 1.451 | Acc: 47.375% (12037/25408)\n",
      "Loss: 1.451 | Acc: 47.378% (12068/25472)\n",
      "Loss: 1.452 | Acc: 47.357% (12093/25536)\n",
      "Loss: 1.452 | Acc: 47.352% (12122/25600)\n",
      "Loss: 1.451 | Acc: 47.354% (12153/25664)\n",
      "Loss: 1.451 | Acc: 47.353% (12183/25728)\n",
      "Loss: 1.451 | Acc: 47.340% (12210/25792)\n",
      "Loss: 1.451 | Acc: 47.331% (12238/25856)\n",
      "Loss: 1.451 | Acc: 47.323% (12266/25920)\n",
      "Loss: 1.451 | Acc: 47.341% (12301/25984)\n",
      "Loss: 1.451 | Acc: 47.347% (12333/26048)\n",
      "Loss: 1.451 | Acc: 47.350% (12364/26112)\n",
      "Loss: 1.452 | Acc: 47.333% (12390/26176)\n",
      "Loss: 1.453 | Acc: 47.317% (12416/26240)\n",
      "Loss: 1.453 | Acc: 47.301% (12442/26304)\n",
      "Loss: 1.453 | Acc: 47.262% (12462/26368)\n",
      "Loss: 1.453 | Acc: 47.246% (12488/26432)\n",
      "Loss: 1.453 | Acc: 47.264% (12523/26496)\n",
      "Loss: 1.453 | Acc: 47.267% (12554/26560)\n",
      "Loss: 1.453 | Acc: 47.266% (12584/26624)\n",
      "Loss: 1.453 | Acc: 47.272% (12616/26688)\n",
      "Loss: 1.453 | Acc: 47.241% (12638/26752)\n",
      "Loss: 1.453 | Acc: 47.270% (12676/26816)\n",
      "Loss: 1.452 | Acc: 47.303% (12715/26880)\n",
      "Loss: 1.452 | Acc: 47.309% (12747/26944)\n",
      "Loss: 1.452 | Acc: 47.308% (12777/27008)\n",
      "Loss: 1.452 | Acc: 47.315% (12809/27072)\n",
      "Loss: 1.451 | Acc: 47.336% (12845/27136)\n",
      "Loss: 1.451 | Acc: 47.327% (12873/27200)\n",
      "Loss: 1.452 | Acc: 47.304% (12897/27264)\n",
      "Loss: 1.452 | Acc: 47.307% (12928/27328)\n",
      "Loss: 1.452 | Acc: 47.313% (12960/27392)\n",
      "Loss: 1.452 | Acc: 47.301% (12987/27456)\n",
      "Loss: 1.452 | Acc: 47.297% (13016/27520)\n",
      "Loss: 1.452 | Acc: 47.296% (13046/27584)\n",
      "Loss: 1.452 | Acc: 47.320% (13083/27648)\n",
      "Loss: 1.451 | Acc: 47.319% (13113/27712)\n",
      "Loss: 1.451 | Acc: 47.329% (13146/27776)\n",
      "Loss: 1.452 | Acc: 47.313% (13172/27840)\n",
      "Loss: 1.452 | Acc: 47.287% (13195/27904)\n",
      "Loss: 1.452 | Acc: 47.304% (13230/27968)\n",
      "Loss: 1.451 | Acc: 47.307% (13261/28032)\n",
      "Loss: 1.451 | Acc: 47.313% (13293/28096)\n",
      "Loss: 1.451 | Acc: 47.319% (13325/28160)\n",
      "Loss: 1.451 | Acc: 47.329% (13358/28224)\n",
      "Loss: 1.450 | Acc: 47.342% (13392/28288)\n",
      "Loss: 1.450 | Acc: 47.355% (13426/28352)\n",
      "Loss: 1.449 | Acc: 47.361% (13458/28416)\n",
      "Loss: 1.450 | Acc: 47.353% (13486/28480)\n",
      "Loss: 1.450 | Acc: 47.348% (13515/28544)\n",
      "Loss: 1.450 | Acc: 47.354% (13547/28608)\n",
      "Loss: 1.449 | Acc: 47.367% (13581/28672)\n",
      "Loss: 1.449 | Acc: 47.390% (13618/28736)\n",
      "Loss: 1.448 | Acc: 47.396% (13650/28800)\n",
      "Loss: 1.449 | Acc: 47.388% (13678/28864)\n",
      "Loss: 1.448 | Acc: 47.397% (13711/28928)\n",
      "Loss: 1.448 | Acc: 47.403% (13743/28992)\n",
      "Loss: 1.449 | Acc: 47.395% (13771/29056)\n",
      "Loss: 1.449 | Acc: 47.394% (13801/29120)\n",
      "Loss: 1.449 | Acc: 47.389% (13830/29184)\n",
      "Loss: 1.449 | Acc: 47.395% (13862/29248)\n",
      "Loss: 1.449 | Acc: 47.349% (13879/29312)\n",
      "Loss: 1.449 | Acc: 47.348% (13909/29376)\n",
      "Loss: 1.449 | Acc: 47.354% (13941/29440)\n",
      "Loss: 1.449 | Acc: 47.383% (13980/29504)\n",
      "Loss: 1.448 | Acc: 47.399% (14015/29568)\n",
      "Loss: 1.448 | Acc: 47.378% (14039/29632)\n",
      "Loss: 1.448 | Acc: 47.370% (14067/29696)\n",
      "Loss: 1.448 | Acc: 47.366% (14096/29760)\n",
      "Loss: 1.448 | Acc: 47.385% (14132/29824)\n",
      "Loss: 1.448 | Acc: 47.377% (14160/29888)\n",
      "Loss: 1.448 | Acc: 47.376% (14190/29952)\n",
      "Loss: 1.448 | Acc: 47.351% (14213/30016)\n",
      "Loss: 1.448 | Acc: 47.347% (14242/30080)\n",
      "Loss: 1.448 | Acc: 47.369% (14279/30144)\n",
      "Loss: 1.448 | Acc: 47.372% (14310/30208)\n",
      "Loss: 1.448 | Acc: 47.371% (14340/30272)\n",
      "Loss: 1.448 | Acc: 47.369% (14370/30336)\n",
      "Loss: 1.448 | Acc: 47.382% (14404/30400)\n",
      "Loss: 1.448 | Acc: 47.390% (14437/30464)\n",
      "Loss: 1.448 | Acc: 47.389% (14467/30528)\n",
      "Loss: 1.448 | Acc: 47.398% (14500/30592)\n",
      "Loss: 1.448 | Acc: 47.400% (14531/30656)\n",
      "Loss: 1.448 | Acc: 47.389% (14558/30720)\n",
      "Loss: 1.447 | Acc: 47.417% (14597/30784)\n",
      "Loss: 1.448 | Acc: 47.426% (14630/30848)\n",
      "Loss: 1.448 | Acc: 47.406% (14654/30912)\n",
      "Loss: 1.447 | Acc: 47.421% (14689/30976)\n",
      "Loss: 1.447 | Acc: 47.445% (14727/31040)\n",
      "Loss: 1.447 | Acc: 47.441% (14756/31104)\n",
      "Loss: 1.447 | Acc: 47.436% (14785/31168)\n",
      "Loss: 1.447 | Acc: 47.455% (14821/31232)\n",
      "Loss: 1.446 | Acc: 47.466% (14855/31296)\n",
      "Loss: 1.447 | Acc: 47.452% (14881/31360)\n",
      "Loss: 1.446 | Acc: 47.476% (14919/31424)\n",
      "Loss: 1.446 | Acc: 47.491% (14954/31488)\n",
      "Loss: 1.446 | Acc: 47.496% (14986/31552)\n",
      "Loss: 1.446 | Acc: 47.479% (15011/31616)\n",
      "Loss: 1.446 | Acc: 47.491% (15045/31680)\n",
      "Loss: 1.446 | Acc: 47.483% (15073/31744)\n",
      "Loss: 1.446 | Acc: 47.485% (15104/31808)\n",
      "Loss: 1.446 | Acc: 47.471% (15130/31872)\n",
      "Loss: 1.445 | Acc: 47.489% (15166/31936)\n",
      "Loss: 1.445 | Acc: 47.484% (15195/32000)\n",
      "Loss: 1.445 | Acc: 47.496% (15229/32064)\n",
      "Loss: 1.445 | Acc: 47.501% (15261/32128)\n",
      "Loss: 1.445 | Acc: 47.506% (15293/32192)\n",
      "Loss: 1.445 | Acc: 47.532% (15332/32256)\n",
      "Loss: 1.445 | Acc: 47.546% (15367/32320)\n",
      "Loss: 1.445 | Acc: 47.554% (15400/32384)\n",
      "Loss: 1.445 | Acc: 47.559% (15432/32448)\n",
      "Loss: 1.445 | Acc: 47.558% (15462/32512)\n",
      "Loss: 1.445 | Acc: 47.569% (15496/32576)\n",
      "Loss: 1.445 | Acc: 47.552% (15521/32640)\n",
      "Loss: 1.445 | Acc: 47.566% (15556/32704)\n",
      "Loss: 1.445 | Acc: 47.568% (15587/32768)\n",
      "Loss: 1.445 | Acc: 47.572% (15619/32832)\n",
      "Loss: 1.444 | Acc: 47.614% (15663/32896)\n",
      "Loss: 1.444 | Acc: 47.621% (15696/32960)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.444 | Acc: 47.623% (15727/33024)\n",
      "Loss: 1.444 | Acc: 47.625% (15758/33088)\n",
      "Loss: 1.444 | Acc: 47.632% (15791/33152)\n",
      "Loss: 1.444 | Acc: 47.634% (15822/33216)\n",
      "Loss: 1.444 | Acc: 47.638% (15854/33280)\n",
      "Loss: 1.445 | Acc: 47.628% (15881/33344)\n",
      "Loss: 1.445 | Acc: 47.626% (15911/33408)\n",
      "Loss: 1.445 | Acc: 47.625% (15941/33472)\n",
      "Loss: 1.444 | Acc: 47.617% (15969/33536)\n",
      "Loss: 1.444 | Acc: 47.607% (15996/33600)\n",
      "Loss: 1.445 | Acc: 47.579% (16017/33664)\n",
      "Loss: 1.445 | Acc: 47.578% (16047/33728)\n",
      "Loss: 1.445 | Acc: 47.579% (16078/33792)\n",
      "Loss: 1.445 | Acc: 47.584% (16110/33856)\n",
      "Loss: 1.444 | Acc: 47.591% (16143/33920)\n",
      "Loss: 1.445 | Acc: 47.587% (16172/33984)\n",
      "Loss: 1.444 | Acc: 47.589% (16203/34048)\n",
      "Loss: 1.444 | Acc: 47.602% (16238/34112)\n",
      "Loss: 1.444 | Acc: 47.589% (16264/34176)\n",
      "Loss: 1.444 | Acc: 47.596% (16297/34240)\n",
      "Loss: 1.444 | Acc: 47.604% (16330/34304)\n",
      "Loss: 1.445 | Acc: 47.588% (16355/34368)\n",
      "Loss: 1.444 | Acc: 47.589% (16386/34432)\n",
      "Loss: 1.444 | Acc: 47.597% (16419/34496)\n",
      "Loss: 1.444 | Acc: 47.601% (16451/34560)\n",
      "Loss: 1.444 | Acc: 47.588% (16477/34624)\n",
      "Loss: 1.444 | Acc: 47.599% (16511/34688)\n",
      "Loss: 1.444 | Acc: 47.600% (16542/34752)\n",
      "Loss: 1.444 | Acc: 47.573% (16563/34816)\n",
      "Loss: 1.444 | Acc: 47.600% (16603/34880)\n",
      "Loss: 1.444 | Acc: 47.610% (16637/34944)\n",
      "Loss: 1.444 | Acc: 47.609% (16667/35008)\n",
      "Loss: 1.444 | Acc: 47.608% (16697/35072)\n",
      "Loss: 1.444 | Acc: 47.601% (16725/35136)\n",
      "Loss: 1.444 | Acc: 47.599% (16755/35200)\n",
      "Loss: 1.444 | Acc: 47.598% (16785/35264)\n",
      "Loss: 1.444 | Acc: 47.614% (16821/35328)\n",
      "Loss: 1.443 | Acc: 47.629% (16857/35392)\n",
      "Loss: 1.443 | Acc: 47.637% (16890/35456)\n",
      "Loss: 1.443 | Acc: 47.635% (16920/35520)\n",
      "Loss: 1.443 | Acc: 47.642% (16953/35584)\n",
      "Loss: 1.444 | Acc: 47.630% (16979/35648)\n",
      "Loss: 1.444 | Acc: 47.628% (17009/35712)\n",
      "Loss: 1.444 | Acc: 47.619% (17036/35776)\n",
      "Loss: 1.444 | Acc: 47.620% (17067/35840)\n",
      "Loss: 1.443 | Acc: 47.610% (17094/35904)\n",
      "Loss: 1.443 | Acc: 47.628% (17131/35968)\n",
      "Loss: 1.443 | Acc: 47.644% (17167/36032)\n",
      "Loss: 1.443 | Acc: 47.642% (17197/36096)\n",
      "Loss: 1.443 | Acc: 47.658% (17233/36160)\n",
      "Loss: 1.443 | Acc: 47.656% (17263/36224)\n",
      "Loss: 1.443 | Acc: 47.663% (17296/36288)\n",
      "Loss: 1.443 | Acc: 47.678% (17332/36352)\n",
      "Loss: 1.443 | Acc: 47.669% (17359/36416)\n",
      "Loss: 1.442 | Acc: 47.678% (17393/36480)\n",
      "Loss: 1.442 | Acc: 47.674% (17422/36544)\n",
      "Loss: 1.443 | Acc: 47.664% (17449/36608)\n",
      "Loss: 1.443 | Acc: 47.652% (17475/36672)\n",
      "Loss: 1.443 | Acc: 47.651% (17505/36736)\n",
      "Loss: 1.443 | Acc: 47.660% (17539/36800)\n",
      "Loss: 1.443 | Acc: 47.654% (17567/36864)\n",
      "Loss: 1.443 | Acc: 47.647% (17595/36928)\n",
      "Loss: 1.443 | Acc: 47.654% (17628/36992)\n",
      "Loss: 1.442 | Acc: 47.666% (17663/37056)\n",
      "Loss: 1.442 | Acc: 47.670% (17695/37120)\n",
      "Loss: 1.442 | Acc: 47.676% (17728/37184)\n",
      "Loss: 1.443 | Acc: 47.672% (17757/37248)\n",
      "Loss: 1.443 | Acc: 47.666% (17785/37312)\n",
      "Loss: 1.443 | Acc: 47.656% (17812/37376)\n",
      "Loss: 1.443 | Acc: 47.644% (17838/37440)\n",
      "Loss: 1.443 | Acc: 47.662% (17875/37504)\n",
      "Loss: 1.442 | Acc: 47.676% (17911/37568)\n",
      "Loss: 1.443 | Acc: 47.667% (17938/37632)\n",
      "Loss: 1.443 | Acc: 47.660% (17966/37696)\n",
      "Loss: 1.443 | Acc: 47.662% (17997/37760)\n",
      "Loss: 1.443 | Acc: 47.660% (18027/37824)\n",
      "Loss: 1.443 | Acc: 47.651% (18054/37888)\n",
      "Loss: 1.443 | Acc: 47.658% (18087/37952)\n",
      "Loss: 1.442 | Acc: 47.672% (18123/38016)\n",
      "Loss: 1.443 | Acc: 47.663% (18150/38080)\n",
      "Loss: 1.442 | Acc: 47.680% (18187/38144)\n",
      "Loss: 1.442 | Acc: 47.689% (18221/38208)\n",
      "Loss: 1.442 | Acc: 47.682% (18249/38272)\n",
      "Loss: 1.443 | Acc: 47.665% (18273/38336)\n",
      "Loss: 1.443 | Acc: 47.677% (18308/38400)\n",
      "Loss: 1.442 | Acc: 47.671% (18336/38464)\n",
      "Loss: 1.443 | Acc: 47.648% (18358/38528)\n",
      "Loss: 1.443 | Acc: 47.658% (18392/38592)\n",
      "Loss: 1.443 | Acc: 47.638% (18415/38656)\n",
      "Loss: 1.443 | Acc: 47.621% (18439/38720)\n",
      "Loss: 1.443 | Acc: 47.623% (18470/38784)\n",
      "Loss: 1.444 | Acc: 47.603% (18493/38848)\n",
      "Loss: 1.444 | Acc: 47.607% (18525/38912)\n",
      "Loss: 1.444 | Acc: 47.599% (18552/38976)\n",
      "Loss: 1.444 | Acc: 47.582% (18576/39040)\n",
      "Loss: 1.444 | Acc: 47.594% (18611/39104)\n",
      "Loss: 1.444 | Acc: 47.608% (18647/39168)\n",
      "Loss: 1.444 | Acc: 47.617% (18681/39232)\n",
      "Loss: 1.444 | Acc: 47.598% (18704/39296)\n",
      "Loss: 1.444 | Acc: 47.591% (18732/39360)\n",
      "Loss: 1.444 | Acc: 47.600% (18766/39424)\n",
      "Loss: 1.444 | Acc: 47.602% (18797/39488)\n",
      "Loss: 1.444 | Acc: 47.578% (18818/39552)\n",
      "Loss: 1.444 | Acc: 47.594% (18855/39616)\n",
      "Loss: 1.443 | Acc: 47.606% (18890/39680)\n",
      "Loss: 1.443 | Acc: 47.612% (18923/39744)\n",
      "Loss: 1.443 | Acc: 47.624% (18958/39808)\n",
      "Loss: 1.443 | Acc: 47.635% (18993/39872)\n",
      "Loss: 1.443 | Acc: 47.651% (19030/39936)\n",
      "Loss: 1.443 | Acc: 47.638% (19055/40000)\n",
      "Loss: 1.442 | Acc: 47.649% (19090/40064)\n",
      "Loss: 1.442 | Acc: 47.643% (19118/40128)\n",
      "Loss: 1.443 | Acc: 47.626% (19142/40192)\n",
      "Loss: 1.443 | Acc: 47.610% (19166/40256)\n",
      "Loss: 1.443 | Acc: 47.619% (19200/40320)\n",
      "Loss: 1.443 | Acc: 47.630% (19235/40384)\n",
      "Loss: 1.443 | Acc: 47.627% (19264/40448)\n",
      "Loss: 1.443 | Acc: 47.625% (19294/40512)\n",
      "Loss: 1.443 | Acc: 47.624% (19324/40576)\n",
      "Loss: 1.442 | Acc: 47.613% (19350/40640)\n",
      "Loss: 1.442 | Acc: 47.610% (19379/40704)\n",
      "Loss: 1.442 | Acc: 47.608% (19409/40768)\n",
      "Loss: 1.442 | Acc: 47.610% (19440/40832)\n",
      "Loss: 1.442 | Acc: 47.613% (19472/40896)\n",
      "Loss: 1.442 | Acc: 47.625% (19507/40960)\n",
      "Loss: 1.441 | Acc: 47.628% (19539/41024)\n",
      "Loss: 1.441 | Acc: 47.629% (19570/41088)\n",
      "Loss: 1.441 | Acc: 47.628% (19600/41152)\n",
      "Loss: 1.441 | Acc: 47.632% (19632/41216)\n",
      "Loss: 1.441 | Acc: 47.633% (19663/41280)\n",
      "Loss: 1.441 | Acc: 47.620% (19688/41344)\n",
      "Loss: 1.442 | Acc: 47.604% (19712/41408)\n",
      "Loss: 1.441 | Acc: 47.625% (19751/41472)\n",
      "Loss: 1.441 | Acc: 47.633% (19785/41536)\n",
      "Loss: 1.441 | Acc: 47.623% (19811/41600)\n",
      "Loss: 1.441 | Acc: 47.621% (19841/41664)\n",
      "Loss: 1.441 | Acc: 47.620% (19871/41728)\n",
      "Loss: 1.442 | Acc: 47.622% (19902/41792)\n",
      "Loss: 1.442 | Acc: 47.618% (19931/41856)\n",
      "Loss: 1.442 | Acc: 47.612% (19959/41920)\n",
      "Loss: 1.442 | Acc: 47.611% (19989/41984)\n",
      "Loss: 1.442 | Acc: 47.612% (20020/42048)\n",
      "Loss: 1.441 | Acc: 47.618% (20053/42112)\n",
      "Loss: 1.441 | Acc: 47.615% (20082/42176)\n",
      "Loss: 1.441 | Acc: 47.618% (20114/42240)\n",
      "Loss: 1.441 | Acc: 47.615% (20143/42304)\n",
      "Loss: 1.441 | Acc: 47.611% (20172/42368)\n",
      "Loss: 1.441 | Acc: 47.615% (20204/42432)\n",
      "Loss: 1.441 | Acc: 47.616% (20235/42496)\n",
      "Loss: 1.441 | Acc: 47.606% (20261/42560)\n",
      "Loss: 1.441 | Acc: 47.600% (20289/42624)\n",
      "Loss: 1.441 | Acc: 47.592% (20316/42688)\n",
      "Loss: 1.441 | Acc: 47.593% (20347/42752)\n",
      "Loss: 1.441 | Acc: 47.604% (20382/42816)\n",
      "Loss: 1.440 | Acc: 47.617% (20418/42880)\n",
      "Loss: 1.440 | Acc: 47.613% (20447/42944)\n",
      "Loss: 1.440 | Acc: 47.617% (20479/43008)\n",
      "Loss: 1.440 | Acc: 47.630% (20515/43072)\n",
      "Loss: 1.440 | Acc: 47.626% (20544/43136)\n",
      "Loss: 1.440 | Acc: 47.634% (20578/43200)\n",
      "Loss: 1.440 | Acc: 47.638% (20610/43264)\n",
      "Loss: 1.440 | Acc: 47.639% (20641/43328)\n",
      "Loss: 1.440 | Acc: 47.652% (20677/43392)\n",
      "Loss: 1.440 | Acc: 47.664% (20713/43456)\n",
      "Loss: 1.440 | Acc: 47.652% (20738/43520)\n",
      "Loss: 1.440 | Acc: 47.641% (20764/43584)\n",
      "Loss: 1.440 | Acc: 47.652% (20799/43648)\n",
      "Loss: 1.440 | Acc: 47.648% (20828/43712)\n",
      "Loss: 1.440 | Acc: 47.649% (20859/43776)\n",
      "Loss: 1.440 | Acc: 47.648% (20889/43840)\n",
      "Loss: 1.440 | Acc: 47.645% (20918/43904)\n",
      "Loss: 1.440 | Acc: 47.655% (20953/43968)\n",
      "Loss: 1.440 | Acc: 47.654% (20983/44032)\n",
      "Loss: 1.439 | Acc: 47.657% (21015/44096)\n",
      "Loss: 1.439 | Acc: 47.654% (21044/44160)\n",
      "Loss: 1.439 | Acc: 47.660% (21077/44224)\n",
      "Loss: 1.439 | Acc: 47.672% (21113/44288)\n",
      "Loss: 1.439 | Acc: 47.675% (21145/44352)\n",
      "Loss: 1.438 | Acc: 47.677% (21176/44416)\n",
      "Loss: 1.438 | Acc: 47.691% (21213/44480)\n",
      "Loss: 1.438 | Acc: 47.692% (21244/44544)\n",
      "Loss: 1.438 | Acc: 47.698% (21277/44608)\n",
      "Loss: 1.437 | Acc: 47.723% (21319/44672)\n",
      "Loss: 1.437 | Acc: 47.720% (21348/44736)\n",
      "Loss: 1.437 | Acc: 47.723% (21380/44800)\n",
      "Loss: 1.437 | Acc: 47.711% (21405/44864)\n",
      "Loss: 1.437 | Acc: 47.734% (21446/44928)\n",
      "Loss: 1.436 | Acc: 47.742% (21480/44992)\n",
      "Loss: 1.436 | Acc: 47.752% (21515/45056)\n",
      "Loss: 1.437 | Acc: 47.750% (21545/45120)\n",
      "Loss: 1.436 | Acc: 47.747% (21574/45184)\n",
      "Loss: 1.437 | Acc: 47.739% (21601/45248)\n",
      "Loss: 1.437 | Acc: 47.747% (21635/45312)\n",
      "Loss: 1.437 | Acc: 47.746% (21665/45376)\n",
      "Loss: 1.437 | Acc: 47.735% (21691/45440)\n",
      "Loss: 1.437 | Acc: 47.739% (21723/45504)\n",
      "Loss: 1.437 | Acc: 47.742% (21755/45568)\n",
      "Loss: 1.436 | Acc: 47.763% (21795/45632)\n",
      "Loss: 1.436 | Acc: 47.768% (21828/45696)\n",
      "Loss: 1.436 | Acc: 47.756% (21853/45760)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.436 | Acc: 47.759% (21885/45824)\n",
      "Loss: 1.436 | Acc: 47.771% (21921/45888)\n",
      "Loss: 1.436 | Acc: 47.769% (21951/45952)\n",
      "Loss: 1.436 | Acc: 47.766% (21980/46016)\n",
      "Loss: 1.436 | Acc: 47.767% (22011/46080)\n",
      "Loss: 1.436 | Acc: 47.774% (22045/46144)\n",
      "Loss: 1.436 | Acc: 47.767% (22072/46208)\n",
      "Loss: 1.436 | Acc: 47.774% (22106/46272)\n",
      "Loss: 1.436 | Acc: 47.777% (22138/46336)\n",
      "Loss: 1.436 | Acc: 47.789% (22174/46400)\n",
      "Loss: 1.436 | Acc: 47.785% (22203/46464)\n",
      "Loss: 1.436 | Acc: 47.791% (22236/46528)\n",
      "Loss: 1.436 | Acc: 47.800% (22271/46592)\n",
      "Loss: 1.435 | Acc: 47.814% (22308/46656)\n",
      "Loss: 1.435 | Acc: 47.812% (22338/46720)\n",
      "Loss: 1.435 | Acc: 47.826% (22375/46784)\n",
      "Loss: 1.435 | Acc: 47.825% (22405/46848)\n",
      "Loss: 1.435 | Acc: 47.826% (22436/46912)\n",
      "Loss: 1.435 | Acc: 47.831% (22469/46976)\n",
      "Loss: 1.435 | Acc: 47.834% (22501/47040)\n",
      "Loss: 1.435 | Acc: 47.830% (22530/47104)\n",
      "Loss: 1.435 | Acc: 47.838% (22564/47168)\n",
      "Loss: 1.434 | Acc: 47.851% (22601/47232)\n",
      "Loss: 1.435 | Acc: 47.854% (22633/47296)\n",
      "Loss: 1.434 | Acc: 47.867% (22670/47360)\n",
      "Loss: 1.434 | Acc: 47.877% (22705/47424)\n",
      "Loss: 1.434 | Acc: 47.877% (22736/47488)\n",
      "Loss: 1.434 | Acc: 47.884% (22770/47552)\n",
      "Loss: 1.434 | Acc: 47.879% (22798/47616)\n",
      "Loss: 1.434 | Acc: 47.884% (22831/47680)\n",
      "Loss: 1.434 | Acc: 47.876% (22858/47744)\n",
      "Loss: 1.434 | Acc: 47.883% (22892/47808)\n",
      "Loss: 1.434 | Acc: 47.892% (22927/47872)\n",
      "Loss: 1.434 | Acc: 47.889% (22956/47936)\n",
      "Loss: 1.434 | Acc: 47.890% (22987/48000)\n",
      "Loss: 1.434 | Acc: 47.890% (23018/48064)\n",
      "Loss: 1.434 | Acc: 47.893% (23050/48128)\n",
      "Loss: 1.433 | Acc: 47.898% (23083/48192)\n",
      "Loss: 1.433 | Acc: 47.903% (23116/48256)\n",
      "Loss: 1.433 | Acc: 47.891% (23141/48320)\n",
      "Loss: 1.433 | Acc: 47.915% (23183/48384)\n",
      "Loss: 1.433 | Acc: 47.903% (23208/48448)\n",
      "Loss: 1.433 | Acc: 47.904% (23239/48512)\n",
      "Loss: 1.433 | Acc: 47.898% (23267/48576)\n",
      "Loss: 1.433 | Acc: 47.907% (23302/48640)\n",
      "Loss: 1.433 | Acc: 47.904% (23331/48704)\n",
      "Loss: 1.432 | Acc: 47.919% (23369/48768)\n",
      "Loss: 1.432 | Acc: 47.928% (23404/48832)\n",
      "Loss: 1.432 | Acc: 47.938% (23440/48896)\n",
      "Loss: 1.432 | Acc: 47.947% (23475/48960)\n",
      "Loss: 1.432 | Acc: 47.947% (23494/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 47.946938775510205\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.363 | Acc: 54.688% (35/64)\n",
      "Loss: 1.400 | Acc: 46.094% (59/128)\n",
      "Loss: 1.399 | Acc: 47.917% (92/192)\n",
      "Loss: 1.418 | Acc: 47.266% (121/256)\n",
      "Loss: 1.405 | Acc: 47.812% (153/320)\n",
      "Loss: 1.411 | Acc: 48.177% (185/384)\n",
      "Loss: 1.439 | Acc: 47.545% (213/448)\n",
      "Loss: 1.423 | Acc: 47.852% (245/512)\n",
      "Loss: 1.396 | Acc: 48.958% (282/576)\n",
      "Loss: 1.382 | Acc: 49.531% (317/640)\n",
      "Loss: 1.418 | Acc: 48.580% (342/704)\n",
      "Loss: 1.425 | Acc: 48.438% (372/768)\n",
      "Loss: 1.435 | Acc: 47.716% (397/832)\n",
      "Loss: 1.427 | Acc: 47.768% (428/896)\n",
      "Loss: 1.414 | Acc: 48.125% (462/960)\n",
      "Loss: 1.407 | Acc: 48.242% (494/1024)\n",
      "Loss: 1.417 | Acc: 47.702% (519/1088)\n",
      "Loss: 1.411 | Acc: 47.830% (551/1152)\n",
      "Loss: 1.416 | Acc: 47.697% (580/1216)\n",
      "Loss: 1.431 | Acc: 47.188% (604/1280)\n",
      "Loss: 1.425 | Acc: 47.173% (634/1344)\n",
      "Loss: 1.425 | Acc: 47.301% (666/1408)\n",
      "Loss: 1.419 | Acc: 47.486% (699/1472)\n",
      "Loss: 1.427 | Acc: 47.526% (730/1536)\n",
      "Loss: 1.427 | Acc: 47.375% (758/1600)\n",
      "Loss: 1.428 | Acc: 47.356% (788/1664)\n",
      "Loss: 1.424 | Acc: 47.917% (828/1728)\n",
      "Loss: 1.428 | Acc: 48.158% (863/1792)\n",
      "Loss: 1.426 | Acc: 48.168% (894/1856)\n",
      "Loss: 1.429 | Acc: 47.969% (921/1920)\n",
      "Loss: 1.431 | Acc: 47.933% (951/1984)\n",
      "Loss: 1.433 | Acc: 47.852% (980/2048)\n",
      "Loss: 1.425 | Acc: 48.011% (1014/2112)\n",
      "Loss: 1.424 | Acc: 48.116% (1047/2176)\n",
      "Loss: 1.423 | Acc: 48.304% (1082/2240)\n",
      "Loss: 1.417 | Acc: 48.568% (1119/2304)\n",
      "Loss: 1.418 | Acc: 48.564% (1150/2368)\n",
      "Loss: 1.417 | Acc: 48.684% (1184/2432)\n",
      "Loss: 1.413 | Acc: 48.838% (1219/2496)\n",
      "Loss: 1.423 | Acc: 48.555% (1243/2560)\n",
      "Loss: 1.426 | Acc: 48.171% (1264/2624)\n",
      "Loss: 1.427 | Acc: 48.214% (1296/2688)\n",
      "Loss: 1.425 | Acc: 48.183% (1326/2752)\n",
      "Loss: 1.426 | Acc: 48.260% (1359/2816)\n",
      "Loss: 1.427 | Acc: 48.264% (1390/2880)\n",
      "Loss: 1.426 | Acc: 48.166% (1418/2944)\n",
      "Loss: 1.426 | Acc: 48.172% (1449/3008)\n",
      "Loss: 1.425 | Acc: 48.177% (1480/3072)\n",
      "Loss: 1.423 | Acc: 48.119% (1509/3136)\n",
      "Loss: 1.419 | Acc: 48.281% (1545/3200)\n",
      "Loss: 1.421 | Acc: 48.284% (1576/3264)\n",
      "Loss: 1.422 | Acc: 48.227% (1605/3328)\n",
      "Loss: 1.420 | Acc: 48.320% (1639/3392)\n",
      "Loss: 1.419 | Acc: 48.322% (1670/3456)\n",
      "Loss: 1.416 | Acc: 48.466% (1706/3520)\n",
      "Loss: 1.415 | Acc: 48.493% (1738/3584)\n",
      "Loss: 1.419 | Acc: 48.355% (1764/3648)\n",
      "Loss: 1.418 | Acc: 48.411% (1797/3712)\n",
      "Loss: 1.417 | Acc: 48.385% (1827/3776)\n",
      "Loss: 1.415 | Acc: 48.385% (1858/3840)\n",
      "Loss: 1.417 | Acc: 48.463% (1892/3904)\n",
      "Loss: 1.416 | Acc: 48.362% (1919/3968)\n",
      "Loss: 1.414 | Acc: 48.537% (1957/4032)\n",
      "Loss: 1.417 | Acc: 48.486% (1986/4096)\n",
      "Loss: 1.422 | Acc: 48.558% (2020/4160)\n",
      "Loss: 1.422 | Acc: 48.509% (2049/4224)\n",
      "Loss: 1.423 | Acc: 48.368% (2074/4288)\n",
      "Loss: 1.423 | Acc: 48.392% (2106/4352)\n",
      "Loss: 1.419 | Acc: 48.551% (2144/4416)\n",
      "Loss: 1.417 | Acc: 48.705% (2182/4480)\n",
      "Loss: 1.416 | Acc: 48.680% (2212/4544)\n",
      "Loss: 1.418 | Acc: 48.633% (2241/4608)\n",
      "Loss: 1.416 | Acc: 48.609% (2271/4672)\n",
      "Loss: 1.415 | Acc: 48.628% (2303/4736)\n",
      "Loss: 1.416 | Acc: 48.625% (2334/4800)\n",
      "Loss: 1.415 | Acc: 48.643% (2366/4864)\n",
      "Loss: 1.415 | Acc: 48.559% (2393/4928)\n",
      "Loss: 1.417 | Acc: 48.458% (2419/4992)\n",
      "Loss: 1.417 | Acc: 48.497% (2452/5056)\n",
      "Loss: 1.420 | Acc: 48.398% (2478/5120)\n",
      "Loss: 1.421 | Acc: 48.418% (2510/5184)\n",
      "Loss: 1.422 | Acc: 48.342% (2537/5248)\n",
      "Loss: 1.419 | Acc: 48.475% (2575/5312)\n",
      "Loss: 1.422 | Acc: 48.363% (2600/5376)\n",
      "Loss: 1.422 | Acc: 48.235% (2624/5440)\n",
      "Loss: 1.422 | Acc: 48.256% (2656/5504)\n",
      "Loss: 1.424 | Acc: 48.204% (2684/5568)\n",
      "Loss: 1.428 | Acc: 48.153% (2712/5632)\n",
      "Loss: 1.428 | Acc: 48.121% (2741/5696)\n",
      "Loss: 1.427 | Acc: 48.160% (2774/5760)\n",
      "Loss: 1.424 | Acc: 48.283% (2812/5824)\n",
      "Loss: 1.424 | Acc: 48.302% (2844/5888)\n",
      "Loss: 1.425 | Acc: 48.286% (2874/5952)\n",
      "Loss: 1.423 | Acc: 48.404% (2912/6016)\n",
      "Loss: 1.422 | Acc: 48.405% (2943/6080)\n",
      "Loss: 1.422 | Acc: 48.340% (2970/6144)\n",
      "Loss: 1.424 | Acc: 48.325% (3000/6208)\n",
      "Loss: 1.425 | Acc: 48.246% (3026/6272)\n",
      "Loss: 1.425 | Acc: 48.201% (3054/6336)\n",
      "Loss: 1.425 | Acc: 48.156% (3082/6400)\n",
      "Loss: 1.426 | Acc: 48.113% (3110/6464)\n",
      "Loss: 1.426 | Acc: 48.146% (3143/6528)\n",
      "Loss: 1.428 | Acc: 48.089% (3170/6592)\n",
      "Loss: 1.427 | Acc: 48.122% (3203/6656)\n",
      "Loss: 1.428 | Acc: 48.125% (3234/6720)\n",
      "Loss: 1.427 | Acc: 48.157% (3267/6784)\n",
      "Loss: 1.424 | Acc: 48.189% (3300/6848)\n",
      "Loss: 1.425 | Acc: 48.163% (3329/6912)\n",
      "Loss: 1.426 | Acc: 48.179% (3361/6976)\n",
      "Loss: 1.427 | Acc: 48.068% (3384/7040)\n",
      "Loss: 1.426 | Acc: 48.114% (3418/7104)\n",
      "Loss: 1.427 | Acc: 48.131% (3450/7168)\n",
      "Loss: 1.429 | Acc: 48.078% (3477/7232)\n",
      "Loss: 1.429 | Acc: 48.026% (3504/7296)\n",
      "Loss: 1.426 | Acc: 48.125% (3542/7360)\n",
      "Loss: 1.428 | Acc: 48.020% (3565/7424)\n",
      "Loss: 1.426 | Acc: 48.050% (3598/7488)\n",
      "Loss: 1.426 | Acc: 48.067% (3630/7552)\n",
      "Loss: 1.428 | Acc: 48.030% (3658/7616)\n",
      "Loss: 1.427 | Acc: 48.060% (3691/7680)\n",
      "Loss: 1.426 | Acc: 48.128% (3727/7744)\n",
      "Loss: 1.425 | Acc: 48.181% (3762/7808)\n",
      "Loss: 1.426 | Acc: 48.183% (3793/7872)\n",
      "Loss: 1.425 | Acc: 48.211% (3826/7936)\n",
      "Loss: 1.428 | Acc: 48.163% (3853/8000)\n",
      "Loss: 1.427 | Acc: 48.127% (3881/8064)\n",
      "Loss: 1.428 | Acc: 48.167% (3915/8128)\n",
      "Loss: 1.427 | Acc: 48.132% (3943/8192)\n",
      "Loss: 1.429 | Acc: 48.074% (3969/8256)\n",
      "Loss: 1.432 | Acc: 47.969% (3991/8320)\n",
      "Loss: 1.433 | Acc: 47.913% (4017/8384)\n",
      "Loss: 1.432 | Acc: 47.905% (4047/8448)\n",
      "Loss: 1.433 | Acc: 47.897% (4077/8512)\n",
      "Loss: 1.434 | Acc: 47.913% (4109/8576)\n",
      "Loss: 1.436 | Acc: 47.882% (4137/8640)\n",
      "Loss: 1.437 | Acc: 47.863% (4166/8704)\n",
      "Loss: 1.437 | Acc: 47.844% (4195/8768)\n",
      "Loss: 1.436 | Acc: 47.860% (4227/8832)\n",
      "Loss: 1.435 | Acc: 47.909% (4262/8896)\n",
      "Loss: 1.435 | Acc: 47.958% (4297/8960)\n",
      "Loss: 1.435 | Acc: 47.972% (4329/9024)\n",
      "Loss: 1.435 | Acc: 47.975% (4360/9088)\n",
      "Loss: 1.436 | Acc: 47.968% (4390/9152)\n",
      "Loss: 1.434 | Acc: 48.047% (4428/9216)\n",
      "Loss: 1.433 | Acc: 48.071% (4461/9280)\n",
      "Loss: 1.433 | Acc: 48.063% (4491/9344)\n",
      "Loss: 1.434 | Acc: 48.034% (4519/9408)\n",
      "Loss: 1.436 | Acc: 47.952% (4542/9472)\n",
      "Loss: 1.435 | Acc: 47.997% (4577/9536)\n",
      "Loss: 1.434 | Acc: 48.062% (4614/9600)\n",
      "Loss: 1.435 | Acc: 48.024% (4641/9664)\n",
      "Loss: 1.434 | Acc: 48.078% (4677/9728)\n",
      "Loss: 1.435 | Acc: 48.080% (4708/9792)\n",
      "Loss: 1.435 | Acc: 48.072% (4738/9856)\n",
      "Loss: 1.436 | Acc: 48.065% (4768/9920)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.436 | Acc: 48.047% (4797/9984)\n",
      "Loss: 1.435 | Acc: 48.040% (4804/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 48.04\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.552 | Acc: 48.438% (31/64)\n",
      "Loss: 1.494 | Acc: 47.656% (61/128)\n",
      "Loss: 1.486 | Acc: 47.396% (91/192)\n",
      "Loss: 1.444 | Acc: 47.656% (122/256)\n",
      "Loss: 1.422 | Acc: 47.812% (153/320)\n",
      "Loss: 1.394 | Acc: 48.438% (186/384)\n",
      "Loss: 1.415 | Acc: 47.545% (213/448)\n",
      "Loss: 1.403 | Acc: 48.047% (246/512)\n",
      "Loss: 1.433 | Acc: 47.222% (272/576)\n",
      "Loss: 1.418 | Acc: 47.500% (304/640)\n",
      "Loss: 1.402 | Acc: 47.869% (337/704)\n",
      "Loss: 1.388 | Acc: 48.177% (370/768)\n",
      "Loss: 1.408 | Acc: 47.716% (397/832)\n",
      "Loss: 1.413 | Acc: 47.991% (430/896)\n",
      "Loss: 1.395 | Acc: 48.646% (467/960)\n",
      "Loss: 1.400 | Acc: 48.340% (495/1024)\n",
      "Loss: 1.390 | Acc: 48.713% (530/1088)\n",
      "Loss: 1.397 | Acc: 48.351% (557/1152)\n",
      "Loss: 1.407 | Acc: 47.944% (583/1216)\n",
      "Loss: 1.403 | Acc: 48.203% (617/1280)\n",
      "Loss: 1.400 | Acc: 48.661% (654/1344)\n",
      "Loss: 1.395 | Acc: 48.793% (687/1408)\n",
      "Loss: 1.387 | Acc: 49.049% (722/1472)\n",
      "Loss: 1.385 | Acc: 48.698% (748/1536)\n",
      "Loss: 1.377 | Acc: 48.938% (783/1600)\n",
      "Loss: 1.372 | Acc: 49.279% (820/1664)\n",
      "Loss: 1.377 | Acc: 49.248% (851/1728)\n",
      "Loss: 1.379 | Acc: 49.051% (879/1792)\n",
      "Loss: 1.378 | Acc: 49.138% (912/1856)\n",
      "Loss: 1.390 | Acc: 48.490% (931/1920)\n",
      "Loss: 1.393 | Acc: 48.337% (959/1984)\n",
      "Loss: 1.394 | Acc: 48.389% (991/2048)\n",
      "Loss: 1.397 | Acc: 48.106% (1016/2112)\n",
      "Loss: 1.401 | Acc: 48.024% (1045/2176)\n",
      "Loss: 1.393 | Acc: 48.214% (1080/2240)\n",
      "Loss: 1.390 | Acc: 48.307% (1113/2304)\n",
      "Loss: 1.389 | Acc: 48.438% (1147/2368)\n",
      "Loss: 1.389 | Acc: 48.479% (1179/2432)\n",
      "Loss: 1.389 | Acc: 48.438% (1209/2496)\n",
      "Loss: 1.387 | Acc: 48.398% (1239/2560)\n",
      "Loss: 1.384 | Acc: 48.666% (1277/2624)\n",
      "Loss: 1.383 | Acc: 48.921% (1315/2688)\n",
      "Loss: 1.383 | Acc: 49.092% (1351/2752)\n",
      "Loss: 1.384 | Acc: 49.006% (1380/2816)\n",
      "Loss: 1.378 | Acc: 49.201% (1417/2880)\n",
      "Loss: 1.377 | Acc: 49.219% (1449/2944)\n",
      "Loss: 1.378 | Acc: 49.269% (1482/3008)\n",
      "Loss: 1.376 | Acc: 49.284% (1514/3072)\n",
      "Loss: 1.375 | Acc: 49.394% (1549/3136)\n",
      "Loss: 1.373 | Acc: 49.406% (1581/3200)\n",
      "Loss: 1.367 | Acc: 49.694% (1622/3264)\n",
      "Loss: 1.364 | Acc: 49.700% (1654/3328)\n",
      "Loss: 1.362 | Acc: 49.794% (1689/3392)\n",
      "Loss: 1.359 | Acc: 49.826% (1722/3456)\n",
      "Loss: 1.357 | Acc: 49.886% (1756/3520)\n",
      "Loss: 1.360 | Acc: 49.749% (1783/3584)\n",
      "Loss: 1.358 | Acc: 49.863% (1819/3648)\n",
      "Loss: 1.355 | Acc: 49.946% (1854/3712)\n",
      "Loss: 1.351 | Acc: 50.106% (1892/3776)\n",
      "Loss: 1.346 | Acc: 50.234% (1929/3840)\n",
      "Loss: 1.347 | Acc: 50.205% (1960/3904)\n",
      "Loss: 1.342 | Acc: 50.479% (2003/3968)\n",
      "Loss: 1.340 | Acc: 50.570% (2039/4032)\n",
      "Loss: 1.341 | Acc: 50.610% (2073/4096)\n",
      "Loss: 1.342 | Acc: 50.625% (2106/4160)\n",
      "Loss: 1.344 | Acc: 50.568% (2136/4224)\n",
      "Loss: 1.342 | Acc: 50.583% (2169/4288)\n",
      "Loss: 1.340 | Acc: 50.620% (2203/4352)\n",
      "Loss: 1.340 | Acc: 50.498% (2230/4416)\n",
      "Loss: 1.340 | Acc: 50.402% (2258/4480)\n",
      "Loss: 1.341 | Acc: 50.330% (2287/4544)\n",
      "Loss: 1.343 | Acc: 50.304% (2318/4608)\n",
      "Loss: 1.345 | Acc: 50.193% (2345/4672)\n",
      "Loss: 1.345 | Acc: 50.253% (2380/4736)\n",
      "Loss: 1.345 | Acc: 50.167% (2408/4800)\n",
      "Loss: 1.346 | Acc: 50.103% (2437/4864)\n",
      "Loss: 1.345 | Acc: 50.122% (2470/4928)\n",
      "Loss: 1.346 | Acc: 50.060% (2499/4992)\n",
      "Loss: 1.343 | Acc: 50.277% (2542/5056)\n",
      "Loss: 1.344 | Acc: 50.273% (2574/5120)\n",
      "Loss: 1.342 | Acc: 50.367% (2611/5184)\n",
      "Loss: 1.345 | Acc: 50.286% (2639/5248)\n",
      "Loss: 1.344 | Acc: 50.320% (2673/5312)\n",
      "Loss: 1.344 | Acc: 50.260% (2702/5376)\n",
      "Loss: 1.341 | Acc: 50.441% (2744/5440)\n",
      "Loss: 1.343 | Acc: 50.436% (2776/5504)\n",
      "Loss: 1.343 | Acc: 50.485% (2811/5568)\n",
      "Loss: 1.341 | Acc: 50.550% (2847/5632)\n",
      "Loss: 1.341 | Acc: 50.527% (2878/5696)\n",
      "Loss: 1.341 | Acc: 50.486% (2908/5760)\n",
      "Loss: 1.342 | Acc: 50.498% (2941/5824)\n",
      "Loss: 1.343 | Acc: 50.560% (2977/5888)\n",
      "Loss: 1.342 | Acc: 50.538% (3008/5952)\n",
      "Loss: 1.342 | Acc: 50.549% (3041/6016)\n",
      "Loss: 1.341 | Acc: 50.559% (3074/6080)\n",
      "Loss: 1.342 | Acc: 50.488% (3102/6144)\n",
      "Loss: 1.342 | Acc: 50.467% (3133/6208)\n",
      "Loss: 1.343 | Acc: 50.462% (3165/6272)\n",
      "Loss: 1.347 | Acc: 50.442% (3196/6336)\n",
      "Loss: 1.347 | Acc: 50.438% (3228/6400)\n",
      "Loss: 1.348 | Acc: 50.402% (3258/6464)\n",
      "Loss: 1.348 | Acc: 50.429% (3292/6528)\n",
      "Loss: 1.351 | Acc: 50.303% (3316/6592)\n",
      "Loss: 1.350 | Acc: 50.361% (3352/6656)\n",
      "Loss: 1.353 | Acc: 50.298% (3380/6720)\n",
      "Loss: 1.354 | Acc: 50.221% (3407/6784)\n",
      "Loss: 1.353 | Acc: 50.248% (3441/6848)\n",
      "Loss: 1.354 | Acc: 50.260% (3474/6912)\n",
      "Loss: 1.357 | Acc: 50.186% (3501/6976)\n",
      "Loss: 1.359 | Acc: 50.156% (3531/7040)\n",
      "Loss: 1.358 | Acc: 50.225% (3568/7104)\n",
      "Loss: 1.359 | Acc: 50.209% (3599/7168)\n",
      "Loss: 1.360 | Acc: 50.207% (3631/7232)\n",
      "Loss: 1.358 | Acc: 50.260% (3667/7296)\n",
      "Loss: 1.358 | Acc: 50.326% (3704/7360)\n",
      "Loss: 1.357 | Acc: 50.350% (3738/7424)\n",
      "Loss: 1.357 | Acc: 50.374% (3772/7488)\n",
      "Loss: 1.356 | Acc: 50.424% (3808/7552)\n",
      "Loss: 1.356 | Acc: 50.460% (3843/7616)\n",
      "Loss: 1.357 | Acc: 50.430% (3873/7680)\n",
      "Loss: 1.357 | Acc: 50.478% (3909/7744)\n",
      "Loss: 1.357 | Acc: 50.461% (3940/7808)\n",
      "Loss: 1.355 | Acc: 50.610% (3984/7872)\n",
      "Loss: 1.355 | Acc: 50.655% (4020/7936)\n",
      "Loss: 1.355 | Acc: 50.725% (4058/8000)\n",
      "Loss: 1.354 | Acc: 50.769% (4094/8064)\n",
      "Loss: 1.352 | Acc: 50.812% (4130/8128)\n",
      "Loss: 1.351 | Acc: 50.879% (4168/8192)\n",
      "Loss: 1.353 | Acc: 50.799% (4194/8256)\n",
      "Loss: 1.353 | Acc: 50.865% (4232/8320)\n",
      "Loss: 1.352 | Acc: 50.835% (4262/8384)\n",
      "Loss: 1.352 | Acc: 50.817% (4293/8448)\n",
      "Loss: 1.352 | Acc: 50.811% (4325/8512)\n",
      "Loss: 1.353 | Acc: 50.758% (4353/8576)\n",
      "Loss: 1.353 | Acc: 50.799% (4389/8640)\n",
      "Loss: 1.353 | Acc: 50.770% (4419/8704)\n",
      "Loss: 1.354 | Acc: 50.821% (4456/8768)\n",
      "Loss: 1.356 | Acc: 50.759% (4483/8832)\n",
      "Loss: 1.355 | Acc: 50.753% (4515/8896)\n",
      "Loss: 1.356 | Acc: 50.714% (4544/8960)\n",
      "Loss: 1.357 | Acc: 50.676% (4573/9024)\n",
      "Loss: 1.358 | Acc: 50.660% (4604/9088)\n",
      "Loss: 1.357 | Acc: 50.721% (4642/9152)\n",
      "Loss: 1.355 | Acc: 50.825% (4684/9216)\n",
      "Loss: 1.356 | Acc: 50.776% (4712/9280)\n",
      "Loss: 1.356 | Acc: 50.781% (4745/9344)\n",
      "Loss: 1.356 | Acc: 50.765% (4776/9408)\n",
      "Loss: 1.356 | Acc: 50.792% (4811/9472)\n",
      "Loss: 1.354 | Acc: 50.849% (4849/9536)\n",
      "Loss: 1.356 | Acc: 50.823% (4879/9600)\n",
      "Loss: 1.357 | Acc: 50.776% (4907/9664)\n",
      "Loss: 1.356 | Acc: 50.822% (4944/9728)\n",
      "Loss: 1.356 | Acc: 50.817% (4976/9792)\n",
      "Loss: 1.356 | Acc: 50.862% (5013/9856)\n",
      "Loss: 1.357 | Acc: 50.827% (5042/9920)\n",
      "Loss: 1.358 | Acc: 50.761% (5068/9984)\n",
      "Loss: 1.359 | Acc: 50.756% (5100/10048)\n",
      "Loss: 1.357 | Acc: 50.841% (5141/10112)\n",
      "Loss: 1.357 | Acc: 50.855% (5175/10176)\n",
      "Loss: 1.358 | Acc: 50.820% (5204/10240)\n",
      "Loss: 1.358 | Acc: 50.757% (5230/10304)\n",
      "Loss: 1.357 | Acc: 50.801% (5267/10368)\n",
      "Loss: 1.358 | Acc: 50.796% (5299/10432)\n",
      "Loss: 1.358 | Acc: 50.810% (5333/10496)\n",
      "Loss: 1.356 | Acc: 50.909% (5376/10560)\n",
      "Loss: 1.357 | Acc: 50.875% (5405/10624)\n",
      "Loss: 1.356 | Acc: 50.908% (5441/10688)\n",
      "Loss: 1.356 | Acc: 50.967% (5480/10752)\n",
      "Loss: 1.356 | Acc: 50.962% (5512/10816)\n",
      "Loss: 1.356 | Acc: 50.974% (5546/10880)\n",
      "Loss: 1.356 | Acc: 50.914% (5572/10944)\n",
      "Loss: 1.357 | Acc: 50.899% (5603/11008)\n",
      "Loss: 1.357 | Acc: 50.894% (5635/11072)\n",
      "Loss: 1.359 | Acc: 50.853% (5663/11136)\n",
      "Loss: 1.360 | Acc: 50.848% (5695/11200)\n",
      "Loss: 1.361 | Acc: 50.808% (5723/11264)\n",
      "Loss: 1.360 | Acc: 50.821% (5757/11328)\n",
      "Loss: 1.360 | Acc: 50.825% (5790/11392)\n",
      "Loss: 1.359 | Acc: 50.794% (5819/11456)\n",
      "Loss: 1.359 | Acc: 50.738% (5845/11520)\n",
      "Loss: 1.359 | Acc: 50.734% (5877/11584)\n",
      "Loss: 1.358 | Acc: 50.790% (5916/11648)\n",
      "Loss: 1.359 | Acc: 50.786% (5948/11712)\n",
      "Loss: 1.358 | Acc: 50.815% (5984/11776)\n",
      "Loss: 1.357 | Acc: 50.794% (6014/11840)\n",
      "Loss: 1.356 | Acc: 50.790% (6046/11904)\n",
      "Loss: 1.356 | Acc: 50.777% (6077/11968)\n",
      "Loss: 1.356 | Acc: 50.781% (6110/12032)\n",
      "Loss: 1.356 | Acc: 50.794% (6144/12096)\n",
      "Loss: 1.358 | Acc: 50.765% (6173/12160)\n",
      "Loss: 1.359 | Acc: 50.728% (6201/12224)\n",
      "Loss: 1.360 | Acc: 50.741% (6235/12288)\n",
      "Loss: 1.360 | Acc: 50.729% (6266/12352)\n",
      "Loss: 1.359 | Acc: 50.757% (6302/12416)\n",
      "Loss: 1.360 | Acc: 50.737% (6332/12480)\n",
      "Loss: 1.360 | Acc: 50.773% (6369/12544)\n",
      "Loss: 1.359 | Acc: 50.801% (6405/12608)\n",
      "Loss: 1.359 | Acc: 50.773% (6434/12672)\n",
      "Loss: 1.359 | Acc: 50.793% (6469/12736)\n",
      "Loss: 1.358 | Acc: 50.797% (6502/12800)\n",
      "Loss: 1.359 | Acc: 50.777% (6532/12864)\n",
      "Loss: 1.358 | Acc: 50.828% (6571/12928)\n",
      "Loss: 1.357 | Acc: 50.854% (6607/12992)\n",
      "Loss: 1.357 | Acc: 50.812% (6634/13056)\n",
      "Loss: 1.357 | Acc: 50.800% (6665/13120)\n",
      "Loss: 1.357 | Acc: 50.796% (6697/13184)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.356 | Acc: 50.823% (6733/13248)\n",
      "Loss: 1.356 | Acc: 50.811% (6764/13312)\n",
      "Loss: 1.356 | Acc: 50.785% (6793/13376)\n",
      "Loss: 1.356 | Acc: 50.774% (6824/13440)\n",
      "Loss: 1.356 | Acc: 50.733% (6851/13504)\n",
      "Loss: 1.356 | Acc: 50.693% (6878/13568)\n",
      "Loss: 1.356 | Acc: 50.682% (6909/13632)\n",
      "Loss: 1.357 | Acc: 50.679% (6941/13696)\n",
      "Loss: 1.356 | Acc: 50.734% (6981/13760)\n",
      "Loss: 1.358 | Acc: 50.716% (7011/13824)\n",
      "Loss: 1.357 | Acc: 50.713% (7043/13888)\n",
      "Loss: 1.357 | Acc: 50.731% (7078/13952)\n",
      "Loss: 1.358 | Acc: 50.742% (7112/14016)\n",
      "Loss: 1.357 | Acc: 50.760% (7147/14080)\n",
      "Loss: 1.358 | Acc: 50.735% (7176/14144)\n",
      "Loss: 1.358 | Acc: 50.753% (7211/14208)\n",
      "Loss: 1.358 | Acc: 50.750% (7243/14272)\n",
      "Loss: 1.359 | Acc: 50.774% (7279/14336)\n",
      "Loss: 1.359 | Acc: 50.785% (7313/14400)\n",
      "Loss: 1.358 | Acc: 50.823% (7351/14464)\n",
      "Loss: 1.358 | Acc: 50.840% (7386/14528)\n",
      "Loss: 1.359 | Acc: 50.761% (7407/14592)\n",
      "Loss: 1.359 | Acc: 50.744% (7437/14656)\n",
      "Loss: 1.359 | Acc: 50.768% (7473/14720)\n",
      "Loss: 1.359 | Acc: 50.785% (7508/14784)\n",
      "Loss: 1.359 | Acc: 50.828% (7547/14848)\n",
      "Loss: 1.359 | Acc: 50.825% (7579/14912)\n",
      "Loss: 1.358 | Acc: 50.841% (7614/14976)\n",
      "Loss: 1.358 | Acc: 50.864% (7650/15040)\n",
      "Loss: 1.359 | Acc: 50.821% (7676/15104)\n",
      "Loss: 1.360 | Acc: 50.804% (7706/15168)\n",
      "Loss: 1.360 | Acc: 50.814% (7740/15232)\n",
      "Loss: 1.360 | Acc: 50.791% (7769/15296)\n",
      "Loss: 1.360 | Acc: 50.788% (7801/15360)\n",
      "Loss: 1.359 | Acc: 50.784% (7833/15424)\n",
      "Loss: 1.360 | Acc: 50.730% (7857/15488)\n",
      "Loss: 1.359 | Acc: 50.765% (7895/15552)\n",
      "Loss: 1.359 | Acc: 50.724% (7921/15616)\n",
      "Loss: 1.360 | Acc: 50.721% (7953/15680)\n",
      "Loss: 1.359 | Acc: 50.749% (7990/15744)\n",
      "Loss: 1.359 | Acc: 50.753% (8023/15808)\n",
      "Loss: 1.359 | Acc: 50.731% (8052/15872)\n",
      "Loss: 1.359 | Acc: 50.759% (8089/15936)\n",
      "Loss: 1.359 | Acc: 50.750% (8120/16000)\n",
      "Loss: 1.359 | Acc: 50.722% (8148/16064)\n",
      "Loss: 1.359 | Acc: 50.781% (8190/16128)\n",
      "Loss: 1.358 | Acc: 50.778% (8222/16192)\n",
      "Loss: 1.358 | Acc: 50.800% (8258/16256)\n",
      "Loss: 1.357 | Acc: 50.839% (8297/16320)\n",
      "Loss: 1.355 | Acc: 50.891% (8338/16384)\n",
      "Loss: 1.354 | Acc: 50.918% (8375/16448)\n",
      "Loss: 1.355 | Acc: 50.890% (8403/16512)\n",
      "Loss: 1.356 | Acc: 50.881% (8434/16576)\n",
      "Loss: 1.355 | Acc: 50.901% (8470/16640)\n",
      "Loss: 1.355 | Acc: 50.892% (8501/16704)\n",
      "Loss: 1.355 | Acc: 50.936% (8541/16768)\n",
      "Loss: 1.355 | Acc: 50.968% (8579/16832)\n",
      "Loss: 1.355 | Acc: 50.959% (8610/16896)\n",
      "Loss: 1.354 | Acc: 50.973% (8645/16960)\n",
      "Loss: 1.355 | Acc: 50.934% (8671/17024)\n",
      "Loss: 1.354 | Acc: 50.977% (8711/17088)\n",
      "Loss: 1.354 | Acc: 50.991% (8746/17152)\n",
      "Loss: 1.354 | Acc: 50.987% (8778/17216)\n",
      "Loss: 1.353 | Acc: 50.995% (8812/17280)\n",
      "Loss: 1.353 | Acc: 51.003% (8846/17344)\n",
      "Loss: 1.353 | Acc: 51.000% (8878/17408)\n",
      "Loss: 1.352 | Acc: 51.019% (8914/17472)\n",
      "Loss: 1.351 | Acc: 51.032% (8949/17536)\n",
      "Loss: 1.351 | Acc: 51.051% (8985/17600)\n",
      "Loss: 1.351 | Acc: 51.030% (9014/17664)\n",
      "Loss: 1.351 | Acc: 51.032% (9047/17728)\n",
      "Loss: 1.351 | Acc: 51.023% (9078/17792)\n",
      "Loss: 1.352 | Acc: 51.036% (9113/17856)\n",
      "Loss: 1.352 | Acc: 51.021% (9143/17920)\n",
      "Loss: 1.352 | Acc: 50.995% (9171/17984)\n",
      "Loss: 1.353 | Acc: 50.975% (9200/18048)\n",
      "Loss: 1.353 | Acc: 51.005% (9238/18112)\n",
      "Loss: 1.353 | Acc: 50.996% (9269/18176)\n",
      "Loss: 1.353 | Acc: 50.992% (9301/18240)\n",
      "Loss: 1.353 | Acc: 50.972% (9330/18304)\n",
      "Loss: 1.354 | Acc: 50.975% (9363/18368)\n",
      "Loss: 1.354 | Acc: 50.955% (9392/18432)\n",
      "Loss: 1.354 | Acc: 50.952% (9424/18496)\n",
      "Loss: 1.353 | Acc: 50.964% (9459/18560)\n",
      "Loss: 1.353 | Acc: 50.940% (9487/18624)\n",
      "Loss: 1.354 | Acc: 50.926% (9517/18688)\n",
      "Loss: 1.354 | Acc: 50.917% (9548/18752)\n",
      "Loss: 1.354 | Acc: 50.898% (9577/18816)\n",
      "Loss: 1.354 | Acc: 50.895% (9609/18880)\n",
      "Loss: 1.353 | Acc: 50.924% (9647/18944)\n",
      "Loss: 1.353 | Acc: 50.905% (9676/19008)\n",
      "Loss: 1.354 | Acc: 50.865% (9701/19072)\n",
      "Loss: 1.354 | Acc: 50.841% (9729/19136)\n",
      "Loss: 1.353 | Acc: 50.865% (9766/19200)\n",
      "Loss: 1.353 | Acc: 50.882% (9802/19264)\n",
      "Loss: 1.353 | Acc: 50.895% (9837/19328)\n",
      "Loss: 1.354 | Acc: 50.871% (9865/19392)\n",
      "Loss: 1.354 | Acc: 50.879% (9899/19456)\n",
      "Loss: 1.354 | Acc: 50.902% (9936/19520)\n",
      "Loss: 1.354 | Acc: 50.888% (9966/19584)\n",
      "Loss: 1.354 | Acc: 50.880% (9997/19648)\n",
      "Loss: 1.354 | Acc: 50.928% (10039/19712)\n",
      "Loss: 1.355 | Acc: 50.900% (10066/19776)\n",
      "Loss: 1.355 | Acc: 50.907% (10100/19840)\n",
      "Loss: 1.354 | Acc: 50.934% (10138/19904)\n",
      "Loss: 1.354 | Acc: 50.931% (10170/19968)\n",
      "Loss: 1.355 | Acc: 50.924% (10201/20032)\n",
      "Loss: 1.355 | Acc: 50.901% (10229/20096)\n",
      "Loss: 1.354 | Acc: 50.893% (10260/20160)\n",
      "Loss: 1.354 | Acc: 50.930% (10300/20224)\n",
      "Loss: 1.354 | Acc: 50.951% (10337/20288)\n",
      "Loss: 1.354 | Acc: 50.929% (10365/20352)\n",
      "Loss: 1.354 | Acc: 50.940% (10400/20416)\n",
      "Loss: 1.354 | Acc: 50.938% (10432/20480)\n",
      "Loss: 1.354 | Acc: 50.920% (10461/20544)\n",
      "Loss: 1.353 | Acc: 50.946% (10499/20608)\n",
      "Loss: 1.353 | Acc: 50.934% (10529/20672)\n",
      "Loss: 1.354 | Acc: 50.907% (10556/20736)\n",
      "Loss: 1.353 | Acc: 50.928% (10593/20800)\n",
      "Loss: 1.353 | Acc: 50.944% (10629/20864)\n",
      "Loss: 1.354 | Acc: 50.898% (10652/20928)\n",
      "Loss: 1.354 | Acc: 50.915% (10688/20992)\n",
      "Loss: 1.354 | Acc: 50.902% (10718/21056)\n",
      "Loss: 1.354 | Acc: 50.914% (10753/21120)\n",
      "Loss: 1.355 | Acc: 50.911% (10785/21184)\n",
      "Loss: 1.355 | Acc: 50.932% (10822/21248)\n",
      "Loss: 1.355 | Acc: 50.929% (10854/21312)\n",
      "Loss: 1.354 | Acc: 50.959% (10893/21376)\n",
      "Loss: 1.354 | Acc: 50.956% (10925/21440)\n",
      "Loss: 1.354 | Acc: 50.967% (10960/21504)\n",
      "Loss: 1.354 | Acc: 50.960% (10991/21568)\n",
      "Loss: 1.354 | Acc: 50.985% (11029/21632)\n",
      "Loss: 1.354 | Acc: 50.986% (11062/21696)\n",
      "Loss: 1.353 | Acc: 50.988% (11095/21760)\n",
      "Loss: 1.353 | Acc: 50.994% (11129/21824)\n",
      "Loss: 1.353 | Acc: 50.996% (11162/21888)\n",
      "Loss: 1.353 | Acc: 51.002% (11196/21952)\n",
      "Loss: 1.353 | Acc: 50.999% (11228/22016)\n",
      "Loss: 1.352 | Acc: 51.024% (11266/22080)\n",
      "Loss: 1.353 | Acc: 51.003% (11294/22144)\n",
      "Loss: 1.354 | Acc: 50.964% (11318/22208)\n",
      "Loss: 1.353 | Acc: 50.970% (11352/22272)\n",
      "Loss: 1.352 | Acc: 50.998% (11391/22336)\n",
      "Loss: 1.352 | Acc: 51.009% (11426/22400)\n",
      "Loss: 1.353 | Acc: 50.975% (11451/22464)\n",
      "Loss: 1.352 | Acc: 50.981% (11485/22528)\n",
      "Loss: 1.352 | Acc: 50.996% (11521/22592)\n",
      "Loss: 1.352 | Acc: 50.989% (11552/22656)\n",
      "Loss: 1.353 | Acc: 50.973% (11581/22720)\n",
      "Loss: 1.353 | Acc: 50.957% (11610/22784)\n",
      "Loss: 1.353 | Acc: 50.972% (11646/22848)\n",
      "Loss: 1.353 | Acc: 50.973% (11679/22912)\n",
      "Loss: 1.352 | Acc: 51.010% (11720/22976)\n",
      "Loss: 1.352 | Acc: 51.020% (11755/23040)\n",
      "Loss: 1.352 | Acc: 51.017% (11787/23104)\n",
      "Loss: 1.352 | Acc: 51.006% (11817/23168)\n",
      "Loss: 1.352 | Acc: 51.024% (11854/23232)\n",
      "Loss: 1.352 | Acc: 51.022% (11886/23296)\n",
      "Loss: 1.352 | Acc: 51.019% (11918/23360)\n",
      "Loss: 1.352 | Acc: 50.990% (11944/23424)\n",
      "Loss: 1.352 | Acc: 51.001% (11979/23488)\n",
      "Loss: 1.352 | Acc: 51.006% (12013/23552)\n",
      "Loss: 1.352 | Acc: 50.974% (12038/23616)\n",
      "Loss: 1.352 | Acc: 50.992% (12075/23680)\n",
      "Loss: 1.352 | Acc: 51.011% (12112/23744)\n",
      "Loss: 1.352 | Acc: 51.000% (12142/23808)\n",
      "Loss: 1.351 | Acc: 51.018% (12179/23872)\n",
      "Loss: 1.351 | Acc: 51.032% (12215/23936)\n",
      "Loss: 1.351 | Acc: 51.046% (12251/24000)\n",
      "Loss: 1.352 | Acc: 51.031% (12280/24064)\n",
      "Loss: 1.352 | Acc: 51.024% (12311/24128)\n",
      "Loss: 1.352 | Acc: 51.013% (12341/24192)\n",
      "Loss: 1.352 | Acc: 51.002% (12371/24256)\n",
      "Loss: 1.352 | Acc: 51.012% (12406/24320)\n",
      "Loss: 1.352 | Acc: 51.013% (12439/24384)\n",
      "Loss: 1.352 | Acc: 51.018% (12473/24448)\n",
      "Loss: 1.352 | Acc: 51.004% (12502/24512)\n",
      "Loss: 1.352 | Acc: 50.997% (12533/24576)\n",
      "Loss: 1.352 | Acc: 51.015% (12570/24640)\n",
      "Loss: 1.352 | Acc: 50.992% (12597/24704)\n",
      "Loss: 1.352 | Acc: 50.993% (12630/24768)\n",
      "Loss: 1.352 | Acc: 50.991% (12662/24832)\n",
      "Loss: 1.352 | Acc: 51.008% (12699/24896)\n",
      "Loss: 1.351 | Acc: 51.034% (12738/24960)\n",
      "Loss: 1.351 | Acc: 51.047% (12774/25024)\n",
      "Loss: 1.351 | Acc: 51.056% (12809/25088)\n",
      "Loss: 1.351 | Acc: 51.058% (12842/25152)\n",
      "Loss: 1.351 | Acc: 51.023% (12866/25216)\n",
      "Loss: 1.351 | Acc: 51.025% (12899/25280)\n",
      "Loss: 1.352 | Acc: 50.998% (12925/25344)\n",
      "Loss: 1.352 | Acc: 50.949% (12945/25408)\n",
      "Loss: 1.353 | Acc: 50.950% (12978/25472)\n",
      "Loss: 1.353 | Acc: 50.928% (13005/25536)\n",
      "Loss: 1.353 | Acc: 50.926% (13037/25600)\n",
      "Loss: 1.354 | Acc: 50.916% (13067/25664)\n",
      "Loss: 1.354 | Acc: 50.913% (13099/25728)\n",
      "Loss: 1.354 | Acc: 50.907% (13130/25792)\n",
      "Loss: 1.354 | Acc: 50.920% (13166/25856)\n",
      "Loss: 1.354 | Acc: 50.914% (13197/25920)\n",
      "Loss: 1.354 | Acc: 50.889% (13223/25984)\n",
      "Loss: 1.354 | Acc: 50.887% (13255/26048)\n",
      "Loss: 1.354 | Acc: 50.900% (13291/26112)\n",
      "Loss: 1.353 | Acc: 50.913% (13327/26176)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.354 | Acc: 50.907% (13358/26240)\n",
      "Loss: 1.354 | Acc: 50.863% (13379/26304)\n",
      "Loss: 1.354 | Acc: 50.891% (13419/26368)\n",
      "Loss: 1.354 | Acc: 50.874% (13447/26432)\n",
      "Loss: 1.354 | Acc: 50.891% (13484/26496)\n",
      "Loss: 1.354 | Acc: 50.885% (13515/26560)\n",
      "Loss: 1.354 | Acc: 50.886% (13548/26624)\n",
      "Loss: 1.354 | Acc: 50.854% (13572/26688)\n",
      "Loss: 1.353 | Acc: 50.860% (13606/26752)\n",
      "Loss: 1.353 | Acc: 50.884% (13645/26816)\n",
      "Loss: 1.353 | Acc: 50.893% (13680/26880)\n",
      "Loss: 1.353 | Acc: 50.909% (13717/26944)\n",
      "Loss: 1.353 | Acc: 50.915% (13751/27008)\n",
      "Loss: 1.353 | Acc: 50.894% (13778/27072)\n",
      "Loss: 1.354 | Acc: 50.859% (13801/27136)\n",
      "Loss: 1.354 | Acc: 50.882% (13840/27200)\n",
      "Loss: 1.353 | Acc: 50.899% (13877/27264)\n",
      "Loss: 1.353 | Acc: 50.893% (13908/27328)\n",
      "Loss: 1.353 | Acc: 50.898% (13942/27392)\n",
      "Loss: 1.353 | Acc: 50.889% (13972/27456)\n",
      "Loss: 1.353 | Acc: 50.890% (14005/27520)\n",
      "Loss: 1.353 | Acc: 50.910% (14043/27584)\n",
      "Loss: 1.353 | Acc: 50.930% (14081/27648)\n",
      "Loss: 1.353 | Acc: 50.924% (14112/27712)\n",
      "Loss: 1.353 | Acc: 50.922% (14144/27776)\n",
      "Loss: 1.353 | Acc: 50.912% (14174/27840)\n",
      "Loss: 1.353 | Acc: 50.943% (14215/27904)\n",
      "Loss: 1.353 | Acc: 50.937% (14246/27968)\n",
      "Loss: 1.353 | Acc: 50.928% (14276/28032)\n",
      "Loss: 1.353 | Acc: 50.925% (14308/28096)\n",
      "Loss: 1.353 | Acc: 50.930% (14342/28160)\n",
      "Loss: 1.353 | Acc: 50.932% (14375/28224)\n",
      "Loss: 1.353 | Acc: 50.958% (14415/28288)\n",
      "Loss: 1.353 | Acc: 50.952% (14446/28352)\n",
      "Loss: 1.353 | Acc: 50.950% (14478/28416)\n",
      "Loss: 1.353 | Acc: 50.962% (14514/28480)\n",
      "Loss: 1.353 | Acc: 50.970% (14549/28544)\n",
      "Loss: 1.353 | Acc: 50.944% (14574/28608)\n",
      "Loss: 1.353 | Acc: 50.959% (14611/28672)\n",
      "Loss: 1.353 | Acc: 50.967% (14646/28736)\n",
      "Loss: 1.353 | Acc: 50.955% (14675/28800)\n",
      "Loss: 1.352 | Acc: 50.963% (14710/28864)\n",
      "Loss: 1.353 | Acc: 50.954% (14740/28928)\n",
      "Loss: 1.353 | Acc: 50.966% (14776/28992)\n",
      "Loss: 1.353 | Acc: 50.960% (14807/29056)\n",
      "Loss: 1.353 | Acc: 50.938% (14833/29120)\n",
      "Loss: 1.353 | Acc: 50.946% (14868/29184)\n",
      "Loss: 1.354 | Acc: 50.933% (14897/29248)\n",
      "Loss: 1.353 | Acc: 50.945% (14933/29312)\n",
      "Loss: 1.353 | Acc: 50.943% (14965/29376)\n",
      "Loss: 1.353 | Acc: 50.931% (14994/29440)\n",
      "Loss: 1.353 | Acc: 50.949% (15032/29504)\n",
      "Loss: 1.353 | Acc: 50.944% (15063/29568)\n",
      "Loss: 1.353 | Acc: 50.955% (15099/29632)\n",
      "Loss: 1.353 | Acc: 50.973% (15137/29696)\n",
      "Loss: 1.353 | Acc: 50.988% (15174/29760)\n",
      "Loss: 1.353 | Acc: 50.982% (15205/29824)\n",
      "Loss: 1.353 | Acc: 50.970% (15234/29888)\n",
      "Loss: 1.353 | Acc: 50.975% (15268/29952)\n",
      "Loss: 1.353 | Acc: 50.953% (15294/30016)\n",
      "Loss: 1.353 | Acc: 50.964% (15330/30080)\n",
      "Loss: 1.353 | Acc: 50.992% (15371/30144)\n",
      "Loss: 1.352 | Acc: 51.000% (15406/30208)\n",
      "Loss: 1.352 | Acc: 51.004% (15440/30272)\n",
      "Loss: 1.352 | Acc: 50.999% (15471/30336)\n",
      "Loss: 1.352 | Acc: 50.987% (15500/30400)\n",
      "Loss: 1.352 | Acc: 50.978% (15530/30464)\n",
      "Loss: 1.353 | Acc: 50.953% (15555/30528)\n",
      "Loss: 1.353 | Acc: 50.945% (15585/30592)\n",
      "Loss: 1.353 | Acc: 50.926% (15612/30656)\n",
      "Loss: 1.354 | Acc: 50.915% (15641/30720)\n",
      "Loss: 1.353 | Acc: 50.926% (15677/30784)\n",
      "Loss: 1.353 | Acc: 50.927% (15710/30848)\n",
      "Loss: 1.353 | Acc: 50.925% (15742/30912)\n",
      "Loss: 1.354 | Acc: 50.933% (15777/30976)\n",
      "Loss: 1.353 | Acc: 50.931% (15809/31040)\n",
      "Loss: 1.354 | Acc: 50.913% (15836/31104)\n",
      "Loss: 1.354 | Acc: 50.908% (15867/31168)\n",
      "Loss: 1.354 | Acc: 50.919% (15903/31232)\n",
      "Loss: 1.353 | Acc: 50.927% (15938/31296)\n",
      "Loss: 1.354 | Acc: 50.915% (15967/31360)\n",
      "Loss: 1.354 | Acc: 50.926% (16003/31424)\n",
      "Loss: 1.354 | Acc: 50.924% (16035/31488)\n",
      "Loss: 1.354 | Acc: 50.932% (16070/31552)\n",
      "Loss: 1.354 | Acc: 50.927% (16101/31616)\n",
      "Loss: 1.354 | Acc: 50.896% (16124/31680)\n",
      "Loss: 1.354 | Acc: 50.888% (16154/31744)\n",
      "Loss: 1.355 | Acc: 50.883% (16185/31808)\n",
      "Loss: 1.355 | Acc: 50.885% (16218/31872)\n",
      "Loss: 1.355 | Acc: 50.880% (16249/31936)\n",
      "Loss: 1.355 | Acc: 50.859% (16275/32000)\n",
      "Loss: 1.355 | Acc: 50.886% (16316/32064)\n",
      "Loss: 1.355 | Acc: 50.900% (16353/32128)\n",
      "Loss: 1.355 | Acc: 50.907% (16388/32192)\n",
      "Loss: 1.354 | Acc: 50.924% (16426/32256)\n",
      "Loss: 1.354 | Acc: 50.928% (16460/32320)\n",
      "Loss: 1.354 | Acc: 50.926% (16492/32384)\n",
      "Loss: 1.354 | Acc: 50.940% (16529/32448)\n",
      "Loss: 1.354 | Acc: 50.941% (16562/32512)\n",
      "Loss: 1.355 | Acc: 50.936% (16593/32576)\n",
      "Loss: 1.355 | Acc: 50.931% (16624/32640)\n",
      "Loss: 1.354 | Acc: 50.936% (16658/32704)\n",
      "Loss: 1.354 | Acc: 50.952% (16696/32768)\n",
      "Loss: 1.354 | Acc: 50.950% (16728/32832)\n",
      "Loss: 1.354 | Acc: 50.945% (16759/32896)\n",
      "Loss: 1.355 | Acc: 50.925% (16785/32960)\n",
      "Loss: 1.355 | Acc: 50.921% (16816/33024)\n",
      "Loss: 1.355 | Acc: 50.904% (16843/33088)\n",
      "Loss: 1.355 | Acc: 50.902% (16875/33152)\n",
      "Loss: 1.355 | Acc: 50.894% (16905/33216)\n",
      "Loss: 1.356 | Acc: 50.880% (16933/33280)\n",
      "Loss: 1.356 | Acc: 50.873% (16963/33344)\n",
      "Loss: 1.355 | Acc: 50.865% (16993/33408)\n",
      "Loss: 1.355 | Acc: 50.881% (17031/33472)\n",
      "Loss: 1.355 | Acc: 50.904% (17071/33536)\n",
      "Loss: 1.355 | Acc: 50.896% (17101/33600)\n",
      "Loss: 1.355 | Acc: 50.909% (17138/33664)\n",
      "Loss: 1.354 | Acc: 50.922% (17175/33728)\n",
      "Loss: 1.354 | Acc: 50.920% (17207/33792)\n",
      "Loss: 1.354 | Acc: 50.922% (17240/33856)\n",
      "Loss: 1.354 | Acc: 50.917% (17271/33920)\n",
      "Loss: 1.355 | Acc: 50.892% (17295/33984)\n",
      "Loss: 1.355 | Acc: 50.896% (17329/34048)\n",
      "Loss: 1.354 | Acc: 50.882% (17357/34112)\n",
      "Loss: 1.354 | Acc: 50.881% (17389/34176)\n",
      "Loss: 1.355 | Acc: 50.870% (17418/34240)\n",
      "Loss: 1.355 | Acc: 50.860% (17447/34304)\n",
      "Loss: 1.354 | Acc: 50.855% (17478/34368)\n",
      "Loss: 1.354 | Acc: 50.851% (17509/34432)\n",
      "Loss: 1.355 | Acc: 50.838% (17537/34496)\n",
      "Loss: 1.354 | Acc: 50.842% (17571/34560)\n",
      "Loss: 1.354 | Acc: 50.843% (17604/34624)\n",
      "Loss: 1.354 | Acc: 50.833% (17633/34688)\n",
      "Loss: 1.355 | Acc: 50.814% (17659/34752)\n",
      "Loss: 1.354 | Acc: 50.813% (17691/34816)\n",
      "Loss: 1.354 | Acc: 50.806% (17721/34880)\n",
      "Loss: 1.355 | Acc: 50.793% (17749/34944)\n",
      "Loss: 1.355 | Acc: 50.774% (17775/35008)\n",
      "Loss: 1.356 | Acc: 50.764% (17804/35072)\n",
      "Loss: 1.355 | Acc: 50.771% (17839/35136)\n",
      "Loss: 1.355 | Acc: 50.761% (17868/35200)\n",
      "Loss: 1.355 | Acc: 50.771% (17904/35264)\n",
      "Loss: 1.355 | Acc: 50.764% (17934/35328)\n",
      "Loss: 1.355 | Acc: 50.763% (17966/35392)\n",
      "Loss: 1.355 | Acc: 50.770% (18001/35456)\n",
      "Loss: 1.355 | Acc: 50.766% (18032/35520)\n",
      "Loss: 1.355 | Acc: 50.764% (18064/35584)\n",
      "Loss: 1.356 | Acc: 50.738% (18087/35648)\n",
      "Loss: 1.356 | Acc: 50.748% (18123/35712)\n",
      "Loss: 1.355 | Acc: 50.760% (18160/35776)\n",
      "Loss: 1.355 | Acc: 50.778% (18199/35840)\n",
      "Loss: 1.355 | Acc: 50.780% (18232/35904)\n",
      "Loss: 1.355 | Acc: 50.770% (18261/35968)\n",
      "Loss: 1.355 | Acc: 50.785% (18299/36032)\n",
      "Loss: 1.355 | Acc: 50.751% (18319/36096)\n",
      "Loss: 1.355 | Acc: 50.783% (18363/36160)\n",
      "Loss: 1.354 | Acc: 50.792% (18399/36224)\n",
      "Loss: 1.355 | Acc: 50.785% (18429/36288)\n",
      "Loss: 1.355 | Acc: 50.790% (18463/36352)\n",
      "Loss: 1.355 | Acc: 50.785% (18494/36416)\n",
      "Loss: 1.355 | Acc: 50.809% (18535/36480)\n",
      "Loss: 1.355 | Acc: 50.805% (18566/36544)\n",
      "Loss: 1.356 | Acc: 50.789% (18593/36608)\n",
      "Loss: 1.355 | Acc: 50.802% (18630/36672)\n",
      "Loss: 1.355 | Acc: 50.798% (18661/36736)\n",
      "Loss: 1.355 | Acc: 50.804% (18696/36800)\n",
      "Loss: 1.356 | Acc: 50.795% (18725/36864)\n",
      "Loss: 1.356 | Acc: 50.804% (18761/36928)\n",
      "Loss: 1.356 | Acc: 50.808% (18795/36992)\n",
      "Loss: 1.356 | Acc: 50.799% (18824/37056)\n",
      "Loss: 1.356 | Acc: 50.800% (18857/37120)\n",
      "Loss: 1.356 | Acc: 50.801% (18890/37184)\n",
      "Loss: 1.356 | Acc: 50.797% (18921/37248)\n",
      "Loss: 1.356 | Acc: 50.785% (18949/37312)\n",
      "Loss: 1.356 | Acc: 50.784% (18981/37376)\n",
      "Loss: 1.356 | Acc: 50.793% (19017/37440)\n",
      "Loss: 1.356 | Acc: 50.792% (19049/37504)\n",
      "Loss: 1.356 | Acc: 50.793% (19082/37568)\n",
      "Loss: 1.356 | Acc: 50.784% (19111/37632)\n",
      "Loss: 1.356 | Acc: 50.791% (19146/37696)\n",
      "Loss: 1.356 | Acc: 50.808% (19185/37760)\n",
      "Loss: 1.356 | Acc: 50.809% (19218/37824)\n",
      "Loss: 1.356 | Acc: 50.800% (19247/37888)\n",
      "Loss: 1.356 | Acc: 50.806% (19282/37952)\n",
      "Loss: 1.356 | Acc: 50.808% (19315/38016)\n",
      "Loss: 1.356 | Acc: 50.806% (19347/38080)\n",
      "Loss: 1.357 | Acc: 50.792% (19374/38144)\n",
      "Loss: 1.356 | Acc: 50.801% (19410/38208)\n",
      "Loss: 1.356 | Acc: 50.802% (19443/38272)\n",
      "Loss: 1.356 | Acc: 50.796% (19473/38336)\n",
      "Loss: 1.356 | Acc: 50.794% (19505/38400)\n",
      "Loss: 1.356 | Acc: 50.793% (19537/38464)\n",
      "Loss: 1.356 | Acc: 50.799% (19572/38528)\n",
      "Loss: 1.356 | Acc: 50.783% (19598/38592)\n",
      "Loss: 1.357 | Acc: 50.766% (19624/38656)\n",
      "Loss: 1.357 | Acc: 50.772% (19659/38720)\n",
      "Loss: 1.356 | Acc: 50.786% (19697/38784)\n",
      "Loss: 1.356 | Acc: 50.803% (19736/38848)\n",
      "Loss: 1.356 | Acc: 50.820% (19775/38912)\n",
      "Loss: 1.355 | Acc: 50.824% (19809/38976)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.355 | Acc: 50.827% (19843/39040)\n",
      "Loss: 1.355 | Acc: 50.836% (19879/39104)\n",
      "Loss: 1.355 | Acc: 50.817% (19904/39168)\n",
      "Loss: 1.356 | Acc: 50.800% (19930/39232)\n",
      "Loss: 1.356 | Acc: 50.809% (19966/39296)\n",
      "Loss: 1.356 | Acc: 50.805% (19997/39360)\n",
      "Loss: 1.355 | Acc: 50.819% (20035/39424)\n",
      "Loss: 1.356 | Acc: 50.815% (20066/39488)\n",
      "Loss: 1.356 | Acc: 50.807% (20095/39552)\n",
      "Loss: 1.356 | Acc: 50.800% (20125/39616)\n",
      "Loss: 1.356 | Acc: 50.806% (20160/39680)\n",
      "Loss: 1.356 | Acc: 50.805% (20192/39744)\n",
      "Loss: 1.356 | Acc: 50.816% (20229/39808)\n",
      "Loss: 1.356 | Acc: 50.800% (20255/39872)\n",
      "Loss: 1.356 | Acc: 50.799% (20287/39936)\n",
      "Loss: 1.356 | Acc: 50.790% (20316/40000)\n",
      "Loss: 1.356 | Acc: 50.801% (20353/40064)\n",
      "Loss: 1.356 | Acc: 50.817% (20392/40128)\n",
      "Loss: 1.356 | Acc: 50.816% (20424/40192)\n",
      "Loss: 1.356 | Acc: 50.797% (20449/40256)\n",
      "Loss: 1.356 | Acc: 50.794% (20480/40320)\n",
      "Loss: 1.356 | Acc: 50.805% (20517/40384)\n",
      "Loss: 1.357 | Acc: 50.799% (20547/40448)\n",
      "Loss: 1.357 | Acc: 50.795% (20578/40512)\n",
      "Loss: 1.357 | Acc: 50.796% (20611/40576)\n",
      "Loss: 1.357 | Acc: 50.795% (20643/40640)\n",
      "Loss: 1.357 | Acc: 50.791% (20674/40704)\n",
      "Loss: 1.357 | Acc: 50.792% (20707/40768)\n",
      "Loss: 1.357 | Acc: 50.801% (20743/40832)\n",
      "Loss: 1.357 | Acc: 50.795% (20773/40896)\n",
      "Loss: 1.357 | Acc: 50.811% (20812/40960)\n",
      "Loss: 1.356 | Acc: 50.821% (20849/41024)\n",
      "Loss: 1.356 | Acc: 50.810% (20877/41088)\n",
      "Loss: 1.357 | Acc: 50.814% (20911/41152)\n",
      "Loss: 1.357 | Acc: 50.825% (20948/41216)\n",
      "Loss: 1.357 | Acc: 50.812% (20975/41280)\n",
      "Loss: 1.357 | Acc: 50.805% (21005/41344)\n",
      "Loss: 1.357 | Acc: 50.799% (21035/41408)\n",
      "Loss: 1.357 | Acc: 50.786% (21062/41472)\n",
      "Loss: 1.357 | Acc: 50.794% (21098/41536)\n",
      "Loss: 1.357 | Acc: 50.803% (21134/41600)\n",
      "Loss: 1.358 | Acc: 50.780% (21157/41664)\n",
      "Loss: 1.358 | Acc: 50.784% (21191/41728)\n",
      "Loss: 1.357 | Acc: 50.792% (21227/41792)\n",
      "Loss: 1.357 | Acc: 50.796% (21261/41856)\n",
      "Loss: 1.357 | Acc: 50.809% (21299/41920)\n",
      "Loss: 1.357 | Acc: 50.827% (21339/41984)\n",
      "Loss: 1.357 | Acc: 50.820% (21369/42048)\n",
      "Loss: 1.357 | Acc: 50.807% (21396/42112)\n",
      "Loss: 1.357 | Acc: 50.806% (21428/42176)\n",
      "Loss: 1.357 | Acc: 50.807% (21461/42240)\n",
      "Loss: 1.357 | Acc: 50.806% (21493/42304)\n",
      "Loss: 1.358 | Acc: 50.784% (21516/42368)\n",
      "Loss: 1.358 | Acc: 50.789% (21551/42432)\n",
      "Loss: 1.357 | Acc: 50.807% (21591/42496)\n",
      "Loss: 1.357 | Acc: 50.804% (21622/42560)\n",
      "Loss: 1.357 | Acc: 50.795% (21651/42624)\n",
      "Loss: 1.357 | Acc: 50.801% (21686/42688)\n",
      "Loss: 1.357 | Acc: 50.816% (21725/42752)\n",
      "Loss: 1.357 | Acc: 50.813% (21756/42816)\n",
      "Loss: 1.357 | Acc: 50.816% (21790/42880)\n",
      "Loss: 1.357 | Acc: 50.813% (21821/42944)\n",
      "Loss: 1.357 | Acc: 50.805% (21850/43008)\n",
      "Loss: 1.357 | Acc: 50.794% (21878/43072)\n",
      "Loss: 1.357 | Acc: 50.800% (21913/43136)\n",
      "Loss: 1.357 | Acc: 50.801% (21946/43200)\n",
      "Loss: 1.357 | Acc: 50.797% (21977/43264)\n",
      "Loss: 1.357 | Acc: 50.803% (22012/43328)\n",
      "Loss: 1.357 | Acc: 50.784% (22036/43392)\n",
      "Loss: 1.357 | Acc: 50.789% (22071/43456)\n",
      "Loss: 1.357 | Acc: 50.795% (22106/43520)\n",
      "Loss: 1.357 | Acc: 50.789% (22136/43584)\n",
      "Loss: 1.357 | Acc: 50.779% (22164/43648)\n",
      "Loss: 1.357 | Acc: 50.771% (22193/43712)\n",
      "Loss: 1.357 | Acc: 50.765% (22223/43776)\n",
      "Loss: 1.357 | Acc: 50.776% (22260/43840)\n",
      "Loss: 1.357 | Acc: 50.786% (22297/43904)\n",
      "Loss: 1.357 | Acc: 50.789% (22331/43968)\n",
      "Loss: 1.357 | Acc: 50.781% (22360/44032)\n",
      "Loss: 1.357 | Acc: 50.764% (22385/44096)\n",
      "Loss: 1.358 | Acc: 50.763% (22417/44160)\n",
      "Loss: 1.358 | Acc: 50.753% (22445/44224)\n",
      "Loss: 1.358 | Acc: 50.747% (22475/44288)\n",
      "Loss: 1.358 | Acc: 50.762% (22514/44352)\n",
      "Loss: 1.357 | Acc: 50.770% (22550/44416)\n",
      "Loss: 1.357 | Acc: 50.776% (22585/44480)\n",
      "Loss: 1.357 | Acc: 50.777% (22618/44544)\n",
      "Loss: 1.357 | Acc: 50.780% (22652/44608)\n",
      "Loss: 1.357 | Acc: 50.792% (22690/44672)\n",
      "Loss: 1.357 | Acc: 50.782% (22718/44736)\n",
      "Loss: 1.358 | Acc: 50.768% (22744/44800)\n",
      "Loss: 1.358 | Acc: 50.776% (22780/44864)\n",
      "Loss: 1.358 | Acc: 50.775% (22812/44928)\n",
      "Loss: 1.358 | Acc: 50.776% (22845/44992)\n",
      "Loss: 1.357 | Acc: 50.799% (22888/45056)\n",
      "Loss: 1.358 | Acc: 50.782% (22913/45120)\n",
      "Loss: 1.358 | Acc: 50.777% (22943/45184)\n",
      "Loss: 1.358 | Acc: 50.782% (22978/45248)\n",
      "Loss: 1.358 | Acc: 50.781% (23010/45312)\n",
      "Loss: 1.358 | Acc: 50.789% (23046/45376)\n",
      "Loss: 1.358 | Acc: 50.783% (23076/45440)\n",
      "Loss: 1.358 | Acc: 50.798% (23115/45504)\n",
      "Loss: 1.358 | Acc: 50.805% (23151/45568)\n",
      "Loss: 1.358 | Acc: 50.804% (23183/45632)\n",
      "Loss: 1.358 | Acc: 50.803% (23215/45696)\n",
      "Loss: 1.358 | Acc: 50.800% (23246/45760)\n",
      "Loss: 1.358 | Acc: 50.797% (23277/45824)\n",
      "Loss: 1.358 | Acc: 50.793% (23308/45888)\n",
      "Loss: 1.358 | Acc: 50.801% (23344/45952)\n",
      "Loss: 1.357 | Acc: 50.819% (23385/46016)\n",
      "Loss: 1.358 | Acc: 50.820% (23418/46080)\n",
      "Loss: 1.357 | Acc: 50.824% (23452/46144)\n",
      "Loss: 1.357 | Acc: 50.842% (23493/46208)\n",
      "Loss: 1.357 | Acc: 50.860% (23534/46272)\n",
      "Loss: 1.356 | Acc: 50.874% (23573/46336)\n",
      "Loss: 1.356 | Acc: 50.888% (23612/46400)\n",
      "Loss: 1.356 | Acc: 50.887% (23644/46464)\n",
      "Loss: 1.357 | Acc: 50.890% (23678/46528)\n",
      "Loss: 1.356 | Acc: 50.904% (23717/46592)\n",
      "Loss: 1.357 | Acc: 50.894% (23745/46656)\n",
      "Loss: 1.357 | Acc: 50.886% (23774/46720)\n",
      "Loss: 1.357 | Acc: 50.887% (23807/46784)\n",
      "Loss: 1.357 | Acc: 50.907% (23849/46848)\n",
      "Loss: 1.357 | Acc: 50.897% (23877/46912)\n",
      "Loss: 1.357 | Acc: 50.896% (23909/46976)\n",
      "Loss: 1.357 | Acc: 50.889% (23938/47040)\n",
      "Loss: 1.357 | Acc: 50.885% (23969/47104)\n",
      "Loss: 1.357 | Acc: 50.897% (24007/47168)\n",
      "Loss: 1.357 | Acc: 50.898% (24040/47232)\n",
      "Loss: 1.357 | Acc: 50.888% (24068/47296)\n",
      "Loss: 1.356 | Acc: 50.887% (24100/47360)\n",
      "Loss: 1.356 | Acc: 50.898% (24138/47424)\n",
      "Loss: 1.356 | Acc: 50.905% (24174/47488)\n",
      "Loss: 1.356 | Acc: 50.904% (24206/47552)\n",
      "Loss: 1.356 | Acc: 50.907% (24240/47616)\n",
      "Loss: 1.356 | Acc: 50.906% (24272/47680)\n",
      "Loss: 1.356 | Acc: 50.909% (24306/47744)\n",
      "Loss: 1.356 | Acc: 50.906% (24337/47808)\n",
      "Loss: 1.356 | Acc: 50.909% (24371/47872)\n",
      "Loss: 1.356 | Acc: 50.907% (24403/47936)\n",
      "Loss: 1.356 | Acc: 50.908% (24436/48000)\n",
      "Loss: 1.356 | Acc: 50.918% (24473/48064)\n",
      "Loss: 1.356 | Acc: 50.923% (24508/48128)\n",
      "Loss: 1.356 | Acc: 50.923% (24541/48192)\n",
      "Loss: 1.355 | Acc: 50.924% (24574/48256)\n",
      "Loss: 1.355 | Acc: 50.923% (24606/48320)\n",
      "Loss: 1.355 | Acc: 50.920% (24637/48384)\n",
      "Loss: 1.356 | Acc: 50.921% (24670/48448)\n",
      "Loss: 1.355 | Acc: 50.932% (24708/48512)\n",
      "Loss: 1.355 | Acc: 50.926% (24738/48576)\n",
      "Loss: 1.355 | Acc: 50.940% (24777/48640)\n",
      "Loss: 1.355 | Acc: 50.924% (24802/48704)\n",
      "Loss: 1.356 | Acc: 50.917% (24831/48768)\n",
      "Loss: 1.355 | Acc: 50.924% (24867/48832)\n",
      "Loss: 1.356 | Acc: 50.918% (24897/48896)\n",
      "Loss: 1.356 | Acc: 50.919% (24930/48960)\n",
      "Loss: 1.356 | Acc: 50.922% (24952/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 50.922448979591834\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.217 | Acc: 51.562% (33/64)\n",
      "Loss: 1.265 | Acc: 48.438% (62/128)\n",
      "Loss: 1.291 | Acc: 48.438% (93/192)\n",
      "Loss: 1.371 | Acc: 47.266% (121/256)\n",
      "Loss: 1.374 | Acc: 47.500% (152/320)\n",
      "Loss: 1.381 | Acc: 47.656% (183/384)\n",
      "Loss: 1.390 | Acc: 47.768% (214/448)\n",
      "Loss: 1.373 | Acc: 47.656% (244/512)\n",
      "Loss: 1.330 | Acc: 48.958% (282/576)\n",
      "Loss: 1.315 | Acc: 50.312% (322/640)\n",
      "Loss: 1.341 | Acc: 49.006% (345/704)\n",
      "Loss: 1.339 | Acc: 49.219% (378/768)\n",
      "Loss: 1.341 | Acc: 49.279% (410/832)\n",
      "Loss: 1.336 | Acc: 49.777% (446/896)\n",
      "Loss: 1.327 | Acc: 50.000% (480/960)\n",
      "Loss: 1.313 | Acc: 50.586% (518/1024)\n",
      "Loss: 1.309 | Acc: 50.643% (551/1088)\n",
      "Loss: 1.306 | Acc: 50.955% (587/1152)\n",
      "Loss: 1.304 | Acc: 51.151% (622/1216)\n",
      "Loss: 1.317 | Acc: 50.547% (647/1280)\n",
      "Loss: 1.314 | Acc: 50.595% (680/1344)\n",
      "Loss: 1.311 | Acc: 50.781% (715/1408)\n",
      "Loss: 1.305 | Acc: 51.087% (752/1472)\n",
      "Loss: 1.308 | Acc: 50.977% (783/1536)\n",
      "Loss: 1.316 | Acc: 50.625% (810/1600)\n",
      "Loss: 1.322 | Acc: 50.481% (840/1664)\n",
      "Loss: 1.320 | Acc: 50.810% (878/1728)\n",
      "Loss: 1.321 | Acc: 50.893% (912/1792)\n",
      "Loss: 1.319 | Acc: 51.024% (947/1856)\n",
      "Loss: 1.322 | Acc: 50.938% (978/1920)\n",
      "Loss: 1.325 | Acc: 50.806% (1008/1984)\n",
      "Loss: 1.328 | Acc: 50.732% (1039/2048)\n",
      "Loss: 1.319 | Acc: 51.136% (1080/2112)\n",
      "Loss: 1.320 | Acc: 51.057% (1111/2176)\n",
      "Loss: 1.319 | Acc: 51.116% (1145/2240)\n",
      "Loss: 1.314 | Acc: 51.302% (1182/2304)\n",
      "Loss: 1.313 | Acc: 51.394% (1217/2368)\n",
      "Loss: 1.309 | Acc: 51.645% (1256/2432)\n",
      "Loss: 1.310 | Acc: 51.763% (1292/2496)\n",
      "Loss: 1.319 | Acc: 51.328% (1314/2560)\n",
      "Loss: 1.325 | Acc: 51.029% (1339/2624)\n",
      "Loss: 1.323 | Acc: 51.153% (1375/2688)\n",
      "Loss: 1.322 | Acc: 51.163% (1408/2752)\n",
      "Loss: 1.323 | Acc: 51.136% (1440/2816)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.322 | Acc: 51.111% (1472/2880)\n",
      "Loss: 1.320 | Acc: 51.257% (1509/2944)\n",
      "Loss: 1.321 | Acc: 51.363% (1545/3008)\n",
      "Loss: 1.323 | Acc: 51.432% (1580/3072)\n",
      "Loss: 1.322 | Acc: 51.339% (1610/3136)\n",
      "Loss: 1.321 | Acc: 51.312% (1642/3200)\n",
      "Loss: 1.321 | Acc: 51.225% (1672/3264)\n",
      "Loss: 1.321 | Acc: 51.202% (1704/3328)\n",
      "Loss: 1.320 | Acc: 51.238% (1738/3392)\n",
      "Loss: 1.320 | Acc: 51.418% (1777/3456)\n",
      "Loss: 1.318 | Acc: 51.477% (1812/3520)\n",
      "Loss: 1.318 | Acc: 51.535% (1847/3584)\n",
      "Loss: 1.320 | Acc: 51.343% (1873/3648)\n",
      "Loss: 1.319 | Acc: 51.428% (1909/3712)\n",
      "Loss: 1.319 | Acc: 51.351% (1939/3776)\n",
      "Loss: 1.316 | Acc: 51.536% (1979/3840)\n",
      "Loss: 1.318 | Acc: 51.434% (2008/3904)\n",
      "Loss: 1.318 | Acc: 51.462% (2042/3968)\n",
      "Loss: 1.318 | Acc: 51.463% (2075/4032)\n",
      "Loss: 1.321 | Acc: 51.367% (2104/4096)\n",
      "Loss: 1.323 | Acc: 51.346% (2136/4160)\n",
      "Loss: 1.322 | Acc: 51.373% (2170/4224)\n",
      "Loss: 1.321 | Acc: 51.376% (2203/4288)\n",
      "Loss: 1.320 | Acc: 51.425% (2238/4352)\n",
      "Loss: 1.318 | Acc: 51.472% (2273/4416)\n",
      "Loss: 1.316 | Acc: 51.540% (2309/4480)\n",
      "Loss: 1.315 | Acc: 51.585% (2344/4544)\n",
      "Loss: 1.316 | Acc: 51.497% (2373/4608)\n",
      "Loss: 1.313 | Acc: 51.584% (2410/4672)\n",
      "Loss: 1.312 | Acc: 51.731% (2450/4736)\n",
      "Loss: 1.314 | Acc: 51.729% (2483/4800)\n",
      "Loss: 1.313 | Acc: 51.809% (2520/4864)\n",
      "Loss: 1.313 | Acc: 51.786% (2552/4928)\n",
      "Loss: 1.315 | Acc: 51.643% (2578/4992)\n",
      "Loss: 1.314 | Acc: 51.701% (2614/5056)\n",
      "Loss: 1.317 | Acc: 51.543% (2639/5120)\n",
      "Loss: 1.317 | Acc: 51.562% (2673/5184)\n",
      "Loss: 1.317 | Acc: 51.524% (2704/5248)\n",
      "Loss: 1.315 | Acc: 51.581% (2740/5312)\n",
      "Loss: 1.319 | Acc: 51.525% (2770/5376)\n",
      "Loss: 1.320 | Acc: 51.471% (2800/5440)\n",
      "Loss: 1.320 | Acc: 51.472% (2833/5504)\n",
      "Loss: 1.320 | Acc: 51.509% (2868/5568)\n",
      "Loss: 1.323 | Acc: 51.456% (2898/5632)\n",
      "Loss: 1.324 | Acc: 51.440% (2930/5696)\n",
      "Loss: 1.323 | Acc: 51.441% (2963/5760)\n",
      "Loss: 1.321 | Acc: 51.494% (2999/5824)\n",
      "Loss: 1.323 | Acc: 51.376% (3025/5888)\n",
      "Loss: 1.323 | Acc: 51.327% (3055/5952)\n",
      "Loss: 1.323 | Acc: 51.313% (3087/6016)\n",
      "Loss: 1.322 | Acc: 51.332% (3121/6080)\n",
      "Loss: 1.322 | Acc: 51.335% (3154/6144)\n",
      "Loss: 1.322 | Acc: 51.273% (3183/6208)\n",
      "Loss: 1.324 | Acc: 51.244% (3214/6272)\n",
      "Loss: 1.323 | Acc: 51.326% (3252/6336)\n",
      "Loss: 1.323 | Acc: 51.281% (3282/6400)\n",
      "Loss: 1.323 | Acc: 51.284% (3315/6464)\n",
      "Loss: 1.322 | Acc: 51.363% (3353/6528)\n",
      "Loss: 1.323 | Acc: 51.365% (3386/6592)\n",
      "Loss: 1.323 | Acc: 51.367% (3419/6656)\n",
      "Loss: 1.324 | Acc: 51.369% (3452/6720)\n",
      "Loss: 1.323 | Acc: 51.430% (3489/6784)\n",
      "Loss: 1.320 | Acc: 51.533% (3529/6848)\n",
      "Loss: 1.323 | Acc: 51.519% (3561/6912)\n",
      "Loss: 1.324 | Acc: 51.462% (3590/6976)\n",
      "Loss: 1.324 | Acc: 51.420% (3620/7040)\n",
      "Loss: 1.324 | Acc: 51.436% (3654/7104)\n",
      "Loss: 1.324 | Acc: 51.451% (3688/7168)\n",
      "Loss: 1.326 | Acc: 51.341% (3713/7232)\n",
      "Loss: 1.325 | Acc: 51.371% (3748/7296)\n",
      "Loss: 1.323 | Acc: 51.413% (3784/7360)\n",
      "Loss: 1.324 | Acc: 51.360% (3813/7424)\n",
      "Loss: 1.322 | Acc: 51.402% (3849/7488)\n",
      "Loss: 1.322 | Acc: 51.417% (3883/7552)\n",
      "Loss: 1.323 | Acc: 51.379% (3913/7616)\n",
      "Loss: 1.322 | Acc: 51.458% (3952/7680)\n",
      "Loss: 1.320 | Acc: 51.562% (3993/7744)\n",
      "Loss: 1.320 | Acc: 51.614% (4030/7808)\n",
      "Loss: 1.320 | Acc: 51.588% (4061/7872)\n",
      "Loss: 1.320 | Acc: 51.600% (4095/7936)\n",
      "Loss: 1.321 | Acc: 51.538% (4123/8000)\n",
      "Loss: 1.322 | Acc: 51.488% (4152/8064)\n",
      "Loss: 1.323 | Acc: 51.464% (4183/8128)\n",
      "Loss: 1.323 | Acc: 51.440% (4214/8192)\n",
      "Loss: 1.325 | Acc: 51.344% (4239/8256)\n",
      "Loss: 1.327 | Acc: 51.190% (4259/8320)\n",
      "Loss: 1.328 | Acc: 51.169% (4290/8384)\n",
      "Loss: 1.327 | Acc: 51.160% (4322/8448)\n",
      "Loss: 1.328 | Acc: 51.116% (4351/8512)\n",
      "Loss: 1.329 | Acc: 51.084% (4381/8576)\n",
      "Loss: 1.331 | Acc: 51.042% (4410/8640)\n",
      "Loss: 1.332 | Acc: 50.988% (4438/8704)\n",
      "Loss: 1.333 | Acc: 50.981% (4470/8768)\n",
      "Loss: 1.333 | Acc: 51.030% (4507/8832)\n",
      "Loss: 1.332 | Acc: 51.068% (4543/8896)\n",
      "Loss: 1.331 | Acc: 51.105% (4579/8960)\n",
      "Loss: 1.331 | Acc: 51.152% (4616/9024)\n",
      "Loss: 1.331 | Acc: 51.144% (4648/9088)\n",
      "Loss: 1.332 | Acc: 51.169% (4683/9152)\n",
      "Loss: 1.331 | Acc: 51.172% (4716/9216)\n",
      "Loss: 1.329 | Acc: 51.304% (4761/9280)\n",
      "Loss: 1.330 | Acc: 51.327% (4796/9344)\n",
      "Loss: 1.331 | Acc: 51.307% (4827/9408)\n",
      "Loss: 1.332 | Acc: 51.256% (4855/9472)\n",
      "Loss: 1.331 | Acc: 51.353% (4897/9536)\n",
      "Loss: 1.330 | Acc: 51.406% (4935/9600)\n",
      "Loss: 1.331 | Acc: 51.376% (4965/9664)\n",
      "Loss: 1.330 | Acc: 51.388% (4999/9728)\n",
      "Loss: 1.332 | Acc: 51.348% (5028/9792)\n",
      "Loss: 1.331 | Acc: 51.360% (5062/9856)\n",
      "Loss: 1.332 | Acc: 51.341% (5093/9920)\n",
      "Loss: 1.332 | Acc: 51.332% (5125/9984)\n",
      "Loss: 1.333 | Acc: 51.310% (5131/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 51.31\n",
      "\n",
      "Final train set accuracy is 50.922448979591834\n",
      "Final test set accuracy is 51.31\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims, input_dims, output_dims, num_trans_layers, num_heads, image_k, patch_k)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c76a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
